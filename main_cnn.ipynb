{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4845e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from data.cats_and_dogs import BinaryCIFARDataModule\n",
    "from models.model_cnn_mika import CatsDogsModel\n",
    "from models.model_cnn_mika import KaninchenModel\n",
    "from data.cats_and_dogs import KaninchenDataModule\n",
    "from data.Kaninchen_Module import BinaryImageDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117666f9",
   "metadata": {},
   "source": [
    "### Loading Configuration\n",
    "\n",
    "In the following steps, we will load the configuration settings using the `load_configuration` function. The configuration is stored in the `config` variable which will be used throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74321032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.load_configuration import load_configuration\n",
    "config = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcae64d",
   "metadata": {},
   "source": [
    "### Logging in to Weights & Biases (wandb)\n",
    "\n",
    "Before starting any experiment tracking, ensure you are logged in to your Weights & Biases (wandb) account. This enables automatic logging of metrics, model checkpoints, and experiment configurations. The following code logs you in to wandb:\n",
    "\n",
    "```python\n",
    "wandb.login()\n",
    "```\n",
    "If you are running this for the first time, you may be prompted to enter your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Wandb logger\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414378fc",
   "metadata": {},
   "source": [
    "### Setting Seeds for Reproducibility\n",
    "\n",
    "To ensure comparable and reproducible results, we set the random seed using the `seed_everything` function from PyTorch Lightning. This helps in achieving consistent behavior across multiple runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e10d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(config['seed'])\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # disable oneDNN optimizations for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86df64",
   "metadata": {},
   "source": [
    "### Checking for GPU Devices\n",
    "\n",
    "In this step, we check for the availability of GPU devices and print the device currently being used by PyTorch. This ensures that the computations are performed on the most efficient hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b717a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Torch Version: ', torch.__version__)\n",
    "print('Using device: ', device)\n",
    "if device.type == 'cuda':\n",
    "    print('Cuda Version: ', torch.version.cuda)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177b86e",
   "metadata": {},
   "source": [
    "### Defining Transformations and Instantiating DataModule\n",
    "\n",
    "In this step, we will define the necessary data transformations and initialize the `Animal_DataModule` with the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df954014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define transformations here\n",
    "size = config['image_size']\n",
    "\n",
    "# TODO: Mika thinks this function definition leads to the problem that it's no longer Pickleable, so transformer definition needs to leave the Main notebook!!!\n",
    "def center_crop_square(img):\n",
    "    min_side = min(img.width, img.height)\n",
    "    top = max(0, (img.height - min_side) // 2)\n",
    "    left = max(0, (img.width - min_side) // 2)\n",
    "    return transforms.functional.crop(img, top=top, left=left, height=min_side, width=min_side)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(center_crop_square),\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#dm = BinaryCIFARDataModule(transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "\n",
    "# must set workers to 0, otherwise it will not work with the DataLoader\n",
    "\n",
    "dm = BinaryImageDataModule(data_dir=config['path_to_split_aug_pics'],transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "dm.setup()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "print('Train dataset size:', len(dm.train_dataset))\n",
    "print('Validation dataset size:', len(dm.val_dataset))\n",
    "print('Test dataset size:', len(dm.test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c560cb9",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "In this step, we will define the model architecture and print its summary using the `ModelSummary` utility from PyTorch Lightning. This provides an overview of the model's layers, parameters, and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3442708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CatsDogsModel()\n",
    "model = KaninchenModel()\n",
    "print(ModelSummary(model, max_depth=-1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62271018",
   "metadata": {},
   "source": [
    "### Training the Model and Logging with Weights & Biases\n",
    "\n",
    "In this step, we initialize the Wandb logger and configure the experiment name to include a timestamp for better tracking. The `Trainer` from PyTorch Lightning is set up with the Wandb logger and an early stopping callback to monitor validation loss and prevent overfitting. After training, the Wandb run is finished, and the trained model checkpoint is saved with a unique filename containing the current date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Wandb logger\n",
    "# add time to the name of the experiment\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Initialize wandb logger\n",
    "wandb_logger = WandbLogger(\n",
    "    project=config['wandb_project_name'],\n",
    "    name=config['wandb_experiment_name'] + '_' + current_time,\n",
    "    config={\n",
    "        #'dataset': 'CIFAR-binary',\n",
    "        'dataset': 'Kaninchen',\n",
    "        'batch_size': config['batch_size'],\n",
    "        'max_epochs': config['max_epochs'],\n",
    "        'learning_rate': config['learning_rate']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize Trainer with wandb logger, using early stopping callback (https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)\n",
    "trainer = Trainer(\n",
    "    max_epochs=config['max_epochs'], \n",
    "    default_root_dir='model/checkpoint/', #data_directory, \n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\", \n",
    "    strategy=\"auto\",\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')], \n",
    "    logger=wandb_logger)\n",
    "\n",
    "# Training of the model\n",
    "trainer.fit(model=model, datamodule=dm)\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()\n",
    "\n",
    "# Create a filename with date identifier\n",
    "model_filename = f\"{config['wandb_experiment_name']}_{current_time}.ckpt\"\n",
    "\n",
    "# Save the model's state_dict to the path specified in config\n",
    "save_path = os.path.join(os.path.dirname(config['path_to_models']), model_filename)\n",
    "trainer.save_checkpoint(save_path)\n",
    "print(f\"Model checkpoint saved as {save_path}\")\n",
    "config['path_to_model'] = save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a07eb",
   "metadata": {},
   "source": [
    "# Predict with the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import torch\n",
    "# # Load the saved model weights from the path specified in config\n",
    "\n",
    "# def predict_image(path, model):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((150, 150)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "#     ])\n",
    "\n",
    "#     img = Image.open(path).convert('RGB')\n",
    "#     img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         pred = model(img)\n",
    "#         result = \"Dog\" if pred.item() > 0.5 else \"Cat\"\n",
    "#     print(f\"Prediction: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738413b",
   "metadata": {},
   "source": [
    "### Loading and Evaluating the Trained Model\n",
    "\n",
    "The trained model is loaded from the checkpoint specified in the configuration. If the checkpoint exists, the model weights are restored and the model is set to evaluation mode. PyTorch Lightning's `Trainer` is then used to evaluate the model on the test dataset, providing a streamlined way to assess model performance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = config['path_to_model']\n",
    "if model_path and os.path.exists(model_path):\n",
    "    #model = CatsDogsModel.load_from_checkpoint(model_path, map_location=device)\n",
    "    model = KaninchenModel.load_from_checkpoint(model_path, map_location=device)\n",
    "    print(f\"Loaded model weights from {model_path}\")\n",
    "else:\n",
    "    print(\"Model path not found or not specified in config.\")\n",
    "\n",
    "# Ensure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Pytorch Lightning's Trainer can be used to test the model\n",
    "trainer = Trainer()\n",
    "trainer.test(model=model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDKI-Projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
