{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4845e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from models.model_cnn import KaninchenModel, KaninchenModelResidual, KaninchenModel_v1, KaninchenModel_v2, KaninchenModel_v3, KaninchenModel_v4, KaninchenModel_v5\n",
    "from data.datamodule import BinaryImageDataModule, ReducedSizeBinaryImageDataModule\n",
    "\n",
    "import optuna\n",
    "from training.hyperparameter_tuning import CnnOptunaTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117666f9",
   "metadata": {},
   "source": [
    "### Loading Configuration\n",
    "\n",
    "In the following steps, we will load the configuration settings using the `load_configuration` function. The configuration is stored in the `config` variable which will be used throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74321032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Name: DESKTOP-LUKAS\n",
      "Loaded configuration from config/config_lukas.yaml\n"
     ]
    }
   ],
   "source": [
    "from config.load_configuration import load_configuration\n",
    "config = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcae64d",
   "metadata": {},
   "source": [
    "### Logging in to Weights & Biases (wandb)\n",
    "\n",
    "Before starting any experiment tracking, ensure you are logged in to your Weights & Biases (wandb) account. This enables automatic logging of metrics, model checkpoints, and experiment configurations. The following code logs you in to wandb:\n",
    "\n",
    "```python\n",
    "wandb.login()\n",
    "```\n",
    "If you are running this for the first time, you may be prompted to enter your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee8f5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Wandb logger\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414378fc",
   "metadata": {},
   "source": [
    "### Setting Seeds for Reproducibility\n",
    "\n",
    "To ensure comparable and reproducible results, we set the random seed using the `seed_everything` function from PyTorch Lightning. This helps in achieving consistent behavior across multiple runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e10d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # disable oneDNN optimizations for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86df64",
   "metadata": {},
   "source": [
    "### Checking for GPU Devices\n",
    "\n",
    "In this step, we check for the availability of GPU devices and print the device currently being used by PyTorch. This ensures that the computations are performed on the most efficient hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b717a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.7.0+cu128\n",
      "Using device:  cuda\n",
      "Cuda Version:  12.8\n",
      "NVIDIA GeForce RTX 5060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Torch Version: ', torch.__version__)\n",
    "print('Using device: ', device)\n",
    "if device.type == 'cuda':\n",
    "    print('Cuda Version: ', torch.version.cuda)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177b86e",
   "metadata": {},
   "source": [
    "### Defining Transformations and Instantiating DataModule\n",
    "\n",
    "In this step, we will define the necessary data transformations and initialize the `Animal_DataModule` with the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df954014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define transformations here\n",
    "size = config['image_size']\n",
    "\n",
    "import data.custom_transforms as custom_transforms\n",
    "transform = v2.Compose([\n",
    "    custom_transforms.CenterCropSquare(),\n",
    "    v2.Resize((size, size)),\n",
    "    v2.ToTensor(),\n",
    "    # v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dm = ReducedSizeBinaryImageDataModule(data_dir=config['path_to_split_aug_pics'], transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "# dm = BinaryImageDataModule(data_dir=config['path_to_split_aug_pics'], transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "dm.setup()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "# Show a few images from the training set\n",
    "from torchvision.utils import make_grid\n",
    "def show_images(loader):\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images[:16]  # Show only the first 16 images\n",
    "    labels = labels[:16]\n",
    "    grid = make_grid(images, nrow=4, padding=2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "    plt.title('Sample Images')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "show_images(train_loader)\n",
    "\n",
    "print('Train dataset size:', len(dm.train_dataset))\n",
    "print('Validation dataset size:', len(dm.val_dataset))\n",
    "print('Test dataset size:', len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c560cb9",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "In this step, we will define the model architecture and print its summary using the `ModelSummary` utility from PyTorch Lightning. This provides an overview of the model's layers, parameters, and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3442708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                | Type              | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0  | criterion           | BCEWithLogitsLoss | 0      | train\n",
      "1  | sigmoid             | Sigmoid           | 0      | train\n",
      "2  | model               | Sequential        | 1.1 M  | train\n",
      "3  | model.0             | Conv2d            | 896    | train\n",
      "4  | model.1             | BatchNorm2d       | 64     | train\n",
      "5  | model.2             | ReLU              | 0      | train\n",
      "6  | model.3             | MaxPool2d         | 0      | train\n",
      "7  | model.4             | Conv2d            | 18.5 K | train\n",
      "8  | model.5             | BatchNorm2d       | 128    | train\n",
      "9  | model.6             | ReLU              | 0      | train\n",
      "10 | model.7             | MaxPool2d         | 0      | train\n",
      "11 | model.8             | Conv2d            | 73.9 K | train\n",
      "12 | model.9             | BatchNorm2d       | 256    | train\n",
      "13 | model.10            | ReLU              | 0      | train\n",
      "14 | model.11            | MaxPool2d         | 0      | train\n",
      "15 | model.12            | Flatten           | 0      | train\n",
      "16 | model.13            | Linear            | 1.0 M  | train\n",
      "17 | model.14            | ReLU              | 0      | train\n",
      "18 | model.15            | Dropout           | 0      | train\n",
      "19 | model.16            | Linear            | 513    | train\n",
      "20 | train_accuracy      | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy        | BinaryAccuracy    | 0      | train\n",
      "22 | val_precision       | BinaryPrecision   | 0      | train\n",
      "23 | val_recall          | BinaryRecall      | 0      | train\n",
      "24 | test_accuracy       | BinaryAccuracy    | 0      | train\n",
      "25 | init_conv           | Sequential        | 1.9 K  | train\n",
      "26 | init_conv.0         | Conv2d            | 1.8 K  | train\n",
      "27 | init_conv.1         | BatchNorm2d       | 128    | train\n",
      "28 | init_conv.2         | SiLU              | 0      | train\n",
      "29 | layer1              | Sequential        | 221 K  | train\n",
      "30 | layer1.0            | Conv2d            | 73.9 K | train\n",
      "31 | layer1.1            | BatchNorm2d       | 256    | train\n",
      "32 | layer1.2            | SiLU              | 0      | train\n",
      "33 | layer1.3            | Conv2d            | 147 K  | train\n",
      "34 | layer1.4            | BatchNorm2d       | 256    | train\n",
      "35 | layer1.5            | SiLU              | 0      | train\n",
      "36 | layer1.6            | MaxPool2d         | 0      | train\n",
      "37 | layer2              | Sequential        | 295 K  | train\n",
      "38 | layer2.0            | Conv2d            | 147 K  | train\n",
      "39 | layer2.1            | BatchNorm2d       | 256    | train\n",
      "40 | layer2.2            | SiLU              | 0      | train\n",
      "41 | layer2.3            | Conv2d            | 147 K  | train\n",
      "42 | layer2.4            | BatchNorm2d       | 256    | train\n",
      "43 | layer2.5            | SiLU              | 0      | train\n",
      "44 | layer2.6            | MaxPool2d         | 0      | train\n",
      "45 | layer3              | Sequential        | 886 K  | train\n",
      "46 | layer3.0            | Conv2d            | 295 K  | train\n",
      "47 | layer3.1            | BatchNorm2d       | 512    | train\n",
      "48 | layer3.2            | SiLU              | 0      | train\n",
      "49 | layer3.3            | Conv2d            | 590 K  | train\n",
      "50 | layer3.4            | BatchNorm2d       | 512    | train\n",
      "51 | layer3.5            | SiLU              | 0      | train\n",
      "52 | layer3.6            | MaxPool2d         | 0      | train\n",
      "53 | layer4              | ResidualBlock     | 1.2 M  | train\n",
      "54 | layer4.conv_block   | Sequential        | 1.2 M  | train\n",
      "55 | layer4.conv_block.0 | Conv2d            | 590 K  | train\n",
      "56 | layer4.conv_block.1 | BatchNorm2d       | 512    | train\n",
      "57 | layer4.conv_block.2 | ReLU              | 0      | train\n",
      "58 | layer4.conv_block.3 | Conv2d            | 590 K  | train\n",
      "59 | layer4.conv_block.4 | BatchNorm2d       | 512    | train\n",
      "60 | layer4.skip         | Sequential        | 66.3 K | train\n",
      "61 | layer4.skip.0       | Conv2d            | 65.8 K | train\n",
      "62 | layer4.skip.1       | BatchNorm2d       | 512    | train\n",
      "63 | layer4.relu         | ReLU              | 0      | train\n",
      "64 | flatten             | Flatten           | 0      | train\n",
      "65 | fc                  | Linear            | 16.4 K | train\n",
      "-------------------------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.252    Total estimated model params size (MB)\n",
      "66        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "KaninchenModel_v1\n"
     ]
    }
   ],
   "source": [
    "#model = CatsDogsModel()\n",
    "# model = KaninchenModel()\n",
    "model = KaninchenModel_v1()\n",
    "print(ModelSummary(model, max_depth=-1))  \n",
    "print(type(model).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62271018",
   "metadata": {},
   "source": [
    "### Training the Model and Logging with Weights & Biases\n",
    "\n",
    "In this step, we initialize the Wandb logger and configure the experiment name to include a timestamp for better tracking. The `Trainer` from PyTorch Lightning is set up with the Wandb logger and an early stopping callback to monitor validation loss and prevent overfitting. After training, the Wandb run is finished, and the trained model checkpoint is saved with a unique filename containing the current date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d5b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250601_225437-m4sgwj3f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/m4sgwj3f' target=\"_blank\">binaryClassification_CNN_KaninchenModel_v1_2025-06-01_22-54-37</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/m4sgwj3f' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/m4sgwj3f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "1  | sigmoid        | Sigmoid           | 0      | train\n",
      "2  | model          | Sequential        | 1.1 M  | train\n",
      "3  | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4  | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5  | val_precision  | BinaryPrecision   | 0      | train\n",
      "6  | val_recall     | BinaryRecall      | 0      | train\n",
      "7  | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "8  | init_conv      | Sequential        | 1.9 K  | train\n",
      "9  | layer1         | Sequential        | 221 K  | train\n",
      "10 | layer2         | Sequential        | 295 K  | train\n",
      "11 | layer3         | Sequential        | 886 K  | train\n",
      "12 | layer4         | ResidualBlock     | 1.2 M  | train\n",
      "13 | flatten        | Flatten           | 0      | train\n",
      "14 | fc             | Linear            | 16.4 K | train\n",
      "--------------------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.252    Total estimated model params size (MB)\n",
      "66        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 161/161 [00:33<00:00,  4.85it/s, v_num=wj3f, val_loss=0.619]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>▁▇▇██▇█▇██</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_acc</td><td>▁▃▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▄▇▂▃▄▄▅▃▄▄▄▄▂▄▃▃▃▁▄▄▂▂▅▃▁▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▅▁▆█▆▆▄██</td></tr><tr><td>val_loss</td><td>▄█▃▁▄▄█▁▃</td></tr><tr><td>val_precision</td><td>▅▁▅█▇▅▃▆▆</td></tr><tr><td>val_recall</td><td>▂█▇▁▁██▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.77836</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>train_acc</td><td>0.79717</td></tr><tr><td>train_loss</td><td>0.43745</td></tr><tr><td>trainer/global_step</td><td>1448</td></tr><tr><td>val_acc</td><td>0.71281</td></tr><tr><td>val_loss</td><td>0.61903</td></tr><tr><td>val_precision</td><td>0.57008</td></tr><tr><td>val_recall</td><td>0.71542</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">binaryClassification_CNN_KaninchenModel_v1_2025-06-01_22-54-37</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/m4sgwj3f' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/m4sgwj3f</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 30 media file(s), 40 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250601_225437-m4sgwj3f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved as C:\\Users\\lukas\\SynologyDrive_IMS/SS25_MSYS_KAER-AI-PoseAct/21_Test_Data/Models/CNN\\binaryClassification_CNN_KaninchenModel_v1_2025-06-01_22-54-37.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Wandb logger\n",
    "# add time to the name of the experiment\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Initialize wandb logger\n",
    "wandb_logger = WandbLogger(\n",
    "    project=config['wandb_project_name'],\n",
    "    name=f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}\",\n",
    "    config={\n",
    "        'model': type(model).__name__,\n",
    "        'dataset': 'DwarfRabbits-binary',\n",
    "        'batch_size': config['batch_size'],\n",
    "        'max_epochs': config['max_epochs'],\n",
    "        'learning_rate': config['learning_rate']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize Trainer with wandb logger, using early stopping callback (https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)\n",
    "trainer = Trainer(\n",
    "    max_epochs=config['max_epochs'], \n",
    "    default_root_dir='model/checkpoint/', #data_directory, \n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\", \n",
    "    strategy=\"auto\",\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')], \n",
    "    logger=wandb_logger)\n",
    "\n",
    "# Training of the model\n",
    "trainer.fit(model=model, datamodule=dm)\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()\n",
    "\n",
    "# Create a filename with date identifier\n",
    "model_filename = f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}.ckpt\"\n",
    "\n",
    "# Save the model's state_dict to the path specified in config\n",
    "save_path = os.path.join(os.path.dirname(config['path_to_models']), model_filename)\n",
    "trainer.save_checkpoint(save_path)\n",
    "print(f\"Model checkpoint saved as {save_path}\")\n",
    "config['path_to_model'] = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                | Type              | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0  | criterion           | BCEWithLogitsLoss | 0      | train\n",
      "1  | sigmoid             | Sigmoid           | 0      | train\n",
      "2  | model               | Sequential        | 1.1 M  | train\n",
      "3  | model.0             | Conv2d            | 896    | train\n",
      "4  | model.1             | BatchNorm2d       | 64     | train\n",
      "5  | model.2             | ReLU              | 0      | train\n",
      "6  | model.3             | MaxPool2d         | 0      | train\n",
      "7  | model.4             | Conv2d            | 18.5 K | train\n",
      "8  | model.5             | BatchNorm2d       | 128    | train\n",
      "9  | model.6             | ReLU              | 0      | train\n",
      "10 | model.7             | MaxPool2d         | 0      | train\n",
      "11 | model.8             | Conv2d            | 73.9 K | train\n",
      "12 | model.9             | BatchNorm2d       | 256    | train\n",
      "13 | model.10            | ReLU              | 0      | train\n",
      "14 | model.11            | MaxPool2d         | 0      | train\n",
      "15 | model.12            | Flatten           | 0      | train\n",
      "16 | model.13            | Linear            | 1.0 M  | train\n",
      "17 | model.14            | ReLU              | 0      | train\n",
      "18 | model.15            | Dropout           | 0      | train\n",
      "19 | model.16            | Linear            | 513    | train\n",
      "20 | train_accuracy      | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy        | BinaryAccuracy    | 0      | train\n",
      "22 | val_precision       | BinaryPrecision   | 0      | train\n",
      "23 | val_recall          | BinaryRecall      | 0      | train\n",
      "24 | test_accuracy       | BinaryAccuracy    | 0      | train\n",
      "25 | init_conv           | Sequential        | 1.9 K  | train\n",
      "26 | init_conv.0         | Conv2d            | 1.8 K  | train\n",
      "27 | init_conv.1         | BatchNorm2d       | 128    | train\n",
      "28 | init_conv.2         | SiLU              | 0      | train\n",
      "29 | layer1              | Sequential        | 221 K  | train\n",
      "30 | layer1.0            | Conv2d            | 73.9 K | train\n",
      "31 | layer1.1            | BatchNorm2d       | 256    | train\n",
      "32 | layer1.2            | SiLU              | 0      | train\n",
      "33 | layer1.3            | Conv2d            | 147 K  | train\n",
      "34 | layer1.4            | BatchNorm2d       | 256    | train\n",
      "35 | layer1.5            | SiLU              | 0      | train\n",
      "36 | layer1.6            | MaxPool2d         | 0      | train\n",
      "37 | layer2              | Sequential        | 295 K  | train\n",
      "38 | layer2.0            | Conv2d            | 147 K  | train\n",
      "39 | layer2.1            | BatchNorm2d       | 256    | train\n",
      "40 | layer2.2            | SiLU              | 0      | train\n",
      "41 | layer2.3            | Conv2d            | 147 K  | train\n",
      "42 | layer2.4            | BatchNorm2d       | 256    | train\n",
      "43 | layer2.5            | SiLU              | 0      | train\n",
      "44 | layer2.6            | MaxPool2d         | 0      | train\n",
      "45 | layer3              | Sequential        | 886 K  | train\n",
      "46 | layer3.0            | Conv2d            | 295 K  | train\n",
      "47 | layer3.1            | BatchNorm2d       | 512    | train\n",
      "48 | layer3.2            | SiLU              | 0      | train\n",
      "49 | layer3.3            | Conv2d            | 590 K  | train\n",
      "50 | layer3.4            | BatchNorm2d       | 512    | train\n",
      "51 | layer3.5            | SiLU              | 0      | train\n",
      "52 | layer3.6            | MaxPool2d         | 0      | train\n",
      "53 | layer4              | ResidualBlock     | 1.2 M  | train\n",
      "54 | layer4.conv_block   | Sequential        | 1.2 M  | train\n",
      "55 | layer4.conv_block.0 | Conv2d            | 590 K  | train\n",
      "56 | layer4.conv_block.1 | BatchNorm2d       | 512    | train\n",
      "57 | layer4.conv_block.2 | ReLU              | 0      | train\n",
      "58 | layer4.conv_block.3 | Conv2d            | 590 K  | train\n",
      "59 | layer4.conv_block.4 | BatchNorm2d       | 512    | train\n",
      "60 | layer4.skip         | Sequential        | 66.3 K | train\n",
      "61 | layer4.skip.0       | Conv2d            | 65.8 K | train\n",
      "62 | layer4.skip.1       | BatchNorm2d       | 512    | train\n",
      "63 | layer4.relu         | ReLU              | 0      | train\n",
      "64 | flatten             | Flatten           | 0      | train\n",
      "65 | fc                  | Linear            | 16.4 K | train\n",
      "-------------------------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.252    Total estimated model params size (MB)\n",
      "66        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "KaninchenModel_v1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250602_083110-uvlirovh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uvlirovh' target=\"_blank\">binaryClassification_CNN_KaninchenModel_v1_2025-06-02_08-31-10</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uvlirovh' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uvlirovh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "1  | sigmoid        | Sigmoid           | 0      | train\n",
      "2  | model          | Sequential        | 1.1 M  | train\n",
      "3  | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4  | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5  | val_precision  | BinaryPrecision   | 0      | train\n",
      "6  | val_recall     | BinaryRecall      | 0      | train\n",
      "7  | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "8  | init_conv      | Sequential        | 1.9 K  | train\n",
      "9  | layer1         | Sequential        | 221 K  | train\n",
      "10 | layer2         | Sequential        | 295 K  | train\n",
      "11 | layer3         | Sequential        | 886 K  | train\n",
      "12 | layer4         | ResidualBlock     | 1.2 M  | train\n",
      "13 | flatten        | Flatten           | 0      | train\n",
      "14 | fc             | Linear            | 16.4 K | train\n",
      "--------------------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.252    Total estimated model params size (MB)\n",
      "66        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 161/161 [00:31<00:00,  5.05it/s, v_num=rovh, val_loss=0.590]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>▁▆▇▆██▇████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▆▆▇▇█</td></tr><tr><td>train_loss</td><td>▆█▆▇▆▄▃▃▄▅▄▃▃▂▄▃▄▂▄▄▅▂▂▁▂▄▄▁▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>val_acc</td><td>▁▄▄▅▅▇▇▆▆█</td></tr><tr><td>val_loss</td><td>▆▂█▂▁▃▂▃▂▂</td></tr><tr><td>val_precision</td><td>▁▄▅▄▅▆▆▅▅█</td></tr><tr><td>val_recall</td><td>▅▄▁█▇▅▇█▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.77801</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>0.82395</td></tr><tr><td>train_loss</td><td>0.35472</td></tr><tr><td>trainer/global_step</td><td>1609</td></tr><tr><td>val_acc</td><td>0.73209</td></tr><tr><td>val_loss</td><td>0.58979</td></tr><tr><td>val_precision</td><td>0.62062</td></tr><tr><td>val_recall</td><td>0.59486</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">binaryClassification_CNN_KaninchenModel_v1_2025-06-02_08-31-10</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uvlirovh' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uvlirovh</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 33 media file(s), 44 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250602_083110-uvlirovh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved as C:\\Users\\lukas\\SynologyDrive_IMS/SS25_MSYS_KAER-AI-PoseAct/21_Test_Data/Models/CNN\\binaryClassification_CNN_KaninchenModel_v1_2025-06-02_08-31-10.ckpt\n",
      "Completed training for KaninchenModel_v1\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "1  | sigmoid        | Sigmoid           | 0      | train\n",
      "2  | model          | Sequential        | 1.1 M  | train\n",
      "3  | model.0        | Conv2d            | 896    | train\n",
      "4  | model.1        | BatchNorm2d       | 64     | train\n",
      "5  | model.2        | ReLU              | 0      | train\n",
      "6  | model.3        | MaxPool2d         | 0      | train\n",
      "7  | model.4        | Conv2d            | 18.5 K | train\n",
      "8  | model.5        | BatchNorm2d       | 128    | train\n",
      "9  | model.6        | ReLU              | 0      | train\n",
      "10 | model.7        | MaxPool2d         | 0      | train\n",
      "11 | model.8        | Conv2d            | 73.9 K | train\n",
      "12 | model.9        | BatchNorm2d       | 256    | train\n",
      "13 | model.10       | ReLU              | 0      | train\n",
      "14 | model.11       | MaxPool2d         | 0      | train\n",
      "15 | model.12       | Flatten           | 0      | train\n",
      "16 | model.13       | Linear            | 1.0 M  | train\n",
      "17 | model.14       | ReLU              | 0      | train\n",
      "18 | model.15       | Dropout           | 0      | train\n",
      "19 | model.16       | Linear            | 513    | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | val_precision  | BinaryPrecision   | 0      | train\n",
      "23 | val_recall     | BinaryRecall      | 0      | train\n",
      "24 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.573     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "KaninchenModel_v2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250602_083641-ox0161b3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ox0161b3' target=\"_blank\">binaryClassification_CNN_KaninchenModel_v2_2025-06-02_08-36-41</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ox0161b3' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ox0161b3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "1 | sigmoid        | Sigmoid           | 0      | train\n",
      "2 | model          | Sequential        | 1.1 M  | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "7 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.573     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KaninchenModel_v2' object has no attribute 'init_conv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     30\u001b[39m trainer = Trainer(\n\u001b[32m     31\u001b[39m     max_epochs=config[\u001b[33m'\u001b[39m\u001b[33mmax_epochs\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     32\u001b[39m     default_root_dir=\u001b[33m'\u001b[39m\u001b[33mmodel/checkpoint/\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     callbacks=[EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m5\u001b[39m, mode=\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m)], \n\u001b[32m     37\u001b[39m     logger=wandb_logger)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Training of the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Finish wandb\u001b[39;00m\n\u001b[32m     43\u001b[39m wandb.finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1080\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:145\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:437\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    431\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m step_args = (\n\u001b[32m    433\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    436\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\Documents\\HKA_DEV\\HKA_VDKI\\models\\model_cnn.py:175\u001b[39m, in \u001b[36mCnnModel.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03mPerforms a single validation step during model evaluation.\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m \u001b[33;03m    - Logs loss and metrics for monitoring.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    174\u001b[39m x, y = batch\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(logits.squeeze(), y.float())\n\u001b[32m    178\u001b[39m probs = \u001b[38;5;28mself\u001b[39m.sigmoid(logits.squeeze())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\Documents\\HKA_DEV\\HKA_VDKI\\models\\model_cnn.py:542\u001b[39m, in \u001b[36mKaninchenModel_v2.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_conv\u001b[49m(x)\n\u001b[32m    543\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.layer1(x)\n\u001b[32m    544\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.layer2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'KaninchenModel_v2' object has no attribute 'init_conv'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "# List of all model classes to train\n",
    "model_classes = [KaninchenModel_v1, KaninchenModel_v2, KaninchenModel_v3, KaninchenModel_v4, KaninchenModel_v5]\n",
    "\n",
    "import datetime\n",
    "for model_class in model_classes:\n",
    "    # Create model instance\n",
    "    model = model_class()\n",
    "    print(ModelSummary(model, max_depth=-1))  \n",
    "    print(type(model).__name__)\n",
    "\n",
    "    # Initialize the Wandb logger\n",
    "    # add time to the name of the experiment\n",
    "    now = datetime.datetime.now()\n",
    "    current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Initialize wandb logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=config['wandb_project_name'],\n",
    "        name=f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}\",\n",
    "        config={\n",
    "            'model': type(model).__name__,\n",
    "            'dataset': 'DwarfRabbits-binary',\n",
    "            'batch_size': config['batch_size'],\n",
    "            'max_epochs': config['max_epochs'],\n",
    "            'learning_rate': config['learning_rate']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with wandb logger, using early stopping callback\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config['max_epochs'], \n",
    "        default_root_dir='model/checkpoint/',\n",
    "        accelerator=\"auto\", \n",
    "        devices=\"auto\", \n",
    "        strategy=\"auto\",\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')], \n",
    "        logger=wandb_logger)\n",
    "\n",
    "    # Training of the model\n",
    "    trainer.fit(model=model, datamodule=dm)\n",
    "\n",
    "    # Finish wandb\n",
    "    wandb.finish()\n",
    "\n",
    "    # Create a filename with date identifier\n",
    "    model_filename = f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}.ckpt\"\n",
    "\n",
    "    # Save the model's state_dict to the path specified in config\n",
    "    save_path = os.path.join(os.path.dirname(config['path_to_models']), model_filename)\n",
    "    trainer.save_checkpoint(save_path)\n",
    "    print(f\"Model checkpoint saved as {save_path}\")\n",
    "    \n",
    "    # Update config with the last trained model path\n",
    "    config['path_to_model'] = save_path\n",
    "    \n",
    "    print(f\"Completed training for {model_class.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95161133",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization with Optuna\n",
    "\n",
    "This section performs automated hyperparameter tuning using Optuna to find the optimal model configuration. The optimization process searches for hyperparameters that minimize validation loss across multiple trials, helping to improve model performance beyond manual tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576170ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-01 17:30:27,053] A new study created in memory with name: no-name-708c7fc9-52b8-443f-9e48-87978ea44278\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250601_173027-fx1n6zz9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/fx1n6zz9' target=\"_blank\">KaninchenModel_bs32_img128_optAdam_lr2e-03_wd6e-05_sch_CosineAnnealingLR_2025-06-01_17-30</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/fx1n6zz9' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/fx1n6zz9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "1  | sigmoid        | Sigmoid           | 0      | train\n",
      "2  | model          | Sequential        | 13.1 M | train\n",
      "3  | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4  | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5  | val_precision  | BinaryPrecision   | 0      | train\n",
      "6  | val_recall     | BinaryRecall      | 0      | train\n",
      "7  | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "8  | conv1          | Sequential        | 39.0 K | train\n",
      "9  | conv2          | Sequential        | 221 K  | train\n",
      "10 | conv3          | Sequential        | 886 K  | train\n",
      "11 | conv4          | Sequential        | 3.5 M  | train\n",
      "12 | conv5          | Sequential        | 3.5 M  | train\n",
      "13 | cnn            | Sequential        | 4.7 M  | train\n",
      "14 | classifier     | Sequential        | 8.4 M  | train\n",
      "--------------------------------------------------------------\n",
      "16.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.6 M    Total params\n",
      "66.481    Total estimated model params size (MB)\n",
      "55        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "import datetime\n",
    "config['sweep_id'] = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "def objective(trial):\n",
    "    model = KaninchenModel                  # or another model's class, depending on your choice\n",
    "    trainer = CnnOptunaTrainer(\n",
    "        model=model,                        # Function to create the model\n",
    "        config=config,\n",
    "        normalize_mean=None, #[0.485, 0.456, 0.406], \n",
    "        normalize_std=None, #[0.229, 0.224, 0.225],\n",
    "        dataset_name=\"DwarfRabbits-binary\"\n",
    "    )\n",
    "    return trainer.run_training(trial)\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")  # because we minimize val_loss\n",
    "\n",
    "# Set verbosity to WARNING to reduce output clutter\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Start the hyperparameter optimization\n",
    "study.optimize(objective, n_trials=config['number_of_trials'])\n",
    "# study.optimize(objective, n_trials=3)\n",
    "\n",
    "# Best result\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best value (val_loss):\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378aefa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94714cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a07eb",
   "metadata": {},
   "source": [
    "# Predict with the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import torch\n",
    "# # Load the saved model weights from the path specified in config\n",
    "\n",
    "# def predict_image(path, model):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((150, 150)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "#     ])\n",
    "\n",
    "#     img = Image.open(path).convert('RGB')\n",
    "#     img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         pred = model(img)\n",
    "#         result = \"Dog\" if pred.item() > 0.5 else \"Cat\"\n",
    "#     print(f\"Prediction: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738413b",
   "metadata": {},
   "source": [
    "### Loading and Evaluating the Trained Model\n",
    "\n",
    "The trained model is loaded from the checkpoint specified in the configuration. If the checkpoint exists, the model weights are restored and the model is set to evaluation mode. PyTorch Lightning's `Trainer` is then used to evaluate the model on the test dataset, providing a streamlined way to assess model performance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fc6fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for KaninchenModel:\n\tMissing key(s) in state_dict: \"model.0.0.0.weight\", \"model.0.0.0.bias\", \"model.0.0.1.weight\", \"model.0.0.1.bias\", \"model.0.0.1.running_mean\", \"model.0.0.1.running_var\", \"model.0.0.3.weight\", \"model.0.0.3.bias\", \"model.0.0.4.weight\", \"model.0.0.4.bias\", \"model.0.0.4.running_mean\", \"model.0.0.4.running_var\", \"model.0.1.0.weight\", \"model.0.1.0.bias\", \"model.0.1.1.weight\", \"model.0.1.1.bias\", \"model.0.1.1.running_mean\", \"model.0.1.1.running_var\", \"model.0.1.3.weight\", \"model.0.1.3.bias\", \"model.0.1.4.weight\", \"model.0.1.4.bias\", \"model.0.1.4.running_mean\", \"model.0.1.4.running_var\", \"model.0.2.0.weight\", \"model.0.2.0.bias\", \"model.0.2.1.weight\", \"model.0.2.1.bias\", \"model.0.2.1.running_mean\", \"model.0.2.1.running_var\", \"model.0.2.3.weight\", \"model.0.2.3.bias\", \"model.0.2.4.weight\", \"model.0.2.4.bias\", \"model.0.2.4.running_mean\", \"model.0.2.4.running_var\", \"model.0.3.0.weight\", \"model.0.3.0.bias\", \"model.0.3.1.weight\", \"model.0.3.1.bias\", \"model.0.3.1.running_mean\", \"model.0.3.1.running_var\", \"model.0.3.3.weight\", \"model.0.3.3.bias\", \"model.0.3.4.weight\", \"model.0.3.4.bias\", \"model.0.3.4.running_mean\", \"model.0.3.4.running_var\", \"model.1.1.weight\", \"model.1.1.bias\", \"model.1.4.weight\", \"model.1.4.bias\", \"conv1.0.weight\", \"conv1.0.bias\", \"conv1.1.weight\", \"conv1.1.bias\", \"conv1.1.running_mean\", \"conv1.1.running_var\", \"conv1.3.weight\", \"conv1.3.bias\", \"conv1.4.weight\", \"conv1.4.bias\", \"conv1.4.running_mean\", \"conv1.4.running_var\", \"conv2.0.weight\", \"conv2.0.bias\", \"conv2.1.weight\", \"conv2.1.bias\", \"conv2.1.running_mean\", \"conv2.1.running_var\", \"conv2.3.weight\", \"conv2.3.bias\", \"conv2.4.weight\", \"conv2.4.bias\", \"conv2.4.running_mean\", \"conv2.4.running_var\", \"conv3.0.weight\", \"conv3.0.bias\", \"conv3.1.weight\", \"conv3.1.bias\", \"conv3.1.running_mean\", \"conv3.1.running_var\", \"conv3.3.weight\", \"conv3.3.bias\", \"conv3.4.weight\", \"conv3.4.bias\", \"conv3.4.running_mean\", \"conv3.4.running_var\", \"conv4.0.weight\", \"conv4.0.bias\", \"conv4.1.weight\", \"conv4.1.bias\", \"conv4.1.running_mean\", \"conv4.1.running_var\", \"conv4.3.weight\", \"conv4.3.bias\", \"conv4.4.weight\", \"conv4.4.bias\", \"conv4.4.running_mean\", \"conv4.4.running_var\", \"conv5.0.weight\", \"conv5.0.bias\", \"conv5.1.weight\", \"conv5.1.bias\", \"conv5.1.running_mean\", \"conv5.1.running_var\", \"conv5.3.weight\", \"conv5.3.bias\", \"conv5.4.weight\", \"conv5.4.bias\", \"conv5.4.running_mean\", \"conv5.4.running_var\", \"cnn.0.0.weight\", \"cnn.0.0.bias\", \"cnn.0.1.weight\", \"cnn.0.1.bias\", \"cnn.0.1.running_mean\", \"cnn.0.1.running_var\", \"cnn.0.3.weight\", \"cnn.0.3.bias\", \"cnn.0.4.weight\", \"cnn.0.4.bias\", \"cnn.0.4.running_mean\", \"cnn.0.4.running_var\", \"cnn.1.0.weight\", \"cnn.1.0.bias\", \"cnn.1.1.weight\", \"cnn.1.1.bias\", \"cnn.1.1.running_mean\", \"cnn.1.1.running_var\", \"cnn.1.3.weight\", \"cnn.1.3.bias\", \"cnn.1.4.weight\", \"cnn.1.4.bias\", \"cnn.1.4.running_mean\", \"cnn.1.4.running_var\", \"cnn.2.0.weight\", \"cnn.2.0.bias\", \"cnn.2.1.weight\", \"cnn.2.1.bias\", \"cnn.2.1.running_mean\", \"cnn.2.1.running_var\", \"cnn.2.3.weight\", \"cnn.2.3.bias\", \"cnn.2.4.weight\", \"cnn.2.4.bias\", \"cnn.2.4.running_mean\", \"cnn.2.4.running_var\", \"cnn.3.0.weight\", \"cnn.3.0.bias\", \"cnn.3.1.weight\", \"cnn.3.1.bias\", \"cnn.3.1.running_mean\", \"cnn.3.1.running_var\", \"cnn.3.3.weight\", \"cnn.3.3.bias\", \"cnn.3.4.weight\", \"cnn.3.4.bias\", \"cnn.3.4.running_mean\", \"cnn.3.4.running_var\", \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.4.weight\", \"classifier.4.bias\". \n\tUnexpected key(s) in state_dict: \"init_conv.0.weight\", \"init_conv.0.bias\", \"init_conv.1.weight\", \"init_conv.1.bias\", \"init_conv.1.running_mean\", \"init_conv.1.running_var\", \"init_conv.1.num_batches_tracked\", \"layer1.0.weight\", \"layer1.0.bias\", \"layer1.1.weight\", \"layer1.1.bias\", \"layer1.1.running_mean\", \"layer1.1.running_var\", \"layer1.1.num_batches_tracked\", \"layer1.3.weight\", \"layer1.3.bias\", \"layer1.4.weight\", \"layer1.4.bias\", \"layer1.4.running_mean\", \"layer1.4.running_var\", \"layer1.4.num_batches_tracked\", \"layer2.0.weight\", \"layer2.0.bias\", \"layer2.1.weight\", \"layer2.1.bias\", \"layer2.1.running_mean\", \"layer2.1.running_var\", \"layer2.1.num_batches_tracked\", \"layer2.3.weight\", \"layer2.3.bias\", \"layer2.4.weight\", \"layer2.4.bias\", \"layer2.4.running_mean\", \"layer2.4.running_var\", \"layer2.4.num_batches_tracked\", \"layer3.0.weight\", \"layer3.0.bias\", \"layer3.1.weight\", \"layer3.1.bias\", \"layer3.1.running_mean\", \"layer3.1.running_var\", \"layer3.1.num_batches_tracked\", \"layer3.3.weight\", \"layer3.3.bias\", \"layer3.4.weight\", \"layer3.4.bias\", \"layer3.4.running_mean\", \"layer3.4.running_var\", \"layer3.4.num_batches_tracked\", \"layer4.conv_block.0.weight\", \"layer4.conv_block.0.bias\", \"layer4.conv_block.1.weight\", \"layer4.conv_block.1.bias\", \"layer4.conv_block.1.running_mean\", \"layer4.conv_block.1.running_var\", \"layer4.conv_block.1.num_batches_tracked\", \"layer4.conv_block.3.weight\", \"layer4.conv_block.3.bias\", \"layer4.conv_block.4.weight\", \"layer4.conv_block.4.bias\", \"layer4.conv_block.4.running_mean\", \"layer4.conv_block.4.running_var\", \"layer4.conv_block.4.num_batches_tracked\", \"layer4.skip.0.weight\", \"layer4.skip.0.bias\", \"layer4.skip.1.weight\", \"layer4.skip.1.bias\", \"layer4.skip.1.running_mean\", \"layer4.skip.1.running_var\", \"layer4.skip.1.num_batches_tracked\", \"fc.weight\", \"fc.bias\", \"model.4.weight\", \"model.4.bias\", \"model.5.weight\", \"model.5.bias\", \"model.5.running_mean\", \"model.5.running_var\", \"model.5.num_batches_tracked\", \"model.8.weight\", \"model.8.bias\", \"model.9.weight\", \"model.9.bias\", \"model.9.running_mean\", \"model.9.running_var\", \"model.9.num_batches_tracked\", \"model.13.weight\", \"model.13.bias\", \"model.16.weight\", \"model.16.bias\", \"model.0.weight\", \"model.0.bias\", \"model.1.weight\", \"model.1.bias\", \"model.1.running_mean\", \"model.1.running_var\", \"model.1.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m model_path = config[\u001b[33m'\u001b[39m\u001b[33mpath_to_model\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mand\u001b[39;00m os.path.exists(model_path):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m#model = CatsDogsModel.load_from_checkpoint(model_path, map_location=device)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     model = \u001b[43mKaninchenModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded model weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:125\u001b[39m, in \u001b[36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    122\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.method.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be called on an instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1581\u001b[39m, in \u001b[36mLightningModule.load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[39m\n\u001b[32m   1492\u001b[39m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_checkpoint\u001b[39m(\n\u001b[32m   1494\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1499\u001b[39m     **kwargs: Any,\n\u001b[32m   1500\u001b[39m ) -> Self:\n\u001b[32m   1501\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[33;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[32m   1503\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1579\u001b[39m \n\u001b[32m   1580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1581\u001b[39m     loaded = \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1582\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:91\u001b[39m, in \u001b[36m_load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, **kwargs)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl.LightningModule):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     model = \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     state_dict = checkpoint[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:187\u001b[39m, in \u001b[36m_load_state\u001b[39m\u001b[34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[39m\n\u001b[32m    184\u001b[39m     obj.on_load_checkpoint(checkpoint)\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m keys = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_dict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m keys.missing_keys:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for KaninchenModel:\n\tMissing key(s) in state_dict: \"model.0.0.0.weight\", \"model.0.0.0.bias\", \"model.0.0.1.weight\", \"model.0.0.1.bias\", \"model.0.0.1.running_mean\", \"model.0.0.1.running_var\", \"model.0.0.3.weight\", \"model.0.0.3.bias\", \"model.0.0.4.weight\", \"model.0.0.4.bias\", \"model.0.0.4.running_mean\", \"model.0.0.4.running_var\", \"model.0.1.0.weight\", \"model.0.1.0.bias\", \"model.0.1.1.weight\", \"model.0.1.1.bias\", \"model.0.1.1.running_mean\", \"model.0.1.1.running_var\", \"model.0.1.3.weight\", \"model.0.1.3.bias\", \"model.0.1.4.weight\", \"model.0.1.4.bias\", \"model.0.1.4.running_mean\", \"model.0.1.4.running_var\", \"model.0.2.0.weight\", \"model.0.2.0.bias\", \"model.0.2.1.weight\", \"model.0.2.1.bias\", \"model.0.2.1.running_mean\", \"model.0.2.1.running_var\", \"model.0.2.3.weight\", \"model.0.2.3.bias\", \"model.0.2.4.weight\", \"model.0.2.4.bias\", \"model.0.2.4.running_mean\", \"model.0.2.4.running_var\", \"model.0.3.0.weight\", \"model.0.3.0.bias\", \"model.0.3.1.weight\", \"model.0.3.1.bias\", \"model.0.3.1.running_mean\", \"model.0.3.1.running_var\", \"model.0.3.3.weight\", \"model.0.3.3.bias\", \"model.0.3.4.weight\", \"model.0.3.4.bias\", \"model.0.3.4.running_mean\", \"model.0.3.4.running_var\", \"model.1.1.weight\", \"model.1.1.bias\", \"model.1.4.weight\", \"model.1.4.bias\", \"conv1.0.weight\", \"conv1.0.bias\", \"conv1.1.weight\", \"conv1.1.bias\", \"conv1.1.running_mean\", \"conv1.1.running_var\", \"conv1.3.weight\", \"conv1.3.bias\", \"conv1.4.weight\", \"conv1.4.bias\", \"conv1.4.running_mean\", \"conv1.4.running_var\", \"conv2.0.weight\", \"conv2.0.bias\", \"conv2.1.weight\", \"conv2.1.bias\", \"conv2.1.running_mean\", \"conv2.1.running_var\", \"conv2.3.weight\", \"conv2.3.bias\", \"conv2.4.weight\", \"conv2.4.bias\", \"conv2.4.running_mean\", \"conv2.4.running_var\", \"conv3.0.weight\", \"conv3.0.bias\", \"conv3.1.weight\", \"conv3.1.bias\", \"conv3.1.running_mean\", \"conv3.1.running_var\", \"conv3.3.weight\", \"conv3.3.bias\", \"conv3.4.weight\", \"conv3.4.bias\", \"conv3.4.running_mean\", \"conv3.4.running_var\", \"conv4.0.weight\", \"conv4.0.bias\", \"conv4.1.weight\", \"conv4.1.bias\", \"conv4.1.running_mean\", \"conv4.1.running_var\", \"conv4.3.weight\", \"conv4.3.bias\", \"conv4.4.weight\", \"conv4.4.bias\", \"conv4.4.running_mean\", \"conv4.4.running_var\", \"conv5.0.weight\", \"conv5.0.bias\", \"conv5.1.weight\", \"conv5.1.bias\", \"conv5.1.running_mean\", \"conv5.1.running_var\", \"conv5.3.weight\", \"conv5.3.bias\", \"conv5.4.weight\", \"conv5.4.bias\", \"conv5.4.running_mean\", \"conv5.4.running_var\", \"cnn.0.0.weight\", \"cnn.0.0.bias\", \"cnn.0.1.weight\", \"cnn.0.1.bias\", \"cnn.0.1.running_mean\", \"cnn.0.1.running_var\", \"cnn.0.3.weight\", \"cnn.0.3.bias\", \"cnn.0.4.weight\", \"cnn.0.4.bias\", \"cnn.0.4.running_mean\", \"cnn.0.4.running_var\", \"cnn.1.0.weight\", \"cnn.1.0.bias\", \"cnn.1.1.weight\", \"cnn.1.1.bias\", \"cnn.1.1.running_mean\", \"cnn.1.1.running_var\", \"cnn.1.3.weight\", \"cnn.1.3.bias\", \"cnn.1.4.weight\", \"cnn.1.4.bias\", \"cnn.1.4.running_mean\", \"cnn.1.4.running_var\", \"cnn.2.0.weight\", \"cnn.2.0.bias\", \"cnn.2.1.weight\", \"cnn.2.1.bias\", \"cnn.2.1.running_mean\", \"cnn.2.1.running_var\", \"cnn.2.3.weight\", \"cnn.2.3.bias\", \"cnn.2.4.weight\", \"cnn.2.4.bias\", \"cnn.2.4.running_mean\", \"cnn.2.4.running_var\", \"cnn.3.0.weight\", \"cnn.3.0.bias\", \"cnn.3.1.weight\", \"cnn.3.1.bias\", \"cnn.3.1.running_mean\", \"cnn.3.1.running_var\", \"cnn.3.3.weight\", \"cnn.3.3.bias\", \"cnn.3.4.weight\", \"cnn.3.4.bias\", \"cnn.3.4.running_mean\", \"cnn.3.4.running_var\", \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.4.weight\", \"classifier.4.bias\". \n\tUnexpected key(s) in state_dict: \"init_conv.0.weight\", \"init_conv.0.bias\", \"init_conv.1.weight\", \"init_conv.1.bias\", \"init_conv.1.running_mean\", \"init_conv.1.running_var\", \"init_conv.1.num_batches_tracked\", \"layer1.0.weight\", \"layer1.0.bias\", \"layer1.1.weight\", \"layer1.1.bias\", \"layer1.1.running_mean\", \"layer1.1.running_var\", \"layer1.1.num_batches_tracked\", \"layer1.3.weight\", \"layer1.3.bias\", \"layer1.4.weight\", \"layer1.4.bias\", \"layer1.4.running_mean\", \"layer1.4.running_var\", \"layer1.4.num_batches_tracked\", \"layer2.0.weight\", \"layer2.0.bias\", \"layer2.1.weight\", \"layer2.1.bias\", \"layer2.1.running_mean\", \"layer2.1.running_var\", \"layer2.1.num_batches_tracked\", \"layer2.3.weight\", \"layer2.3.bias\", \"layer2.4.weight\", \"layer2.4.bias\", \"layer2.4.running_mean\", \"layer2.4.running_var\", \"layer2.4.num_batches_tracked\", \"layer3.0.weight\", \"layer3.0.bias\", \"layer3.1.weight\", \"layer3.1.bias\", \"layer3.1.running_mean\", \"layer3.1.running_var\", \"layer3.1.num_batches_tracked\", \"layer3.3.weight\", \"layer3.3.bias\", \"layer3.4.weight\", \"layer3.4.bias\", \"layer3.4.running_mean\", \"layer3.4.running_var\", \"layer3.4.num_batches_tracked\", \"layer4.conv_block.0.weight\", \"layer4.conv_block.0.bias\", \"layer4.conv_block.1.weight\", \"layer4.conv_block.1.bias\", \"layer4.conv_block.1.running_mean\", \"layer4.conv_block.1.running_var\", \"layer4.conv_block.1.num_batches_tracked\", \"layer4.conv_block.3.weight\", \"layer4.conv_block.3.bias\", \"layer4.conv_block.4.weight\", \"layer4.conv_block.4.bias\", \"layer4.conv_block.4.running_mean\", \"layer4.conv_block.4.running_var\", \"layer4.conv_block.4.num_batches_tracked\", \"layer4.skip.0.weight\", \"layer4.skip.0.bias\", \"layer4.skip.1.weight\", \"layer4.skip.1.bias\", \"layer4.skip.1.running_mean\", \"layer4.skip.1.running_var\", \"layer4.skip.1.num_batches_tracked\", \"fc.weight\", \"fc.bias\", \"model.4.weight\", \"model.4.bias\", \"model.5.weight\", \"model.5.bias\", \"model.5.running_mean\", \"model.5.running_var\", \"model.5.num_batches_tracked\", \"model.8.weight\", \"model.8.bias\", \"model.9.weight\", \"model.9.bias\", \"model.9.running_mean\", \"model.9.running_var\", \"model.9.num_batches_tracked\", \"model.13.weight\", \"model.13.bias\", \"model.16.weight\", \"model.16.bias\", \"model.0.weight\", \"model.0.bias\", \"model.1.weight\", \"model.1.bias\", \"model.1.running_mean\", \"model.1.running_var\", \"model.1.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "model_path = config['path_to_model']\n",
    "if model_path and os.path.exists(model_path):\n",
    "    #model = CatsDogsModel.load_from_checkpoint(model_path, map_location=device)\n",
    "    model = KaninchenModel.load_from_checkpoint(model_path, map_location=device)\n",
    "    print(f\"Loaded model weights from {model_path}\")\n",
    "else:\n",
    "    print(\"Model path not found or not specified in config.\")\n",
    "\n",
    "# Ensure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Pytorch Lightning's Trainer can be used to test the model\n",
    "trainer = Trainer()\n",
    "trainer.test(model=model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDKI-Projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
