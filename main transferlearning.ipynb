{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4845e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryPrecisionRecallCurve\n",
    "from torchvision.transforms import v2\n",
    "from torchinfo import summary\n",
    "import timm\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.datamodule import BinaryImageDataModule\n",
    "from training.hyperparameter_tuning import OptunaTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117666f9",
   "metadata": {},
   "source": [
    "### Loading Configuration\n",
    "\n",
    "In the following steps, we will load the configuration settings using the `load_configuration` function. The configuration is stored in the `config` variable which will be used throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74321032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Name: DESKTOP-LUKAS\n",
      "Loaded configuration from config/config_lukas.yaml\n"
     ]
    }
   ],
   "source": [
    "from config.load_configuration import load_configuration\n",
    "config = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d537e99",
   "metadata": {},
   "source": [
    "### Logging in to Weights & Biases (wandb)\n",
    "\n",
    "Before starting any experiment tracking, ensure you are logged in to your Weights & Biases (wandb) account. This enables automatic logging of metrics, model checkpoints, and experiment configurations. The following code logs you in to wandb:\n",
    "\n",
    "```python\n",
    "wandb.login()\n",
    "```\n",
    "If you are running this for the first time, you may be prompted to enter your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90bd2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukas-pelz\u001b[0m (\u001b[33mHKA-EKG-Signalverarbeitung\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Wandb logger\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414378fc",
   "metadata": {},
   "source": [
    "### Setting Seeds for Reproducibility\n",
    "\n",
    "To ensure comparable and reproducible results, we set the random seed using the `seed_everything` function from PyTorch Lightning. This helps in achieving consistent behavior across multiple runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06e10d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # disable oneDNN optimizations for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86df64",
   "metadata": {},
   "source": [
    "### Checking for GPU Devices\n",
    "\n",
    "In this step, we check for the availability of GPU devices and print the device currently being used by PyTorch. This ensures that the computations are performed on the most efficient hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b717a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.7.0+cu128\n",
      "Using device:  cuda\n",
      "Cuda Version:  12.8\n",
      "NVIDIA GeForce RTX 5060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Torch Version: ', torch.__version__)\n",
    "print('Using device: ', device)\n",
    "if device.type == 'cuda':\n",
    "    print('Cuda Version: ', torch.version.cuda)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c560cb9",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "In this step, we will define the model architecture and print its summary using the `ModelSummary` utility from PyTorch Lightning. This provides an overview of the model's layers, parameters, and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937521f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0f7b0",
   "metadata": {},
   "source": [
    "### Transfer Learning with EfficientNet_B3\n",
    "\n",
    "In this step, we utilize the EfficientNet_B3 model for transfer learning. The model is pre-trained on ImageNet, and we adapt it to our specific task by modifying the classifier layer to match the number of output classes (`num_classes`). \n",
    "\n",
    "We freeze all layers except the classifier to retain the pre-trained features while allowing the classifier to learn task-specific features. The model summary provides an overview of the architecture and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3442708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEfficientNetB3_model(amount_of_trainable_linear_layers=1):\n",
    "    \"\"\"\n",
    "    Function to get the EfficientNet B3 model with pretrained weights.\n",
    "    Returns:\n",
    "        model: A PyTorch model instance of EfficientNet B3.\n",
    "    \"\"\"\n",
    "    # Load the EfficientNet B3 model with pretrained weights\n",
    "    model = timm.create_model('efficientnet_b3', pretrained=True)\n",
    "    \n",
    "    # Modify the classifier for binary classification\n",
    "    num_classes = 1  # For binary classification (OK/NOK)\n",
    "    if amount_of_trainable_linear_layers == 1:\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n",
    "    elif amount_of_trainable_linear_layers == 2:\n",
    "        # If two linear layers are trainable, we add an intermediate layer\n",
    "        model.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),  # Add dropout for regularization\n",
    "            torch.nn.Linear(model.classifier.in_features, 256),  # Intermediate layer\n",
    "            torch.nn.ReLU(),  # Activation function\n",
    "            torch.nn.Dropout(p=0.2),  # Another dropout layer\n",
    "            torch.nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    # Freeze all layers except the classifier\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model, \"TL_EfficientNetB3\"\n",
    "\n",
    "# Wrap the model in the LightningModule\n",
    "# from models.model_transferlearning import TransferLearningModule\n",
    "# lightning_model = TransferLearningModule(model, config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a93c6",
   "metadata": {},
   "source": [
    "### Data Preparation for EfficientNet_B3\n",
    "\n",
    "To prepare the data for training with EfficientNet_B3, we define a set of image transformations that resize all images to 300x300 pixels, convert them to tensors, and normalize them using the standard ImageNet mean and standard deviation. These transformations ensure compatibility with the input requirements of the EfficientNet architecture.\n",
    "\n",
    "We then instantiate the `BinaryCIFARDataModule` with the defined transformations, batch size, and number of workers from the configuration. After setup, we create the training, validation, and test data loaders. The sizes of each dataset split are printed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcaac6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Happens within hyperparameter tuning\n",
    "\n",
    "# import data.custom_transforms as custom_transforms\n",
    "# size = 128                                              # Size for the input images, matching EfficientNet input size\n",
    "# transform = v2.Compose([\n",
    "#     custom_transforms.CenterCropSquare(),\n",
    "#     v2.Resize((size, size)),\n",
    "#     v2.ToTensor(),\n",
    "#     v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# dm = BinaryImageDataModule(data_dir=config['path_to_split_aug_pics'], transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "# dm.setup()\n",
    "# train_loader = dm.train_dataloader()\n",
    "# val_loader = dm.val_dataloader()\n",
    "# test_loader = dm.test_dataloader()\n",
    "\n",
    "# print('Train dataset size:', len(dm.train_dataset))\n",
    "# print('Validation dataset size:', len(dm.val_dataset))\n",
    "# print('Test dataset size:', len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005d275",
   "metadata": {},
   "source": [
    "### Training and Logging with Weights & Biases including Hyper-Parameter Tuning\n",
    "\n",
    "In this step, we initialize the Weights & Biases (wandb) logger to track experiment metrics, hyperparameters, and model checkpoints. The logger is configured with project and experiment names, as well as key training parameters such as dataset, batch size, maximum epochs, and learning rate.\n",
    "\n",
    "We then set up the PyTorch Lightning `Trainer` with the wandb logger and an early stopping callback to monitor validation loss. The model is trained using the specified datamodule, and all relevant metrics are automatically logged to wandb for further analysis and visualization. After training, wandb logging is finalized to ensure all data is properly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b648e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 10:14:38,829] A new study created in memory with name: no-name-e2484be4-e7a6-4732-b792-c1badb1a319b\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250531_101439-oqn4ly1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/oqn4ly1o' target=\"_blank\">TL_bs32_img192_optAdamW_lr3e-04_wd1e-03_sch_CosineAnnealingLR_cls1_modelEfficientNetB3_2025-05-31_10-14</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/oqn4ly1o' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/oqn4ly1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "config['sweep_id'] = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "def objective(trial):\n",
    "    model_fct = getEfficientNetB3_model\n",
    "    trainer = OptunaTrainer(\n",
    "        model=model_fct,                        # Function to create the model\n",
    "        config=config,\n",
    "        normalize_mean=[0.485, 0.456, 0.406], \n",
    "        normalize_std=[0.229, 0.224, 0.225],\n",
    "        dataset_name=\"DwarfRabbits-binary\"\n",
    "    )\n",
    "    return trainer.run_training(trial)\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")  # because we minimize val_loss\n",
    "\n",
    "# Set verbosity to WARNING to reduce output clutter\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Start the hyperparameter optimization\n",
    "# study.optimize(objective, n_trials=config['number_of_trials'])\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Best result\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best value (val_loss):\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47ec704",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate parameter importances with only a single trial.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m optuna.visualization.plot_optimization_history(study)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43moptuna\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot_param_importances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\visualization\\_param_importances.py:168\u001b[39m, in \u001b[36mplot_param_importances\u001b[39m\u001b[34m(study, evaluator, params, target, target_name)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Plot hyperparameter importances.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m \u001b[33;03m.. seealso::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m \u001b[33;03m    A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    167\u001b[39m _imports.check()\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m importances_infos = \u001b[43m_get_importances_infos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_importances_plot(importances_infos, study)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\visualization\\_param_importances.py:82\u001b[39m, in \u001b[36m_get_importances_infos\u001b[39m\u001b[34m(study, evaluator, params, target, target_name)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m study._is_multi_objective():\n\u001b[32m     80\u001b[39m     target_name = metric_names[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m metric_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target \u001b[38;5;28;01melse\u001b[39;00m target_name\n\u001b[32m     81\u001b[39m     importances_infos: \u001b[38;5;28mtuple\u001b[39m[_ImportancesInfo, ...] = (\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[43m_get_importances_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     92\u001b[39m     n_objectives = \u001b[38;5;28mlen\u001b[39m(study.directions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\visualization\\_param_importances.py:54\u001b[39m, in \u001b[36m_get_importances_info\u001b[39m\u001b[34m(study, evaluator, params, target, target_name)\u001b[39m\n\u001b[32m     46\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mStudy instance does not contain completed trials.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _ImportancesInfo(\n\u001b[32m     48\u001b[39m         importance_values=[],\n\u001b[32m     49\u001b[39m         param_names=[],\n\u001b[32m     50\u001b[39m         importance_labels=[],\n\u001b[32m     51\u001b[39m         target_name=target_name,\n\u001b[32m     52\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m importances = \u001b[43moptuna\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimportance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_param_importances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m importances = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(importances.items())))\n\u001b[32m     59\u001b[39m importance_values = \u001b[38;5;28mlist\u001b[39m(importances.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\importance\\__init__.py:111\u001b[39m, in \u001b[36mget_param_importances\u001b[39m\u001b[34m(study, evaluator, params, target, normalize)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluator, BaseImportanceEvaluator):\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEvaluator must be a subclass of BaseImportanceEvaluator.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m res = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[32m    113\u001b[39m     s = \u001b[38;5;28msum\u001b[39m(res.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\importance\\_fanova\\_evaluator.py:89\u001b[39m, in \u001b[36mFanovaImportanceEvaluator.evaluate\u001b[39m\u001b[34m(self, study, params, target)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m study._is_multi_objective():\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     84\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf the `study` is being used for multi-objective optimization, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease specify the `target`. For example, use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`target=lambda t: t.values[0]` for the first objective value.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m distributions = \u001b[43m_get_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     params = \u001b[38;5;28mlist\u001b[39m(distributions.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\importance\\_base.py:69\u001b[39m, in \u001b[36m_get_distributions\u001b[39m\u001b[34m(study, params)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_distributions\u001b[39m(study: Study, params: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseDistribution]:\n\u001b[32m     68\u001b[39m     completed_trials = study.get_trials(deepcopy=\u001b[38;5;28;01mFalse\u001b[39;00m, states=(TrialState.COMPLETE,))\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[43m_check_evaluate_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompleted_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     72\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m intersection_search_space(study.get_trials(deepcopy=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\importance\\_base.py:114\u001b[39m, in \u001b[36m_check_evaluate_args\u001b[39m\u001b[34m(completed_trials, params)\u001b[39m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot evaluate parameter importances without completed trials.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(completed_trials) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot evaluate parameter importances with only a single trial.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[31mValueError\u001b[39m: Cannot evaluate parameter importances with only a single trial."
     ]
    }
   ],
   "source": [
    "\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import data.custom_transforms as custom_transforms\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     # Data related hyperparameters    \n",
    "#     batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 192])\n",
    "#     image_size = trial.suggest_categorical(\"image_size\", [128, 192, 256])\n",
    "\n",
    "#     # Trainer hyperparameters\n",
    "#     max_epochs = trial.suggest_int(\"max_epochs\", 10, 50)                                            # Maximum number of epochs to train the model\n",
    "#     accumulate_grad_batches = trial.suggest_categorical(\"accumulate_grad_batches\", [1, 2, 4])       # Accumulate gradients over multiple batches before updating weights\n",
    "#     precision = trial.suggest_categorical(\"precision\", [\"16-mixed\", 32])                                    # Precision for training, 16 for mixed precision, 32 for full precision\n",
    "\n",
    "#     # Optimizer hyperparameters\n",
    "#     optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"])               # Optimizer type\n",
    "#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)                      # Learning rate (size of the step taken in the direction of the gradient)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)                        # Weight decay (L2 penality on the weights to prevent overfitting by discouraging large weights)\n",
    "\n",
    "#     # Learning rate scheduler hyperparameters\n",
    "#     scheduler_name = trial.suggest_categorical(\"scheduler\", [\"StepLR\", \"CosineAnnealingLR\", None])  # Learning rate scheduler type\n",
    "\n",
    "#     # Model related hyperparameters\n",
    "#     # dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    \n",
    "#     # Update config\n",
    "#     config['batch_size'] = batch_size\n",
    "#     config['image_size'] = image_size\n",
    "\n",
    "#     config['max_epochs'] = max_epochs\n",
    "#     config['accumulate_grad_batches'] = accumulate_grad_batches\n",
    "#     config['precision'] = precision\n",
    "\n",
    "#     config['optimizer'] = optimizer_name\n",
    "#     config['learning_rate'] = learning_rate\n",
    "#     config['weight_decay'] = weight_decay\n",
    "\n",
    "#     config['scheduler'] = scheduler_name\n",
    "\n",
    "#     config['wandb_experiment_name'] = (\n",
    "#         f\"TL_bs{batch_size}\"\n",
    "#         f\"_img{image_size}\"\n",
    "#         f\"_opt{optimizer_name}\"\n",
    "#         f\"_lr{learning_rate:.0e}\"\n",
    "#         f\"_wd{weight_decay:.0e}\"\n",
    "#         f\"_sch_{scheduler_name if scheduler_name else 'None'}\"\n",
    "#         f\"_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "#     )\n",
    "\n",
    "#     # WandB logger\n",
    "#     wandb_logger = WandbLogger(\n",
    "#         project=config['wandb_project_name'],\n",
    "#         name=config['wandb_experiment_name'],\n",
    "#         config={\n",
    "#             'batch_size': config['batch_size'],\n",
    "#             'image_size': config['image_size'],\n",
    "#             'max_epochs': config['max_epochs'],\n",
    "#             'accumulate_grad_batches': config['accumulate_grad_batches'],\n",
    "#             'precision': config['precision'],\n",
    "#             'optimizer': config['optimizer'],\n",
    "#             'learning_rate': config['learning_rate'],\n",
    "#             'weight_decay': config['weight_decay'],\n",
    "#             'scheduler': config['scheduler'],\n",
    "#             'dataset': 'DwarfRabbits-binary'\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # Create model and datamodule\n",
    "#     lightning_model = TransferLearningModule(\n",
    "#         model, \n",
    "#         learning_rate=config['learning_rate'],\n",
    "#         optimizer_name=config['optimizer'],\n",
    "#         weight_decay=config['weight_decay'],\n",
    "#         scheduler_name=config['scheduler'],\n",
    "#         )\n",
    "    \n",
    "#     # Define transformations                                       \n",
    "#     transform = v2.Compose([\n",
    "#         custom_transforms.CenterCropSquare(),\n",
    "#         v2.Resize((image_size, image_size)),\n",
    "#         v2.ToTensor(),\n",
    "#         v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     dm = dm = BinaryImageDataModule(\n",
    "#         data_dir=config['path_to_split_aug_pics'], \n",
    "#         transform=transform, \n",
    "#         batch_size=config['batch_size'], \n",
    "#         num_workers=2, \n",
    "#         persistent_workers=True\n",
    "#     )\n",
    "\n",
    "#     # Trainer\n",
    "#     trainer = Trainer(\n",
    "#         max_epochs=config['max_epochs'],\n",
    "#         precision=config['precision'],\n",
    "#         accumulate_grad_batches=config['accumulate_grad_batches'],\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=\"auto\",\n",
    "#         strategy=\"auto\",\n",
    "#         callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3, mode='min')],\n",
    "#         logger=wandb_logger,\n",
    "#         enable_progress_bar=False,  # Speeds up Optuna trials\n",
    "#         log_every_n_steps=10,  # Log every 10 steps\n",
    "#     )\n",
    "\n",
    "#     # Train\n",
    "#     try:\n",
    "#         trainer.fit(model=lightning_model, datamodule=dm)\n",
    "#         checkpoint_path = f\"checkpoints/{config['wandb_experiment_name']}.ckpt\"\n",
    "#         trainer.save_checkpoint(checkpoint_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during training: {e}\")\n",
    "#         wandb.finish()\n",
    "#         return float(\"inf\")\n",
    "\n",
    "#     # Get best validation score\n",
    "#     val_loss = trainer.callback_metrics.get(\"val_loss\")\n",
    "#     wandb.finish()\n",
    "\n",
    "#     # Return float for Optuna to minimize\n",
    "#     return val_loss.item() if val_loss else float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdea599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_105023-cxb8avx1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1' target=\"_blank\">binaryClassification_tl_2025-05-27_10-50-23</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 12/12 [00:26<00:00,  0.45it/s, v_num=avx1, val_loss=0.360]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>█▆▂▁▂▃▄</td></tr><tr><td>epoch</td><td>▁▁▂▂▄▄▅▅▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▄▄▆██</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▆▆▆▆▇▇▇██</td></tr><tr><td>val_acc</td><td>█▁▂▅▅▅</td></tr><tr><td>val_loss</td><td>▁██▇▆▄</td></tr><tr><td>val_precision</td><td>█▅▁▇▇█</td></tr><tr><td>val_recall</td><td>█▁▅▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.90943</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>train_acc</td><td>0.87008</td></tr><tr><td>train_loss</td><td>0.33155</td></tr><tr><td>trainer/global_step</td><td>71</td></tr><tr><td>val_acc</td><td>0.846</td></tr><tr><td>val_loss</td><td>0.36006</td></tr><tr><td>val_precision</td><td>0.80137</td></tr><tr><td>val_recall</td><td>0.71779</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">binaryClassification_tl_2025-05-27_10-50-23</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 21 media file(s), 28 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_105023-cxb8avx1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved as C:\\Users\\lukas\\SynologyDrive_IMS/SS25_MSYS_KAER-AI-PoseAct/21_Test_Data/Models/CNN\\binaryClassification_tl_2025-05-27_10-50-23.ckpt\n"
     ]
    }
   ],
   "source": [
    "# # Initialize the Wandb logger\n",
    "# # add time to the name of the experiment\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# # Initialize wandb logger\n",
    "# wandb_logger = WandbLogger(\n",
    "#     project=config['wandb_project_name'],\n",
    "#     name=config['wandb_experiment_name'] + '_' + current_time,\n",
    "#     config={\n",
    "#         'dataset': 'CIFAR-binary',\n",
    "#         'batch_size': config['batch_size'],\n",
    "#         'max_epochs': config['max_epochs'],\n",
    "#         'learning_rate': config['learning_rate']\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Train the model and log relevant metrics using PyTorch Lightning Trainer and WandbLogger\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=config['max_epochs'],\n",
    "#     default_root_dir='model/checkpoint/',\n",
    "#     accelerator=\"auto\",\n",
    "#     devices=\"auto\",\n",
    "#     strategy=\"auto\",\n",
    "#     callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n",
    "#     logger=wandb_logger\n",
    "# )\n",
    "\n",
    "# # Train of the model\n",
    "# trainer.fit(model=lightning_model, datamodule=dm)\n",
    "\n",
    "# # Finish wandb logging\n",
    "# wandb.finish()\n",
    "\n",
    "# # Create a filename with date identifier\n",
    "# model_filename = f\"{config['wandb_experiment_name']}_{current_time}.ckpt\"\n",
    "\n",
    "# # Save the model's state_dict to the path specified in config\n",
    "# save_path = os.path.join(os.path.dirname(config['path_to_models']), model_filename)\n",
    "# trainer.save_checkpoint(save_path)\n",
    "# print(f\"Model checkpoint saved as {save_path}\")\n",
    "# config['path_to_model'] = save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97973a8",
   "metadata": {},
   "source": [
    "### Transfer Learning with ResNet50D\n",
    "\n",
    "In this step, we utilize the ResNet50D model for transfer learning. The model is pre-trained on ImageNet, and we adapt it to our specific task by modifying the fully connected (`fc`) layer to match the number of output classes (`num_classes`).\n",
    "\n",
    "We freeze all layers except the `fc` layer to retain the pre-trained features while allowing the classifier to learn task-specific features. The model summary provides an overview of the architecture and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a00428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 1]                    --\n",
       "├─Sequential: 1-1                        [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 112, 112]         (864)\n",
       "│    └─BatchNorm2d: 2-2                  [1, 32, 112, 112]         (64)\n",
       "│    └─ReLU: 2-3                         [1, 32, 112, 112]         --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 112, 112]         (9,216)\n",
       "│    └─BatchNorm2d: 2-5                  [1, 32, 112, 112]         (64)\n",
       "│    └─ReLU: 2-6                         [1, 32, 112, 112]         --\n",
       "│    └─Conv2d: 2-7                       [1, 64, 112, 112]         (18,432)\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck: 2-8                   [1, 256, 56, 56]          (75,008)\n",
       "│    └─Bottleneck: 2-9                   [1, 256, 56, 56]          (70,400)\n",
       "│    └─Bottleneck: 2-10                  [1, 256, 56, 56]          (70,400)\n",
       "├─Sequential: 1-6                        [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-11                  [1, 512, 28, 28]          (379,392)\n",
       "│    └─Bottleneck: 2-12                  [1, 512, 28, 28]          (280,064)\n",
       "│    └─Bottleneck: 2-13                  [1, 512, 28, 28]          (280,064)\n",
       "│    └─Bottleneck: 2-14                  [1, 512, 28, 28]          (280,064)\n",
       "├─Sequential: 1-7                        [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-15                  [1, 1024, 14, 14]         (1,512,448)\n",
       "│    └─Bottleneck: 2-16                  [1, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Bottleneck: 2-17                  [1, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Bottleneck: 2-18                  [1, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Bottleneck: 2-19                  [1, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Bottleneck: 2-20                  [1, 1024, 14, 14]         (1,117,184)\n",
       "├─Sequential: 1-8                        [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck: 2-21                  [1, 2048, 7, 7]           (6,039,552)\n",
       "│    └─Bottleneck: 2-22                  [1, 2048, 7, 7]           (4,462,592)\n",
       "│    └─Bottleneck: 2-23                  [1, 2048, 7, 7]           (4,462,592)\n",
       "├─SelectAdaptivePool2d: 1-9              [1, 2048]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-24           [1, 2048, 1, 1]           --\n",
       "│    └─Flatten: 2-25                     [1, 2048]                 --\n",
       "├─Linear: 1-10                           [1, 1]                    (2,049)\n",
       "==========================================================================================\n",
       "Total params: 23,529,313\n",
       "Trainable params: 0\n",
       "Non-trainable params: 23,529,313\n",
       "Total mult-adds (Units.GIGABYTES): 4.33\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 190.67\n",
       "Params size (MB): 94.12\n",
       "Estimated Total Size (MB): 285.39\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load pretrained model\n",
    "# model = timm.create_model(\n",
    "#     'resnet50d',      # Hardcoded for now\n",
    "#     pretrained=True,\n",
    "# )\n",
    "# # Define number of classes and classifier\n",
    "# num_classes = 1             # Hardcoded for now, Dwarf Rabbit OK/NOK output    \n",
    "# model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# # Freeze all layers except the classifier\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Print model summary\n",
    "# summary(model, input_size=(1, 3, 224, 224), depth=2)\n",
    "\n",
    "# # Wrap the model in the LightningModule\n",
    "# from models.model_transferlearning import TransferLearningModule\n",
    "# lightning_model = TransferLearningModule(model, config['learning_rate'])\n",
    "\n",
    "# Wrap the model in the LightningModule\n",
    "# from models.model_transferlearning import TransferLearningModule\n",
    "# lightning_model = TransferLearningModule(model, config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResNet50D_model(amount_of_trainable_linear_layers=1):\n",
    "\n",
    "    # Load the EfficientNet B3 model with pretrained weights\n",
    "    model = timm.create_model('resnet50d', pretrained=True)\n",
    "    \n",
    "    # Modify the classifier for binary classification\n",
    "    num_classes = 1  # For binary classification (OK/NOK)\n",
    "    if amount_of_trainable_linear_layers == 1:\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif amount_of_trainable_linear_layers == 2:\n",
    "        # If two linear layers are trainable, we add an intermediate layer\n",
    "        model.fc = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),                        # Add dropout for regularization\n",
    "            torch.nn.Linear(model.fc.in_features, 256),     # Intermediate layer\n",
    "            torch.nn.ReLU(),                                # Activation function\n",
    "            torch.nn.Dropout(p=0.2),                        # Another dropout layer\n",
    "            torch.nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    # Freeze all layers except the classifier\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model, \"TL_ResNet50D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460f881",
   "metadata": {},
   "source": [
    "### Data Preparation for ResNet50D\n",
    "\n",
    "To prepare the data for training with ResNet50D, we define a set of image transformations that resize all images to 224x224 pixels, convert them to tensors, and normalize them using the standard ImageNet mean and standard deviation. These transformations ensure compatibility with the input requirements of the ResNet architecture.\n",
    "\n",
    "We then instantiate the `BinaryCIFARDataModule` with the defined transformations, batch size, and number of workers from the configuration. After setup, we create the training, validation, and test data loaders. The sizes of each dataset split are printed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations required for the used model\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize images to match ResNet input size\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# dm = BinaryCIFARDataModule(transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "# dm.setup()\n",
    "# train_loader = dm.train_dataloader()\n",
    "# val_loader = dm.val_dataloader()\n",
    "# test_loader = dm.test_dataloader()\n",
    "\n",
    "# print('Train dataset size:', len(dm.train_dataset))\n",
    "# print('Validation dataset size:', len(dm.val_dataset))\n",
    "# print('Test dataset size:', len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398ae57",
   "metadata": {},
   "source": [
    "### Training and Logging with Weights & Biases\n",
    "\n",
    "In this step, we initialize the Weights & Biases (wandb) logger to track experiment metrics, hyperparameters, and model checkpoints. The logger is configured with project and experiment names, as well as key training parameters such as dataset, batch size, maximum epochs, and learning rate.\n",
    "\n",
    "We then set up the PyTorch Lightning `Trainer` with the wandb logger and an early stopping callback to monitor validation loss. The model is trained using the specified datamodule, and all relevant metrics are automatically logged to wandb for further analysis and visualization. After training, wandb logging is finalized to ensure all data is properly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e566d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the Wandb logger\n",
    "# wandb_logger = WandbLogger(\n",
    "#     project=config['wandb_project_name'],\n",
    "#     name=config['wandb_experiment_name'],\n",
    "#     config={\n",
    "#         'dataset': 'CIFAR-binary',\n",
    "#         'batch_size': config['batch_size'],\n",
    "#         'max_epochs': config['max_epochs'],\n",
    "#         'learning_rate': config['learning_rate']\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Train the model and log relevant metrics using PyTorch Lightning Trainer and WandbLogger\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=config['max_epochs'],\n",
    "#     default_root_dir='model/checkpoint/',\n",
    "#     accelerator=\"auto\",\n",
    "#     devices=\"auto\",\n",
    "#     strategy=\"auto\",\n",
    "#     callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n",
    "#     logger=wandb_logger\n",
    "# )\n",
    "\n",
    "# trainer.fit(model=lightning_model, datamodule=dm)\n",
    "\n",
    "# # Finish wandb logging\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672aec7a",
   "metadata": {},
   "source": [
    "### Transfer Learning with InceptionV4\n",
    "\n",
    "In this step, we utilize the InceptionV4 model for transfer learning. The model is pre-trained on ImageNet, and we adapt it to our specific task by modifying the `last_linear` layer to match the number of output classes (`num_classes`).\n",
    "\n",
    "We freeze all layers except the `last_linear` layer to retain the pre-trained features while allowing the classifier to learn task-specific features. The model summary provides an overview of the architecture and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a6fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "InceptionV4                                        [1, 1]                    --\n",
       "├─Sequential: 1-1                                  [1, 1536, 8, 8]           --\n",
       "│    └─ConvNormAct: 2-1                            [1, 32, 149, 149]         (928)\n",
       "│    └─ConvNormAct: 2-2                            [1, 32, 147, 147]         (9,280)\n",
       "│    └─ConvNormAct: 2-3                            [1, 64, 147, 147]         (18,560)\n",
       "│    └─Mixed3a: 2-4                                [1, 160, 73, 73]          (55,488)\n",
       "│    └─Mixed4a: 2-5                                [1, 192, 71, 71]          (189,312)\n",
       "│    └─Mixed5a: 2-6                                [1, 384, 35, 35]          (332,160)\n",
       "│    └─InceptionA: 2-7                             [1, 384, 35, 35]          (317,632)\n",
       "│    └─InceptionA: 2-8                             [1, 384, 35, 35]          (317,632)\n",
       "│    └─InceptionA: 2-9                             [1, 384, 35, 35]          (317,632)\n",
       "│    └─InceptionA: 2-10                            [1, 384, 35, 35]          (317,632)\n",
       "│    └─ReductionA: 2-11                            [1, 1024, 17, 17]         (2,306,112)\n",
       "│    └─InceptionB: 2-12                            [1, 1024, 17, 17]         (2,936,256)\n",
       "│    └─InceptionB: 2-13                            [1, 1024, 17, 17]         (2,936,256)\n",
       "│    └─InceptionB: 2-14                            [1, 1024, 17, 17]         (2,936,256)\n",
       "│    └─InceptionB: 2-15                            [1, 1024, 17, 17]         (2,936,256)\n",
       "│    └─InceptionB: 2-16                            [1, 1024, 17, 17]         (2,936,256)\n",
       "│    └─InceptionB: 2-17                            [1, 1024, 17, 17]         (2,936,256)\n",
       "│    └─InceptionB: 2-18                            [1, 1024, 17, 17]         (2,936,256)\n",
       "│    └─ReductionB: 2-19                            [1, 1536, 8, 8]           (2,747,392)\n",
       "│    └─InceptionC: 2-20                            [1, 1536, 8, 8]           (4,553,088)\n",
       "│    └─InceptionC: 2-21                            [1, 1536, 8, 8]           (4,553,088)\n",
       "│    └─InceptionC: 2-22                            [1, 1536, 8, 8]           (4,553,088)\n",
       "├─SelectAdaptivePool2d: 1-2                        [1, 1536]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-23                     [1, 1536, 1, 1]           --\n",
       "│    └─Flatten: 2-24                               [1, 1536]                 --\n",
       "├─Dropout: 1-3                                     [1, 1536]                 --\n",
       "├─Linear: 1-4                                      [1, 1]                    (1,537)\n",
       "====================================================================================================\n",
       "Total params: 41,144,353\n",
       "Trainable params: 0\n",
       "Non-trainable params: 41,144,353\n",
       "Total mult-adds (Units.GIGABYTES): 12.25\n",
       "====================================================================================================\n",
       "Input size (MB): 1.07\n",
       "Forward/backward pass size (MB): 120.71\n",
       "Params size (MB): 164.32\n",
       "Estimated Total Size (MB): 286.11\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load pretrained model\n",
    "# model = timm.create_model(\n",
    "#     'inception_v4',      # Hardcoded for now\n",
    "#     pretrained=True,\n",
    "# )\n",
    "# # Define number of classes and classifier\n",
    "# num_classes = 1             # Hardcoded for now, Dwarf Rabbit OK/NOK output    \n",
    "# model.last_linear = torch.nn.Linear(model.last_linear.in_features, num_classes)\n",
    "\n",
    "# # Freeze all layers except the classifier\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.last_linear.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Print model summary\n",
    "# summary(model, input_size=(1, 3, 299, 299), depth=2)\n",
    "\n",
    "# # Wrap the model in the LightningModule\n",
    "# from models.model_transferlearning import TransferLearningModule\n",
    "# lightning_model = TransferLearningModule(model, config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c97bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getInceptionV4_model(amount_of_trainable_linear_layers=1):\n",
    "\n",
    "    # Load the EfficientNet B3 model with pretrained weights\n",
    "    model = timm.create_model('inception_v4', pretrained=True)\n",
    "    \n",
    "    # Modify the classifier for binary classification\n",
    "    num_classes = 1  # For binary classification (OK/NOK)\n",
    "    if amount_of_trainable_linear_layers == 1:\n",
    "        model.last_linear = torch.nn.Linear(model.last_linear.in_features, num_classes)\n",
    "    elif amount_of_trainable_linear_layers == 2:\n",
    "        model.last_linear = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),                                # Add dropout for regularization\n",
    "            torch.nn.Linear(model.last_linear.in_features, 256),    # Intermediate layer\n",
    "            torch.nn.ReLU(),                                        # Activation function\n",
    "            torch.nn.Dropout(p=0.2),                                # Another dropout layer\n",
    "            torch.nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    # Freeze all layers except the classifier\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.last_linear.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model, \"TL_InceptionV4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626bf0f",
   "metadata": {},
   "source": [
    "### Data Preparation for InceptionV4\n",
    "\n",
    "To prepare the data for training with InceptionV4, we define a set of image transformations that resize all images to 299x299 pixels, convert them to tensors, and normalize them using a mean and standard deviation of 0.5 for each channel. These transformations ensure compatibility with the input requirements of the InceptionV4 architecture.\n",
    "\n",
    "We then instantiate the `BinaryCIFARDataModule` with the defined transformations, batch size, and number of workers from the configuration. After setup, we create the training, validation, and test data loaders. The sizes of each dataset split are printed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75954051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define transformations required for the used model\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((299, 299)),  # Resize images to match EfficientNet input size\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Standard ImageNet normalization\n",
    "# ])\n",
    "\n",
    "# dm = BinaryCIFARDataModule(transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "# dm.setup()\n",
    "# train_loader = dm.train_dataloader()\n",
    "# val_loader = dm.val_dataloader()\n",
    "# test_loader = dm.test_dataloader()\n",
    "\n",
    "# print('Train dataset size:', len(dm.train_dataset))\n",
    "# print('Validation dataset size:', len(dm.val_dataset))\n",
    "# print('Test dataset size:', len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e1ff0",
   "metadata": {},
   "source": [
    "### Training and Logging with Weights & Biases\n",
    "\n",
    "In this step, we initialize the Weights & Biases (wandb) logger to track experiment metrics, hyperparameters, and model checkpoints. The logger is configured with project and experiment names, as well as key training parameters such as dataset, batch size, maximum epochs, and learning rate.\n",
    "\n",
    "We then set up the PyTorch Lightning `Trainer` with the wandb logger and an early stopping callback to monitor validation loss. The model is trained using the specified datamodule, and all relevant metrics are automatically logged to wandb for further analysis and visualization. After training, wandb logging is finalized to ensure all data is properly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b83af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the Wandb logger\n",
    "# wandb_logger = WandbLogger(\n",
    "#     project=config['wandb_project_name'],\n",
    "#     name=config['wandb_experiment_name'],\n",
    "#     config={\n",
    "#         'dataset': 'CIFAR-binary',\n",
    "#         'batch_size': config['batch_size'],\n",
    "#         'max_epochs': config['max_epochs'],\n",
    "#         'learning_rate': config['learning_rate']\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Train the model and log relevant metrics using PyTorch Lightning Trainer and WandbLogger\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=config['max_epochs'],\n",
    "#     default_root_dir='model/checkpoint/',\n",
    "#     accelerator=\"auto\",\n",
    "#     devices=\"auto\",\n",
    "#     strategy=\"auto\",\n",
    "#     callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n",
    "#     logger=wandb_logger\n",
    "# )\n",
    "\n",
    "# trainer.fit(model=lightning_model, datamodule=dm)\n",
    "\n",
    "# # Finish wandb logging\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDKI-Projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
