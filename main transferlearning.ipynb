{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4845e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryPrecisionRecallCurve\n",
    "from torchvision.transforms import v2\n",
    "from torchinfo import summary\n",
    "import timm\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.datamodule import BinaryImageDataModule\n",
    "from training.hyperparameter_tuning import OptunaTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117666f9",
   "metadata": {},
   "source": [
    "### Loading Configuration\n",
    "\n",
    "In the following steps, we will load the configuration settings using the `load_configuration` function. The configuration is stored in the `config` variable which will be used throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74321032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Name: DESKTOP-LUKAS\n",
      "Loaded configuration from config/config_lukas.yaml\n"
     ]
    }
   ],
   "source": [
    "from config.load_configuration import load_configuration\n",
    "config = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d537e99",
   "metadata": {},
   "source": [
    "### Logging in to Weights & Biases (wandb)\n",
    "\n",
    "Before starting any experiment tracking, ensure you are logged in to your Weights & Biases (wandb) account. This enables automatic logging of metrics, model checkpoints, and experiment configurations. The following code logs you in to wandb:\n",
    "\n",
    "```python\n",
    "wandb.login()\n",
    "```\n",
    "If you are running this for the first time, you may be prompted to enter your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90bd2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukas-pelz\u001b[0m (\u001b[33mHKA-EKG-Signalverarbeitung\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Wandb logger\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414378fc",
   "metadata": {},
   "source": [
    "### Setting Seeds for Reproducibility\n",
    "\n",
    "To ensure comparable and reproducible results, we set the random seed using the `seed_everything` function from PyTorch Lightning. This helps in achieving consistent behavior across multiple runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06e10d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # disable oneDNN optimizations for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86df64",
   "metadata": {},
   "source": [
    "### Checking for GPU Devices\n",
    "\n",
    "In this step, we check for the availability of GPU devices and print the device currently being used by PyTorch. This ensures that the computations are performed on the most efficient hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b717a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.7.0+cu128\n",
      "Using device:  cuda\n",
      "Cuda Version:  12.8\n",
      "NVIDIA GeForce RTX 5060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Torch Version: ', torch.__version__)\n",
    "print('Using device: ', device)\n",
    "if device.type == 'cuda':\n",
    "    print('Cuda Version: ', torch.version.cuda)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177b86e",
   "metadata": {},
   "source": [
    "### Defining Transformations and Instantiating DataModule\n",
    "\n",
    "In this step, we will define the necessary data transformations and initialize the `Animal_DataModule` with the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df954014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Define transformations here --> this now happens within hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c560cb9",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "In this step, we will define the model architecture and print its summary using the `ModelSummary` utility from PyTorch Lightning. This provides an overview of the model's layers, parameters, and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937521f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aimv2_1b_patch14_224',\n",
       " 'aimv2_1b_patch14_336',\n",
       " 'aimv2_1b_patch14_448',\n",
       " 'aimv2_3b_patch14_224',\n",
       " 'aimv2_3b_patch14_336',\n",
       " 'aimv2_3b_patch14_448',\n",
       " 'aimv2_huge_patch14_224',\n",
       " 'aimv2_huge_patch14_336',\n",
       " 'aimv2_huge_patch14_448',\n",
       " 'aimv2_large_patch14_224',\n",
       " 'aimv2_large_patch14_336',\n",
       " 'aimv2_large_patch14_448',\n",
       " 'bat_resnext26ts',\n",
       " 'beit_base_patch16_224',\n",
       " 'beit_base_patch16_384',\n",
       " 'beit_large_patch16_224',\n",
       " 'beit_large_patch16_384',\n",
       " 'beit_large_patch16_512',\n",
       " 'beitv2_base_patch16_224',\n",
       " 'beitv2_large_patch16_224',\n",
       " 'botnet26t_256',\n",
       " 'botnet50ts_256',\n",
       " 'caformer_b36',\n",
       " 'caformer_m36',\n",
       " 'caformer_s18',\n",
       " 'caformer_s36',\n",
       " 'cait_m36_384',\n",
       " 'cait_m48_448',\n",
       " 'cait_s24_224',\n",
       " 'cait_s24_384',\n",
       " 'cait_s36_384',\n",
       " 'cait_xs24_384',\n",
       " 'cait_xxs24_224',\n",
       " 'cait_xxs24_384',\n",
       " 'cait_xxs36_224',\n",
       " 'cait_xxs36_384',\n",
       " 'coat_lite_medium',\n",
       " 'coat_lite_medium_384',\n",
       " 'coat_lite_mini',\n",
       " 'coat_lite_small',\n",
       " 'coat_lite_tiny',\n",
       " 'coat_mini',\n",
       " 'coat_small',\n",
       " 'coat_tiny',\n",
       " 'coatnet_0_224',\n",
       " 'coatnet_0_rw_224',\n",
       " 'coatnet_1_224',\n",
       " 'coatnet_1_rw_224',\n",
       " 'coatnet_2_224',\n",
       " 'coatnet_2_rw_224',\n",
       " 'coatnet_3_224',\n",
       " 'coatnet_3_rw_224',\n",
       " 'coatnet_4_224',\n",
       " 'coatnet_5_224',\n",
       " 'coatnet_bn_0_rw_224',\n",
       " 'coatnet_nano_cc_224',\n",
       " 'coatnet_nano_rw_224',\n",
       " 'coatnet_pico_rw_224',\n",
       " 'coatnet_rmlp_0_rw_224',\n",
       " 'coatnet_rmlp_1_rw2_224',\n",
       " 'coatnet_rmlp_1_rw_224',\n",
       " 'coatnet_rmlp_2_rw_224',\n",
       " 'coatnet_rmlp_2_rw_384',\n",
       " 'coatnet_rmlp_3_rw_224',\n",
       " 'coatnet_rmlp_nano_rw_224',\n",
       " 'coatnext_nano_rw_224',\n",
       " 'convformer_b36',\n",
       " 'convformer_m36',\n",
       " 'convformer_s18',\n",
       " 'convformer_s36',\n",
       " 'convit_base',\n",
       " 'convit_small',\n",
       " 'convit_tiny',\n",
       " 'convmixer_768_32',\n",
       " 'convmixer_1024_20_ks9_p14',\n",
       " 'convmixer_1536_20',\n",
       " 'convnext_atto',\n",
       " 'convnext_atto_ols',\n",
       " 'convnext_atto_rms',\n",
       " 'convnext_base',\n",
       " 'convnext_femto',\n",
       " 'convnext_femto_ols',\n",
       " 'convnext_large',\n",
       " 'convnext_large_mlp',\n",
       " 'convnext_nano',\n",
       " 'convnext_nano_ols',\n",
       " 'convnext_pico',\n",
       " 'convnext_pico_ols',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'convnext_tiny_hnf',\n",
       " 'convnext_xlarge',\n",
       " 'convnext_xxlarge',\n",
       " 'convnext_zepto_rms',\n",
       " 'convnext_zepto_rms_ols',\n",
       " 'convnextv2_atto',\n",
       " 'convnextv2_base',\n",
       " 'convnextv2_femto',\n",
       " 'convnextv2_huge',\n",
       " 'convnextv2_large',\n",
       " 'convnextv2_nano',\n",
       " 'convnextv2_pico',\n",
       " 'convnextv2_small',\n",
       " 'convnextv2_tiny',\n",
       " 'crossvit_9_240',\n",
       " 'crossvit_9_dagger_240',\n",
       " 'crossvit_15_240',\n",
       " 'crossvit_15_dagger_240',\n",
       " 'crossvit_15_dagger_408',\n",
       " 'crossvit_18_240',\n",
       " 'crossvit_18_dagger_240',\n",
       " 'crossvit_18_dagger_408',\n",
       " 'crossvit_base_240',\n",
       " 'crossvit_small_240',\n",
       " 'crossvit_tiny_240',\n",
       " 'cs3darknet_focus_l',\n",
       " 'cs3darknet_focus_m',\n",
       " 'cs3darknet_focus_s',\n",
       " 'cs3darknet_focus_x',\n",
       " 'cs3darknet_l',\n",
       " 'cs3darknet_m',\n",
       " 'cs3darknet_s',\n",
       " 'cs3darknet_x',\n",
       " 'cs3edgenet_x',\n",
       " 'cs3se_edgenet_x',\n",
       " 'cs3sedarknet_l',\n",
       " 'cs3sedarknet_x',\n",
       " 'cs3sedarknet_xdw',\n",
       " 'cspdarknet53',\n",
       " 'cspresnet50',\n",
       " 'cspresnet50d',\n",
       " 'cspresnet50w',\n",
       " 'cspresnext50',\n",
       " 'darknet17',\n",
       " 'darknet21',\n",
       " 'darknet53',\n",
       " 'darknetaa53',\n",
       " 'davit_base',\n",
       " 'davit_base_fl',\n",
       " 'davit_giant',\n",
       " 'davit_huge',\n",
       " 'davit_huge_fl',\n",
       " 'davit_large',\n",
       " 'davit_small',\n",
       " 'davit_tiny',\n",
       " 'deit3_base_patch16_224',\n",
       " 'deit3_base_patch16_384',\n",
       " 'deit3_huge_patch14_224',\n",
       " 'deit3_large_patch16_224',\n",
       " 'deit3_large_patch16_384',\n",
       " 'deit3_medium_patch16_224',\n",
       " 'deit3_small_patch16_224',\n",
       " 'deit3_small_patch16_384',\n",
       " 'deit_base_distilled_patch16_224',\n",
       " 'deit_base_distilled_patch16_384',\n",
       " 'deit_base_patch16_224',\n",
       " 'deit_base_patch16_384',\n",
       " 'deit_small_distilled_patch16_224',\n",
       " 'deit_small_patch16_224',\n",
       " 'deit_tiny_distilled_patch16_224',\n",
       " 'deit_tiny_patch16_224',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'densenet264d',\n",
       " 'densenetblur121d',\n",
       " 'dla34',\n",
       " 'dla46_c',\n",
       " 'dla46x_c',\n",
       " 'dla60',\n",
       " 'dla60_res2net',\n",
       " 'dla60_res2next',\n",
       " 'dla60x',\n",
       " 'dla60x_c',\n",
       " 'dla102',\n",
       " 'dla102x',\n",
       " 'dla102x2',\n",
       " 'dla169',\n",
       " 'dm_nfnet_f0',\n",
       " 'dm_nfnet_f1',\n",
       " 'dm_nfnet_f2',\n",
       " 'dm_nfnet_f3',\n",
       " 'dm_nfnet_f4',\n",
       " 'dm_nfnet_f5',\n",
       " 'dm_nfnet_f6',\n",
       " 'dpn48b',\n",
       " 'dpn68',\n",
       " 'dpn68b',\n",
       " 'dpn92',\n",
       " 'dpn98',\n",
       " 'dpn107',\n",
       " 'dpn131',\n",
       " 'eca_botnext26ts_256',\n",
       " 'eca_halonext26ts',\n",
       " 'eca_nfnet_l0',\n",
       " 'eca_nfnet_l1',\n",
       " 'eca_nfnet_l2',\n",
       " 'eca_nfnet_l3',\n",
       " 'eca_resnet33ts',\n",
       " 'eca_resnext26ts',\n",
       " 'eca_vovnet39b',\n",
       " 'ecaresnet26t',\n",
       " 'ecaresnet50d',\n",
       " 'ecaresnet50d_pruned',\n",
       " 'ecaresnet50t',\n",
       " 'ecaresnet101d',\n",
       " 'ecaresnet101d_pruned',\n",
       " 'ecaresnet200d',\n",
       " 'ecaresnet269d',\n",
       " 'ecaresnetlight',\n",
       " 'ecaresnext26t_32x4d',\n",
       " 'ecaresnext50t_32x4d',\n",
       " 'edgenext_base',\n",
       " 'edgenext_small',\n",
       " 'edgenext_small_rw',\n",
       " 'edgenext_x_small',\n",
       " 'edgenext_xx_small',\n",
       " 'efficientformer_l1',\n",
       " 'efficientformer_l3',\n",
       " 'efficientformer_l7',\n",
       " 'efficientformerv2_l',\n",
       " 'efficientformerv2_s0',\n",
       " 'efficientformerv2_s1',\n",
       " 'efficientformerv2_s2',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b0_g8_gn',\n",
       " 'efficientnet_b0_g16_evos',\n",
       " 'efficientnet_b0_gn',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b1_pruned',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b2_pruned',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b3_g8_gn',\n",
       " 'efficientnet_b3_gn',\n",
       " 'efficientnet_b3_pruned',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_b8',\n",
       " 'efficientnet_blur_b0',\n",
       " 'efficientnet_cc_b0_4e',\n",
       " 'efficientnet_cc_b0_8e',\n",
       " 'efficientnet_cc_b1_8e',\n",
       " 'efficientnet_el',\n",
       " 'efficientnet_el_pruned',\n",
       " 'efficientnet_em',\n",
       " 'efficientnet_es',\n",
       " 'efficientnet_es_pruned',\n",
       " 'efficientnet_h_b5',\n",
       " 'efficientnet_l2',\n",
       " 'efficientnet_lite0',\n",
       " 'efficientnet_lite1',\n",
       " 'efficientnet_lite2',\n",
       " 'efficientnet_lite3',\n",
       " 'efficientnet_lite4',\n",
       " 'efficientnet_x_b3',\n",
       " 'efficientnet_x_b5',\n",
       " 'efficientnetv2_l',\n",
       " 'efficientnetv2_m',\n",
       " 'efficientnetv2_rw_m',\n",
       " 'efficientnetv2_rw_s',\n",
       " 'efficientnetv2_rw_t',\n",
       " 'efficientnetv2_s',\n",
       " 'efficientnetv2_xl',\n",
       " 'efficientvit_b0',\n",
       " 'efficientvit_b1',\n",
       " 'efficientvit_b2',\n",
       " 'efficientvit_b3',\n",
       " 'efficientvit_l1',\n",
       " 'efficientvit_l2',\n",
       " 'efficientvit_l3',\n",
       " 'efficientvit_m0',\n",
       " 'efficientvit_m1',\n",
       " 'efficientvit_m2',\n",
       " 'efficientvit_m3',\n",
       " 'efficientvit_m4',\n",
       " 'efficientvit_m5',\n",
       " 'ese_vovnet19b_dw',\n",
       " 'ese_vovnet19b_slim',\n",
       " 'ese_vovnet19b_slim_dw',\n",
       " 'ese_vovnet39b',\n",
       " 'ese_vovnet39b_evos',\n",
       " 'ese_vovnet57b',\n",
       " 'ese_vovnet99b',\n",
       " 'eva02_base_patch14_224',\n",
       " 'eva02_base_patch14_448',\n",
       " 'eva02_base_patch16_clip_224',\n",
       " 'eva02_enormous_patch14_clip_224',\n",
       " 'eva02_large_patch14_224',\n",
       " 'eva02_large_patch14_448',\n",
       " 'eva02_large_patch14_clip_224',\n",
       " 'eva02_large_patch14_clip_336',\n",
       " 'eva02_small_patch14_224',\n",
       " 'eva02_small_patch14_336',\n",
       " 'eva02_tiny_patch14_224',\n",
       " 'eva02_tiny_patch14_336',\n",
       " 'eva_giant_patch14_224',\n",
       " 'eva_giant_patch14_336',\n",
       " 'eva_giant_patch14_560',\n",
       " 'eva_giant_patch14_clip_224',\n",
       " 'eva_large_patch14_196',\n",
       " 'eva_large_patch14_336',\n",
       " 'fastvit_ma36',\n",
       " 'fastvit_mci0',\n",
       " 'fastvit_mci1',\n",
       " 'fastvit_mci2',\n",
       " 'fastvit_s12',\n",
       " 'fastvit_sa12',\n",
       " 'fastvit_sa24',\n",
       " 'fastvit_sa36',\n",
       " 'fastvit_t8',\n",
       " 'fastvit_t12',\n",
       " 'fbnetc_100',\n",
       " 'fbnetv3_b',\n",
       " 'fbnetv3_d',\n",
       " 'fbnetv3_g',\n",
       " 'flexivit_base',\n",
       " 'flexivit_large',\n",
       " 'flexivit_small',\n",
       " 'focalnet_base_lrf',\n",
       " 'focalnet_base_srf',\n",
       " 'focalnet_huge_fl3',\n",
       " 'focalnet_huge_fl4',\n",
       " 'focalnet_large_fl3',\n",
       " 'focalnet_large_fl4',\n",
       " 'focalnet_small_lrf',\n",
       " 'focalnet_small_srf',\n",
       " 'focalnet_tiny_lrf',\n",
       " 'focalnet_tiny_srf',\n",
       " 'focalnet_xlarge_fl3',\n",
       " 'focalnet_xlarge_fl4',\n",
       " 'gc_efficientnetv2_rw_t',\n",
       " 'gcresnet33ts',\n",
       " 'gcresnet50t',\n",
       " 'gcresnext26ts',\n",
       " 'gcresnext50ts',\n",
       " 'gcvit_base',\n",
       " 'gcvit_small',\n",
       " 'gcvit_tiny',\n",
       " 'gcvit_xtiny',\n",
       " 'gcvit_xxtiny',\n",
       " 'gernet_l',\n",
       " 'gernet_m',\n",
       " 'gernet_s',\n",
       " 'ghostnet_050',\n",
       " 'ghostnet_100',\n",
       " 'ghostnet_130',\n",
       " 'ghostnetv2_100',\n",
       " 'ghostnetv2_130',\n",
       " 'ghostnetv2_160',\n",
       " 'gmixer_12_224',\n",
       " 'gmixer_24_224',\n",
       " 'gmlp_b16_224',\n",
       " 'gmlp_s16_224',\n",
       " 'gmlp_ti16_224',\n",
       " 'halo2botnet50ts_256',\n",
       " 'halonet26t',\n",
       " 'halonet50ts',\n",
       " 'halonet_h1',\n",
       " 'haloregnetz_b',\n",
       " 'hardcorenas_a',\n",
       " 'hardcorenas_b',\n",
       " 'hardcorenas_c',\n",
       " 'hardcorenas_d',\n",
       " 'hardcorenas_e',\n",
       " 'hardcorenas_f',\n",
       " 'hgnet_base',\n",
       " 'hgnet_small',\n",
       " 'hgnet_tiny',\n",
       " 'hgnetv2_b0',\n",
       " 'hgnetv2_b1',\n",
       " 'hgnetv2_b2',\n",
       " 'hgnetv2_b3',\n",
       " 'hgnetv2_b4',\n",
       " 'hgnetv2_b5',\n",
       " 'hgnetv2_b6',\n",
       " 'hiera_base_224',\n",
       " 'hiera_base_abswin_256',\n",
       " 'hiera_base_plus_224',\n",
       " 'hiera_huge_224',\n",
       " 'hiera_large_224',\n",
       " 'hiera_small_224',\n",
       " 'hiera_small_abswin_256',\n",
       " 'hiera_tiny_224',\n",
       " 'hieradet_small',\n",
       " 'hrnet_w18',\n",
       " 'hrnet_w18_small',\n",
       " 'hrnet_w18_small_v2',\n",
       " 'hrnet_w18_ssld',\n",
       " 'hrnet_w30',\n",
       " 'hrnet_w32',\n",
       " 'hrnet_w40',\n",
       " 'hrnet_w44',\n",
       " 'hrnet_w48',\n",
       " 'hrnet_w48_ssld',\n",
       " 'hrnet_w64',\n",
       " 'inception_next_atto',\n",
       " 'inception_next_base',\n",
       " 'inception_next_small',\n",
       " 'inception_next_tiny',\n",
       " 'inception_resnet_v2',\n",
       " 'inception_v3',\n",
       " 'inception_v4',\n",
       " 'lambda_resnet26rpt_256',\n",
       " 'lambda_resnet26t',\n",
       " 'lambda_resnet50ts',\n",
       " 'lamhalobotnet50ts_256',\n",
       " 'lcnet_035',\n",
       " 'lcnet_050',\n",
       " 'lcnet_075',\n",
       " 'lcnet_100',\n",
       " 'lcnet_150',\n",
       " 'legacy_senet154',\n",
       " 'legacy_seresnet18',\n",
       " 'legacy_seresnet34',\n",
       " 'legacy_seresnet50',\n",
       " 'legacy_seresnet101',\n",
       " 'legacy_seresnet152',\n",
       " 'legacy_seresnext26_32x4d',\n",
       " 'legacy_seresnext50_32x4d',\n",
       " 'legacy_seresnext101_32x4d',\n",
       " 'legacy_xception',\n",
       " 'levit_128',\n",
       " 'levit_128s',\n",
       " 'levit_192',\n",
       " 'levit_256',\n",
       " 'levit_256d',\n",
       " 'levit_384',\n",
       " 'levit_384_s8',\n",
       " 'levit_512',\n",
       " 'levit_512_s8',\n",
       " 'levit_512d',\n",
       " 'levit_conv_128',\n",
       " 'levit_conv_128s',\n",
       " 'levit_conv_192',\n",
       " 'levit_conv_256',\n",
       " 'levit_conv_256d',\n",
       " 'levit_conv_384',\n",
       " 'levit_conv_384_s8',\n",
       " 'levit_conv_512',\n",
       " 'levit_conv_512_s8',\n",
       " 'levit_conv_512d',\n",
       " 'mambaout_base',\n",
       " 'mambaout_base_plus_rw',\n",
       " 'mambaout_base_short_rw',\n",
       " 'mambaout_base_tall_rw',\n",
       " 'mambaout_base_wide_rw',\n",
       " 'mambaout_femto',\n",
       " 'mambaout_kobe',\n",
       " 'mambaout_small',\n",
       " 'mambaout_small_rw',\n",
       " 'mambaout_tiny',\n",
       " 'maxvit_base_tf_224',\n",
       " 'maxvit_base_tf_384',\n",
       " 'maxvit_base_tf_512',\n",
       " 'maxvit_large_tf_224',\n",
       " 'maxvit_large_tf_384',\n",
       " 'maxvit_large_tf_512',\n",
       " 'maxvit_nano_rw_256',\n",
       " 'maxvit_pico_rw_256',\n",
       " 'maxvit_rmlp_base_rw_224',\n",
       " 'maxvit_rmlp_base_rw_384',\n",
       " 'maxvit_rmlp_nano_rw_256',\n",
       " 'maxvit_rmlp_pico_rw_256',\n",
       " 'maxvit_rmlp_small_rw_224',\n",
       " 'maxvit_rmlp_small_rw_256',\n",
       " 'maxvit_rmlp_tiny_rw_256',\n",
       " 'maxvit_small_tf_224',\n",
       " 'maxvit_small_tf_384',\n",
       " 'maxvit_small_tf_512',\n",
       " 'maxvit_tiny_pm_256',\n",
       " 'maxvit_tiny_rw_224',\n",
       " 'maxvit_tiny_rw_256',\n",
       " 'maxvit_tiny_tf_224',\n",
       " 'maxvit_tiny_tf_384',\n",
       " 'maxvit_tiny_tf_512',\n",
       " 'maxvit_xlarge_tf_224',\n",
       " 'maxvit_xlarge_tf_384',\n",
       " 'maxvit_xlarge_tf_512',\n",
       " 'maxxvit_rmlp_nano_rw_256',\n",
       " 'maxxvit_rmlp_small_rw_256',\n",
       " 'maxxvit_rmlp_tiny_rw_256',\n",
       " 'maxxvitv2_nano_rw_256',\n",
       " 'maxxvitv2_rmlp_base_rw_224',\n",
       " 'maxxvitv2_rmlp_base_rw_384',\n",
       " 'maxxvitv2_rmlp_large_rw_224',\n",
       " 'mixer_b16_224',\n",
       " 'mixer_b32_224',\n",
       " 'mixer_l16_224',\n",
       " 'mixer_l32_224',\n",
       " 'mixer_s16_224',\n",
       " 'mixer_s32_224',\n",
       " 'mixnet_l',\n",
       " 'mixnet_m',\n",
       " 'mixnet_s',\n",
       " 'mixnet_xl',\n",
       " 'mixnet_xxl',\n",
       " 'mnasnet_050',\n",
       " 'mnasnet_075',\n",
       " 'mnasnet_100',\n",
       " 'mnasnet_140',\n",
       " 'mnasnet_small',\n",
       " 'mobilenet_edgetpu_100',\n",
       " 'mobilenet_edgetpu_v2_l',\n",
       " 'mobilenet_edgetpu_v2_m',\n",
       " 'mobilenet_edgetpu_v2_s',\n",
       " 'mobilenet_edgetpu_v2_xs',\n",
       " 'mobilenetv1_100',\n",
       " 'mobilenetv1_100h',\n",
       " 'mobilenetv1_125',\n",
       " 'mobilenetv2_035',\n",
       " 'mobilenetv2_050',\n",
       " 'mobilenetv2_075',\n",
       " 'mobilenetv2_100',\n",
       " 'mobilenetv2_110d',\n",
       " 'mobilenetv2_120d',\n",
       " 'mobilenetv2_140',\n",
       " 'mobilenetv3_large_075',\n",
       " 'mobilenetv3_large_100',\n",
       " 'mobilenetv3_large_150d',\n",
       " 'mobilenetv3_rw',\n",
       " 'mobilenetv3_small_050',\n",
       " 'mobilenetv3_small_075',\n",
       " 'mobilenetv3_small_100',\n",
       " 'mobilenetv4_conv_aa_large',\n",
       " 'mobilenetv4_conv_aa_medium',\n",
       " 'mobilenetv4_conv_blur_medium',\n",
       " 'mobilenetv4_conv_large',\n",
       " 'mobilenetv4_conv_medium',\n",
       " 'mobilenetv4_conv_small',\n",
       " 'mobilenetv4_conv_small_035',\n",
       " 'mobilenetv4_conv_small_050',\n",
       " 'mobilenetv4_hybrid_large',\n",
       " 'mobilenetv4_hybrid_large_075',\n",
       " 'mobilenetv4_hybrid_medium',\n",
       " 'mobilenetv4_hybrid_medium_075',\n",
       " 'mobileone_s0',\n",
       " 'mobileone_s1',\n",
       " 'mobileone_s2',\n",
       " 'mobileone_s3',\n",
       " 'mobileone_s4',\n",
       " 'mobilevit_s',\n",
       " 'mobilevit_xs',\n",
       " 'mobilevit_xxs',\n",
       " 'mobilevitv2_050',\n",
       " 'mobilevitv2_075',\n",
       " 'mobilevitv2_100',\n",
       " 'mobilevitv2_125',\n",
       " 'mobilevitv2_150',\n",
       " 'mobilevitv2_175',\n",
       " 'mobilevitv2_200',\n",
       " 'mvitv2_base',\n",
       " 'mvitv2_base_cls',\n",
       " 'mvitv2_huge_cls',\n",
       " 'mvitv2_large',\n",
       " 'mvitv2_large_cls',\n",
       " 'mvitv2_small',\n",
       " 'mvitv2_small_cls',\n",
       " 'mvitv2_tiny',\n",
       " 'nasnetalarge',\n",
       " 'nest_base',\n",
       " 'nest_base_jx',\n",
       " 'nest_small',\n",
       " 'nest_small_jx',\n",
       " 'nest_tiny',\n",
       " 'nest_tiny_jx',\n",
       " 'nextvit_base',\n",
       " 'nextvit_large',\n",
       " 'nextvit_small',\n",
       " 'nf_ecaresnet26',\n",
       " 'nf_ecaresnet50',\n",
       " 'nf_ecaresnet101',\n",
       " 'nf_regnet_b0',\n",
       " 'nf_regnet_b1',\n",
       " 'nf_regnet_b2',\n",
       " 'nf_regnet_b3',\n",
       " 'nf_regnet_b4',\n",
       " 'nf_regnet_b5',\n",
       " 'nf_resnet26',\n",
       " 'nf_resnet50',\n",
       " 'nf_resnet101',\n",
       " 'nf_seresnet26',\n",
       " 'nf_seresnet50',\n",
       " 'nf_seresnet101',\n",
       " 'nfnet_f0',\n",
       " 'nfnet_f1',\n",
       " 'nfnet_f2',\n",
       " 'nfnet_f3',\n",
       " 'nfnet_f4',\n",
       " 'nfnet_f5',\n",
       " 'nfnet_f6',\n",
       " 'nfnet_f7',\n",
       " 'nfnet_l0',\n",
       " 'pit_b_224',\n",
       " 'pit_b_distilled_224',\n",
       " 'pit_s_224',\n",
       " 'pit_s_distilled_224',\n",
       " 'pit_ti_224',\n",
       " 'pit_ti_distilled_224',\n",
       " 'pit_xs_224',\n",
       " 'pit_xs_distilled_224',\n",
       " 'pnasnet5large',\n",
       " 'poolformer_m36',\n",
       " 'poolformer_m48',\n",
       " 'poolformer_s12',\n",
       " 'poolformer_s24',\n",
       " 'poolformer_s36',\n",
       " 'poolformerv2_m36',\n",
       " 'poolformerv2_m48',\n",
       " 'poolformerv2_s12',\n",
       " 'poolformerv2_s24',\n",
       " 'poolformerv2_s36',\n",
       " 'pvt_v2_b0',\n",
       " 'pvt_v2_b1',\n",
       " 'pvt_v2_b2',\n",
       " 'pvt_v2_b2_li',\n",
       " 'pvt_v2_b3',\n",
       " 'pvt_v2_b4',\n",
       " 'pvt_v2_b5',\n",
       " 'rdnet_base',\n",
       " 'rdnet_large',\n",
       " 'rdnet_small',\n",
       " 'rdnet_tiny',\n",
       " 'regnetv_040',\n",
       " 'regnetv_064',\n",
       " 'regnetx_002',\n",
       " 'regnetx_004',\n",
       " 'regnetx_004_tv',\n",
       " 'regnetx_006',\n",
       " 'regnetx_008',\n",
       " 'regnetx_016',\n",
       " 'regnetx_032',\n",
       " 'regnetx_040',\n",
       " 'regnetx_064',\n",
       " 'regnetx_080',\n",
       " 'regnetx_120',\n",
       " 'regnetx_160',\n",
       " 'regnetx_320',\n",
       " 'regnety_002',\n",
       " 'regnety_004',\n",
       " 'regnety_006',\n",
       " 'regnety_008',\n",
       " 'regnety_008_tv',\n",
       " 'regnety_016',\n",
       " 'regnety_032',\n",
       " 'regnety_040',\n",
       " 'regnety_040_sgn',\n",
       " 'regnety_064',\n",
       " 'regnety_080',\n",
       " 'regnety_080_tv',\n",
       " 'regnety_120',\n",
       " 'regnety_160',\n",
       " 'regnety_320',\n",
       " 'regnety_640',\n",
       " 'regnety_1280',\n",
       " 'regnety_2560',\n",
       " 'regnetz_005',\n",
       " 'regnetz_040',\n",
       " 'regnetz_040_h',\n",
       " 'regnetz_b16',\n",
       " 'regnetz_b16_evos',\n",
       " 'regnetz_c16',\n",
       " 'regnetz_c16_evos',\n",
       " 'regnetz_d8',\n",
       " 'regnetz_d8_evos',\n",
       " 'regnetz_d32',\n",
       " 'regnetz_e8',\n",
       " 'repghostnet_050',\n",
       " 'repghostnet_058',\n",
       " 'repghostnet_080',\n",
       " 'repghostnet_100',\n",
       " 'repghostnet_111',\n",
       " 'repghostnet_130',\n",
       " 'repghostnet_150',\n",
       " 'repghostnet_200',\n",
       " 'repvgg_a0',\n",
       " 'repvgg_a1',\n",
       " 'repvgg_a2',\n",
       " 'repvgg_b0',\n",
       " 'repvgg_b1',\n",
       " 'repvgg_b1g4',\n",
       " 'repvgg_b2',\n",
       " 'repvgg_b2g4',\n",
       " 'repvgg_b3',\n",
       " 'repvgg_b3g4',\n",
       " 'repvgg_d2se',\n",
       " 'repvit_m0_9',\n",
       " 'repvit_m1',\n",
       " 'repvit_m1_0',\n",
       " 'repvit_m1_1',\n",
       " 'repvit_m1_5',\n",
       " 'repvit_m2',\n",
       " 'repvit_m2_3',\n",
       " 'repvit_m3',\n",
       " 'res2net50_14w_8s',\n",
       " 'res2net50_26w_4s',\n",
       " 'res2net50_26w_6s',\n",
       " 'res2net50_26w_8s',\n",
       " 'res2net50_48w_2s',\n",
       " 'res2net50d',\n",
       " 'res2net101_26w_4s',\n",
       " 'res2net101d',\n",
       " 'res2next50',\n",
       " 'resmlp_12_224',\n",
       " 'resmlp_24_224',\n",
       " 'resmlp_36_224',\n",
       " 'resmlp_big_24_224',\n",
       " 'resnest14d',\n",
       " 'resnest26d',\n",
       " 'resnest50d',\n",
       " 'resnest50d_1s4x24d',\n",
       " 'resnest50d_4s2x40d',\n",
       " 'resnest101e',\n",
       " 'resnest200e',\n",
       " 'resnest269e',\n",
       " 'resnet10t',\n",
       " 'resnet14t',\n",
       " 'resnet18',\n",
       " 'resnet18d',\n",
       " 'resnet26',\n",
       " 'resnet26d',\n",
       " 'resnet26t',\n",
       " 'resnet32ts',\n",
       " 'resnet33ts',\n",
       " 'resnet34',\n",
       " 'resnet34d',\n",
       " 'resnet50',\n",
       " 'resnet50_clip',\n",
       " 'resnet50_clip_gap',\n",
       " 'resnet50_gn',\n",
       " 'resnet50_mlp',\n",
       " 'resnet50c',\n",
       " 'resnet50d',\n",
       " 'resnet50s',\n",
       " 'resnet50t',\n",
       " 'resnet50x4_clip',\n",
       " 'resnet50x4_clip_gap',\n",
       " 'resnet50x16_clip',\n",
       " 'resnet50x16_clip_gap',\n",
       " 'resnet50x64_clip',\n",
       " 'resnet50x64_clip_gap',\n",
       " 'resnet51q',\n",
       " 'resnet61q',\n",
       " 'resnet101',\n",
       " 'resnet101_clip',\n",
       " 'resnet101_clip_gap',\n",
       " 'resnet101c',\n",
       " 'resnet101d',\n",
       " 'resnet101s',\n",
       " 'resnet152',\n",
       " 'resnet152c',\n",
       " 'resnet152d',\n",
       " 'resnet152s',\n",
       " 'resnet200',\n",
       " 'resnet200d',\n",
       " 'resnetaa34d',\n",
       " 'resnetaa50',\n",
       " 'resnetaa50d',\n",
       " 'resnetaa101d',\n",
       " 'resnetblur18',\n",
       " 'resnetblur50',\n",
       " 'resnetblur50d',\n",
       " 'resnetblur101d',\n",
       " 'resnetrs50',\n",
       " 'resnetrs101',\n",
       " 'resnetrs152',\n",
       " 'resnetrs200',\n",
       " 'resnetrs270',\n",
       " 'resnetrs350',\n",
       " 'resnetrs420',\n",
       " 'resnetv2_18',\n",
       " 'resnetv2_18d',\n",
       " 'resnetv2_34',\n",
       " 'resnetv2_34d',\n",
       " 'resnetv2_50',\n",
       " 'resnetv2_50d',\n",
       " 'resnetv2_50d_evos',\n",
       " 'resnetv2_50d_frn',\n",
       " 'resnetv2_50d_gn',\n",
       " 'resnetv2_50t',\n",
       " 'resnetv2_50x1_bit',\n",
       " 'resnetv2_50x3_bit',\n",
       " 'resnetv2_101',\n",
       " 'resnetv2_101d',\n",
       " 'resnetv2_101x1_bit',\n",
       " 'resnetv2_101x3_bit',\n",
       " 'resnetv2_152',\n",
       " 'resnetv2_152d',\n",
       " 'resnetv2_152x2_bit',\n",
       " 'resnetv2_152x4_bit',\n",
       " 'resnext26ts',\n",
       " 'resnext50_32x4d',\n",
       " 'resnext50d_32x4d',\n",
       " 'resnext101_32x4d',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_32x16d',\n",
       " 'resnext101_32x32d',\n",
       " 'resnext101_64x4d',\n",
       " 'rexnet_100',\n",
       " 'rexnet_130',\n",
       " 'rexnet_150',\n",
       " 'rexnet_200',\n",
       " 'rexnet_300',\n",
       " 'rexnetr_100',\n",
       " 'rexnetr_130',\n",
       " 'rexnetr_150',\n",
       " 'rexnetr_200',\n",
       " 'rexnetr_300',\n",
       " 'sam2_hiera_base_plus',\n",
       " 'sam2_hiera_large',\n",
       " 'sam2_hiera_small',\n",
       " 'sam2_hiera_tiny',\n",
       " 'samvit_base_patch16',\n",
       " 'samvit_base_patch16_224',\n",
       " 'samvit_huge_patch16',\n",
       " 'samvit_large_patch16',\n",
       " 'sebotnet33ts_256',\n",
       " 'sedarknet21',\n",
       " 'sehalonet33ts',\n",
       " 'selecsls42',\n",
       " 'selecsls42b',\n",
       " 'selecsls60',\n",
       " 'selecsls60b',\n",
       " 'selecsls84',\n",
       " 'semnasnet_050',\n",
       " 'semnasnet_075',\n",
       " 'semnasnet_100',\n",
       " 'semnasnet_140',\n",
       " 'senet154',\n",
       " 'sequencer2d_l',\n",
       " 'sequencer2d_m',\n",
       " 'sequencer2d_s',\n",
       " 'seresnet18',\n",
       " 'seresnet33ts',\n",
       " 'seresnet34',\n",
       " 'seresnet50',\n",
       " 'seresnet50t',\n",
       " 'seresnet101',\n",
       " 'seresnet152',\n",
       " 'seresnet152d',\n",
       " 'seresnet200d',\n",
       " 'seresnet269d',\n",
       " 'seresnetaa50d',\n",
       " 'seresnext26d_32x4d',\n",
       " 'seresnext26t_32x4d',\n",
       " 'seresnext26ts',\n",
       " 'seresnext50_32x4d',\n",
       " 'seresnext101_32x4d',\n",
       " 'seresnext101_32x8d',\n",
       " 'seresnext101_64x4d',\n",
       " 'seresnext101d_32x8d',\n",
       " 'seresnextaa101d_32x8d',\n",
       " 'seresnextaa201d_32x8d',\n",
       " 'skresnet18',\n",
       " 'skresnet34',\n",
       " 'skresnet50',\n",
       " 'skresnet50d',\n",
       " 'skresnext50_32x4d',\n",
       " 'spnasnet_100',\n",
       " 'swin_base_patch4_window7_224',\n",
       " 'swin_base_patch4_window12_384',\n",
       " 'swin_large_patch4_window7_224',\n",
       " 'swin_large_patch4_window12_384',\n",
       " 'swin_s3_base_224',\n",
       " 'swin_s3_small_224',\n",
       " 'swin_s3_tiny_224',\n",
       " 'swin_small_patch4_window7_224',\n",
       " 'swin_tiny_patch4_window7_224',\n",
       " 'swinv2_base_window8_256',\n",
       " 'swinv2_base_window12_192',\n",
       " 'swinv2_base_window12to16_192to256',\n",
       " 'swinv2_base_window12to24_192to384',\n",
       " 'swinv2_base_window16_256',\n",
       " 'swinv2_cr_base_224',\n",
       " 'swinv2_cr_base_384',\n",
       " 'swinv2_cr_base_ns_224',\n",
       " 'swinv2_cr_giant_224',\n",
       " 'swinv2_cr_giant_384',\n",
       " 'swinv2_cr_huge_224',\n",
       " 'swinv2_cr_huge_384',\n",
       " 'swinv2_cr_large_224',\n",
       " 'swinv2_cr_large_384',\n",
       " 'swinv2_cr_small_224',\n",
       " 'swinv2_cr_small_384',\n",
       " 'swinv2_cr_small_ns_224',\n",
       " 'swinv2_cr_small_ns_256',\n",
       " 'swinv2_cr_tiny_224',\n",
       " 'swinv2_cr_tiny_384',\n",
       " 'swinv2_cr_tiny_ns_224',\n",
       " 'swinv2_large_window12_192',\n",
       " 'swinv2_large_window12to16_192to256',\n",
       " 'swinv2_large_window12to24_192to384',\n",
       " 'swinv2_small_window8_256',\n",
       " 'swinv2_small_window16_256',\n",
       " 'swinv2_tiny_window8_256',\n",
       " 'swinv2_tiny_window16_256',\n",
       " 'test_byobnet',\n",
       " 'test_convnext',\n",
       " 'test_convnext2',\n",
       " 'test_convnext3',\n",
       " 'test_efficientnet',\n",
       " 'test_efficientnet_evos',\n",
       " 'test_efficientnet_gn',\n",
       " 'test_efficientnet_ln',\n",
       " 'test_mambaout',\n",
       " 'test_nfnet',\n",
       " 'test_resnet',\n",
       " 'test_vit',\n",
       " 'test_vit2',\n",
       " 'test_vit3',\n",
       " 'test_vit4',\n",
       " 'tf_efficientnet_b0',\n",
       " 'tf_efficientnet_b1',\n",
       " 'tf_efficientnet_b2',\n",
       " 'tf_efficientnet_b3',\n",
       " 'tf_efficientnet_b4',\n",
       " 'tf_efficientnet_b5',\n",
       " 'tf_efficientnet_b6',\n",
       " 'tf_efficientnet_b7',\n",
       " 'tf_efficientnet_b8',\n",
       " 'tf_efficientnet_cc_b0_4e',\n",
       " 'tf_efficientnet_cc_b0_8e',\n",
       " 'tf_efficientnet_cc_b1_8e',\n",
       " 'tf_efficientnet_el',\n",
       " 'tf_efficientnet_em',\n",
       " 'tf_efficientnet_es',\n",
       " 'tf_efficientnet_l2',\n",
       " 'tf_efficientnet_lite0',\n",
       " 'tf_efficientnet_lite1',\n",
       " 'tf_efficientnet_lite2',\n",
       " 'tf_efficientnet_lite3',\n",
       " 'tf_efficientnet_lite4',\n",
       " 'tf_efficientnetv2_b0',\n",
       " 'tf_efficientnetv2_b1',\n",
       " 'tf_efficientnetv2_b2',\n",
       " 'tf_efficientnetv2_b3',\n",
       " 'tf_efficientnetv2_l',\n",
       " 'tf_efficientnetv2_m',\n",
       " 'tf_efficientnetv2_s',\n",
       " 'tf_efficientnetv2_xl',\n",
       " 'tf_mixnet_l',\n",
       " 'tf_mixnet_m',\n",
       " 'tf_mixnet_s',\n",
       " 'tf_mobilenetv3_large_075',\n",
       " 'tf_mobilenetv3_large_100',\n",
       " 'tf_mobilenetv3_large_minimal_100',\n",
       " 'tf_mobilenetv3_small_075',\n",
       " 'tf_mobilenetv3_small_100',\n",
       " 'tf_mobilenetv3_small_minimal_100',\n",
       " 'tiny_vit_5m_224',\n",
       " 'tiny_vit_11m_224',\n",
       " 'tiny_vit_21m_224',\n",
       " 'tiny_vit_21m_384',\n",
       " 'tiny_vit_21m_512',\n",
       " 'tinynet_a',\n",
       " 'tinynet_b',\n",
       " 'tinynet_c',\n",
       " 'tinynet_d',\n",
       " 'tinynet_e',\n",
       " 'tnt_b_patch16_224',\n",
       " 'tnt_s_patch16_224',\n",
       " 'tresnet_l',\n",
       " 'tresnet_m',\n",
       " 'tresnet_v2_l',\n",
       " 'tresnet_xl',\n",
       " 'twins_pcpvt_base',\n",
       " 'twins_pcpvt_large',\n",
       " 'twins_pcpvt_small',\n",
       " 'twins_svt_base',\n",
       " 'twins_svt_large',\n",
       " 'twins_svt_small',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'visformer_small',\n",
       " 'visformer_tiny',\n",
       " 'vit_base_mci_224',\n",
       " 'vit_base_patch8_224',\n",
       " 'vit_base_patch14_dinov2',\n",
       " 'vit_base_patch14_reg4_dinov2',\n",
       " 'vit_base_patch16_18x2_224',\n",
       " 'vit_base_patch16_224',\n",
       " 'vit_base_patch16_224_miil',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch16_clip_224',\n",
       " 'vit_base_patch16_clip_384',\n",
       " 'vit_base_patch16_clip_quickgelu_224',\n",
       " 'vit_base_patch16_gap_224',\n",
       " 'vit_base_patch16_plus_240',\n",
       " 'vit_base_patch16_plus_clip_240',\n",
       " 'vit_base_patch16_reg4_gap_256',\n",
       " 'vit_base_patch16_rope_reg1_gap_256',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# timm.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0f7b0",
   "metadata": {},
   "source": [
    "### Transfer Learning with EfficientNet_B3\n",
    "\n",
    "In this step, we utilize the EfficientNet_B3 model for transfer learning. The model is pre-trained on ImageNet, and we adapt it to our specific task by modifying the classifier layer to match the number of output classes (`num_classes`). \n",
    "\n",
    "We freeze all layers except the classifier to retain the pre-trained features while allowing the classifier to learn task-specific features. The model summary provides an overview of the architecture and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3442708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEfficientNetB3_model():\n",
    "    \"\"\"\n",
    "    Function to get the EfficientNet B3 model with pretrained weights.\n",
    "    Returns:\n",
    "        model: A PyTorch model instance of EfficientNet B3.\n",
    "    \"\"\"\n",
    "    # Load the EfficientNet B3 model with pretrained weights\n",
    "    model = timm.create_model('efficientnet_b3', pretrained=True)\n",
    "    \n",
    "    # Modify the classifier for binary classification\n",
    "    num_classes = 1  # For binary classification (OK/NOK)\n",
    "    model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n",
    "\n",
    "    # Freeze all layers except the classifier\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Wrap the model in the LightningModule\n",
    "# from models.model_transferlearning import TransferLearningModule\n",
    "# lightning_model = TransferLearningModule(model, config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a93c6",
   "metadata": {},
   "source": [
    "### Data Preparation for EfficientNet_B3\n",
    "\n",
    "To prepare the data for training with EfficientNet_B3, we define a set of image transformations that resize all images to 300x300 pixels, convert them to tensors, and normalize them using the standard ImageNet mean and standard deviation. These transformations ensure compatibility with the input requirements of the EfficientNet architecture.\n",
    "\n",
    "We then instantiate the `BinaryCIFARDataModule` with the defined transformations, batch size, and number of workers from the configuration. After setup, we create the training, validation, and test data loaders. The sizes of each dataset split are printed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaac6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2263\n",
      "Validation dataset size: 487\n",
      "Test dataset size: 485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # TODO: Happens within hyperparameter tuning\n",
    "\n",
    "# import data.custom_transforms as custom_transforms\n",
    "# size = 128                                              # Size for the input images, matching EfficientNet input size\n",
    "# transform = v2.Compose([\n",
    "#     custom_transforms.CenterCropSquare(),\n",
    "#     v2.Resize((size, size)),\n",
    "#     v2.ToTensor(),\n",
    "#     v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# dm = BinaryImageDataModule(data_dir=config['path_to_split_aug_pics'], transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "# dm.setup()\n",
    "# train_loader = dm.train_dataloader()\n",
    "# val_loader = dm.val_dataloader()\n",
    "# test_loader = dm.test_dataloader()\n",
    "\n",
    "# print('Train dataset size:', len(dm.train_dataset))\n",
    "# print('Validation dataset size:', len(dm.val_dataset))\n",
    "# print('Test dataset size:', len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005d275",
   "metadata": {},
   "source": [
    "### Training and Logging with Weights & Biases including Hyper-Parameter Tuning\n",
    "\n",
    "In this step, we initialize the Weights & Biases (wandb) logger to track experiment metrics, hyperparameters, and model checkpoints. The logger is configured with project and experiment names, as well as key training parameters such as dataset, batch size, maximum epochs, and learning rate.\n",
    "\n",
    "We then set up the PyTorch Lightning `Trainer` with the wandb logger and an early stopping callback to monitor validation loss. The model is trained using the specified datamodule, and all relevant metrics are automatically logged to wandb for further analysis and visualization. After training, wandb logging is finalized to ensure all data is properly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b648e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 17:48:51,410] A new study created in memory with name: no-name-3aae8c30-991f-485f-ad09-ae1f0ff0bbf9\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_174851-p5m4is8j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/p5m4is8j' target=\"_blank\">TL_bs32_img128_optAdam_lr2e-03_wd2e-06_sch_None_2025-05-27_17-48-51</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/p5m4is8j' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/p5m4is8j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td> </td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.91322</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>train_acc</td><td>0.8882</td></tr><tr><td>train_loss</td><td>0.35187</td></tr><tr><td>trainer/global_step</td><td>161</td></tr><tr><td>val_acc</td><td>0.86448</td></tr><tr><td>val_loss</td><td>0.36025</td></tr><tr><td>val_precision</td><td>0.83448</td></tr><tr><td>val_recall</td><td>0.74233</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs32_img128_optAdam_lr2e-03_wd2e-06_sch_None_2025-05-27_17-48-51</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/p5m4is8j' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/p5m4is8j</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 30 media file(s), 40 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_174851-p5m4is8j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 17:52:34,707] Trial 0 finished with value: 0.36025136709213257 and parameters: {'batch_size': 32, 'image_size': 128, 'max_epochs': 21, 'accumulate_grad_batches': 4, 'precision': 32, 'optimizer': 'Adam', 'learning_rate': 0.0020743602465473315, 'weight_decay': 1.907816047492283e-06, 'scheduler': None}. Best is trial 0 with value: 0.36025136709213257.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_175235-ff92nzad</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ff92nzad' target=\"_blank\">TL_bs32_img256_optAdamW_lr5e-03_wd4e-06_sch_None_2025-05-27_17-52-34</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ff92nzad' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ff92nzad</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "`Trainer.fit` stopped: `max_epochs=14` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td> </td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98826</td></tr><tr><td>epoch</td><td>13</td></tr><tr><td>train_acc</td><td>0.98011</td></tr><tr><td>train_loss</td><td>0.0255</td></tr><tr><td>trainer/global_step</td><td>503</td></tr><tr><td>val_acc</td><td>0.9384</td></tr><tr><td>val_loss</td><td>0.13282</td></tr><tr><td>val_precision</td><td>0.92357</td></tr><tr><td>val_recall</td><td>0.88957</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs32_img256_optAdamW_lr5e-03_wd4e-06_sch_None_2025-05-27_17-52-34</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ff92nzad' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ff92nzad</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 45 media file(s), 60 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_175235-ff92nzad\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 17:58:26,686] Trial 1 finished with value: 0.1328192800283432 and parameters: {'batch_size': 32, 'image_size': 256, 'max_epochs': 14, 'accumulate_grad_batches': 2, 'precision': 32, 'optimizer': 'AdamW', 'learning_rate': 0.004740969623212949, 'weight_decay': 4.283432242699991e-06, 'scheduler': None}. Best is trial 1 with value: 0.1328192800283432.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_175827-k9j6b2va</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/k9j6b2va' target=\"_blank\">TL_bs128_img192_optSGD_lr7e-03_wd7e-04_sch_StepLR_2025-05-27_17-58-27</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/k9j6b2va' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/k9j6b2va</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "`Trainer.fit` stopped: `max_epochs=23` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.97504</td></tr><tr><td>epoch</td><td>22</td></tr><tr><td>train_acc</td><td>0.95051</td></tr><tr><td>train_loss</td><td>0.17912</td></tr><tr><td>trainer/global_step</td><td>413</td></tr><tr><td>val_acc</td><td>0.91786</td></tr><tr><td>val_loss</td><td>0.20688</td></tr><tr><td>val_precision</td><td>0.91275</td></tr><tr><td>val_recall</td><td>0.83436</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs128_img192_optSGD_lr7e-03_wd7e-04_sch_StepLR_2025-05-27_17-58-27</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/k9j6b2va' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/k9j6b2va</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 72 media file(s), 96 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_175827-k9j6b2va\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 18:07:43,438] Trial 2 finished with value: 0.2068791538476944 and parameters: {'batch_size': 128, 'image_size': 192, 'max_epochs': 23, 'accumulate_grad_batches': 1, 'precision': '16-mixed', 'optimizer': 'SGD', 'learning_rate': 0.007496292266846763, 'weight_decay': 0.0006585603469165824, 'scheduler': 'StepLR'}. Best is trial 1 with value: 0.1328192800283432.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_180743-x6xj1ima</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/x6xj1ima' target=\"_blank\">TL_bs128_img192_optSGD_lr3e-03_wd4e-04_sch_None_2025-05-27_18-07-43</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/x6xj1ima' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/x6xj1ima</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "`Trainer.fit` stopped: `max_epochs=23` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.95172</td></tr><tr><td>epoch</td><td>22</td></tr><tr><td>train_acc</td><td>0.88732</td></tr><tr><td>train_loss</td><td>0.34608</td></tr><tr><td>trainer/global_step</td><td>114</td></tr><tr><td>val_acc</td><td>0.88296</td></tr><tr><td>val_loss</td><td>0.34909</td></tr><tr><td>val_precision</td><td>0.88406</td></tr><tr><td>val_recall</td><td>0.74847</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs128_img192_optSGD_lr3e-03_wd4e-04_sch_None_2025-05-27_18-07-43</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/x6xj1ima' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/x6xj1ima</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 72 media file(s), 96 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_180743-x6xj1ima\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 18:17:14,323] Trial 3 finished with value: 0.34909090399742126 and parameters: {'batch_size': 128, 'image_size': 192, 'max_epochs': 23, 'accumulate_grad_batches': 4, 'precision': '16-mixed', 'optimizer': 'SGD', 'learning_rate': 0.0027530670919164394, 'weight_decay': 0.0004145193135349147, 'scheduler': None}. Best is trial 1 with value: 0.1328192800283432.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_181714-6o4we8fx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/6o4we8fx' target=\"_blank\">TL_bs192_img128_optSGD_lr8e-03_wd3e-05_sch_CosineAnnealingLR_2025-05-27_18-17-14</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/6o4we8fx' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/6o4we8fx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.92456</td></tr><tr><td>epoch</td><td>32</td></tr><tr><td>train_acc</td><td>0.92223</td></tr><tr><td>train_loss</td><td>0.2239</td></tr><tr><td>trainer/global_step</td><td>395</td></tr><tr><td>val_acc</td><td>0.86242</td></tr><tr><td>val_loss</td><td>0.32406</td></tr><tr><td>val_precision</td><td>0.85294</td></tr><tr><td>val_recall</td><td>0.71166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs192_img128_optSGD_lr8e-03_wd3e-05_sch_CosineAnnealingLR_2025-05-27_18-17-14</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/6o4we8fx' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/6o4we8fx</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 102 media file(s), 136 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_181714-6o4we8fx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 18:30:32,788] Trial 4 finished with value: 0.3240637481212616 and parameters: {'batch_size': 192, 'image_size': 128, 'max_epochs': 49, 'accumulate_grad_batches': 1, 'precision': 32, 'optimizer': 'SGD', 'learning_rate': 0.008094944344815178, 'weight_decay': 3.060395126277435e-05, 'scheduler': 'CosineAnnealingLR'}. Best is trial 1 with value: 0.1328192800283432.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_183033-uq6pil5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uq6pil5v' target=\"_blank\">TL_bs64_img192_optAdamW_lr1e-04_wd2e-04_sch_None_2025-05-27_18-30-33</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uq6pil5v' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uq6pil5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "`Trainer.fit` stopped: `max_epochs=22` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td> </td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.96476</td></tr><tr><td>epoch</td><td>21</td></tr><tr><td>train_acc</td><td>0.91516</td></tr><tr><td>train_loss</td><td>0.28601</td></tr><tr><td>trainer/global_step</td><td>791</td></tr><tr><td>val_acc</td><td>0.9076</td></tr><tr><td>val_loss</td><td>0.29688</td></tr><tr><td>val_precision</td><td>0.90411</td></tr><tr><td>val_recall</td><td>0.80982</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs64_img192_optAdamW_lr1e-04_wd2e-04_sch_None_2025-05-27_18-30-33</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uq6pil5v' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/uq6pil5v</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 69 media file(s), 92 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_183033-uq6pil5v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 18:39:12,654] Trial 5 finished with value: 0.296880304813385 and parameters: {'batch_size': 64, 'image_size': 192, 'max_epochs': 22, 'accumulate_grad_batches': 1, 'precision': 32, 'optimizer': 'AdamW', 'learning_rate': 0.00013528410687010343, 'weight_decay': 0.0001653614528560793, 'scheduler': None}. Best is trial 1 with value: 0.1328192800283432.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_183913-4ueud4dy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/4ueud4dy' target=\"_blank\">TL_bs64_img192_optAdamW_lr1e-04_wd4e-04_sch_CosineAnnealingLR_2025-05-27_18-39-12</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/4ueud4dy' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/4ueud4dy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td> </td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.95111</td></tr><tr><td>epoch</td><td>43</td></tr><tr><td>train_acc</td><td>0.88599</td></tr><tr><td>train_loss</td><td>0.3809</td></tr><tr><td>trainer/global_step</td><td>791</td></tr><tr><td>val_acc</td><td>0.86858</td></tr><tr><td>val_loss</td><td>0.37938</td></tr><tr><td>val_precision</td><td>0.88372</td></tr><tr><td>val_recall</td><td>0.69939</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs64_img192_optAdamW_lr1e-04_wd4e-04_sch_CosineAnnealingLR_2025-05-27_18-39-12</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/4ueud4dy' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/4ueud4dy</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 135 media file(s), 180 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_183913-4ueud4dy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 18:56:28,494] Trial 6 finished with value: 0.37938162684440613 and parameters: {'batch_size': 64, 'image_size': 192, 'max_epochs': 45, 'accumulate_grad_batches': 2, 'precision': 32, 'optimizer': 'AdamW', 'learning_rate': 0.00010767086727187348, 'weight_decay': 0.0004496122085886436, 'scheduler': 'CosineAnnealingLR'}. Best is trial 1 with value: 0.1328192800283432.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_185628-ojjbydjn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ojjbydjn' target=\"_blank\">TL_bs64_img128_optAdam_lr1e-03_wd4e-04_sch_None_2025-05-27_18-56-28</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ojjbydjn' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ojjbydjn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "`Trainer.fit` stopped: `max_epochs=13` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td> </td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.9153</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>train_acc</td><td>0.89085</td></tr><tr><td>train_loss</td><td>0.2967</td></tr><tr><td>trainer/global_step</td><td>116</td></tr><tr><td>val_acc</td><td>0.85421</td></tr><tr><td>val_loss</td><td>0.3495</td></tr><tr><td>val_precision</td><td>0.85938</td></tr><tr><td>val_recall</td><td>0.67485</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs64_img128_optAdam_lr1e-03_wd4e-04_sch_None_2025-05-27_18-56-28</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ojjbydjn' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/ojjbydjn</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 42 media file(s), 56 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_185628-ojjbydjn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 19:01:31,682] Trial 7 finished with value: 0.3495047390460968 and parameters: {'batch_size': 64, 'image_size': 128, 'max_epochs': 13, 'accumulate_grad_batches': 4, 'precision': 32, 'optimizer': 'Adam', 'learning_rate': 0.001423202679888196, 'weight_decay': 0.0004048506290608345, 'scheduler': None}. Best is trial 1 with value: 0.1328192800283432.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_190132-tamnx8qa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/tamnx8qa' target=\"_blank\">TL_bs128_img192_optAdam_lr3e-03_wd3e-03_sch_CosineAnnealingLR_2025-05-27_19-01-31</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/tamnx8qa' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/tamnx8qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "`Trainer.fit` stopped: `max_epochs=18` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.97478</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>train_acc</td><td>0.95493</td></tr><tr><td>train_loss</td><td>0.18193</td></tr><tr><td>trainer/global_step</td><td>161</td></tr><tr><td>val_acc</td><td>0.91786</td></tr><tr><td>val_loss</td><td>0.20479</td></tr><tr><td>val_precision</td><td>0.90196</td></tr><tr><td>val_recall</td><td>0.84663</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs128_img192_optAdam_lr3e-03_wd3e-03_sch_CosineAnnealingLR_2025-05-27_19-01-31</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/tamnx8qa' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/tamnx8qa</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 57 media file(s), 76 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_190132-tamnx8qa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 19:08:44,136] Trial 8 finished with value: 0.20479261875152588 and parameters: {'batch_size': 128, 'image_size': 192, 'max_epochs': 18, 'accumulate_grad_batches': 2, 'precision': 32, 'optimizer': 'Adam', 'learning_rate': 0.0031588040473680096, 'weight_decay': 0.0034800226113276245, 'scheduler': 'CosineAnnealingLR'}. Best is trial 1 with value: 0.1328192800283432.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_190844-anaxxtu6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/anaxxtu6' target=\"_blank\">TL_bs32_img192_optAdam_lr2e-04_wd4e-03_sch_StepLR_2025-05-27_19-08-44</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/anaxxtu6' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/anaxxtu6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "`Trainer.fit` stopped: `max_epochs=29` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td> </td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.96344</td></tr><tr><td>epoch</td><td>28</td></tr><tr><td>train_acc</td><td>0.90632</td></tr><tr><td>train_loss</td><td>0.16264</td></tr><tr><td>trainer/global_step</td><td>1043</td></tr><tr><td>val_acc</td><td>0.89528</td></tr><tr><td>val_loss</td><td>0.28928</td></tr><tr><td>val_precision</td><td>0.9058</td></tr><tr><td>val_recall</td><td>0.76687</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_bs32_img192_optAdam_lr2e-04_wd4e-03_sch_StepLR_2025-05-27_19-08-44</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/anaxxtu6' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/anaxxtu6</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 90 media file(s), 120 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_190844-anaxxtu6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 19:20:12,781] Trial 9 finished with value: 0.2892792522907257 and parameters: {'batch_size': 32, 'image_size': 192, 'max_epochs': 29, 'accumulate_grad_batches': 2, 'precision': '16-mixed', 'optimizer': 'Adam', 'learning_rate': 0.00020468546278564826, 'weight_decay': 0.0036746271067120543, 'scheduler': 'StepLR'}. Best is trial 1 with value: 0.1328192800283432.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "{'batch_size': 32, 'image_size': 256, 'max_epochs': 14, 'accumulate_grad_batches': 2, 'precision': 32, 'optimizer': 'AdamW', 'learning_rate': 0.004740969623212949, 'weight_decay': 4.283432242699991e-06, 'scheduler': None}\n",
      "Best value (val_loss): 0.1328192800283432\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(study.best_trial.params)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest value (val_loss):\u001b[39m\u001b[33m\"\u001b[39m, study.best_value)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43moptuna\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m optuna.visualization.plot_param_importances(study)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\visualization\\_optimization_history.py:200\u001b[39m, in \u001b[36mplot_optimization_history\u001b[39m\u001b[34m(study, target, target_name, error_bar)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_optimization_history\u001b[39m(\n\u001b[32m    173\u001b[39m     study: Study | Sequence[Study],\n\u001b[32m    174\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    177\u001b[39m     error_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    178\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    179\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[32m    180\u001b[39m \n\u001b[32m    181\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m \u001b[33;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     info_list = _get_optimization_history_info_list(study, target, target_name, error_bar)\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_optimization_history_plot(info_list, target_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "config['sweep_id'] = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "def objective(trial):\n",
    "    model = getEfficientNetB3_model()\n",
    "    trainer = OptunaTrainer(\n",
    "        model=model,\n",
    "        config=config,\n",
    "        normalize_mean=[0.485, 0.456, 0.406], \n",
    "        normalize_std=[0.229, 0.224, 0.225],\n",
    "        dataset_name=\"DwarfRabbits-binary\"\n",
    "    )\n",
    "    return trainer.run_training(trial)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")  # because we minimize val_loss\n",
    "study.optimize(objective, n_trials=config['number_of_trials'])\n",
    "\n",
    "# Best result\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best value (val_loss):\", study.best_value)\n",
    "\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import data.custom_transforms as custom_transforms\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     # Data related hyperparameters    \n",
    "#     batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 192])\n",
    "#     image_size = trial.suggest_categorical(\"image_size\", [128, 192, 256])\n",
    "\n",
    "#     # Trainer hyperparameters\n",
    "#     max_epochs = trial.suggest_int(\"max_epochs\", 10, 50)                                            # Maximum number of epochs to train the model\n",
    "#     accumulate_grad_batches = trial.suggest_categorical(\"accumulate_grad_batches\", [1, 2, 4])       # Accumulate gradients over multiple batches before updating weights\n",
    "#     precision = trial.suggest_categorical(\"precision\", [\"16-mixed\", 32])                                    # Precision for training, 16 for mixed precision, 32 for full precision\n",
    "\n",
    "#     # Optimizer hyperparameters\n",
    "#     optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"])               # Optimizer type\n",
    "#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)                      # Learning rate (size of the step taken in the direction of the gradient)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)                        # Weight decay (L2 penality on the weights to prevent overfitting by discouraging large weights)\n",
    "\n",
    "#     # Learning rate scheduler hyperparameters\n",
    "#     scheduler_name = trial.suggest_categorical(\"scheduler\", [\"StepLR\", \"CosineAnnealingLR\", None])  # Learning rate scheduler type\n",
    "\n",
    "#     # Model related hyperparameters\n",
    "#     # dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    \n",
    "#     # Update config\n",
    "#     config['batch_size'] = batch_size\n",
    "#     config['image_size'] = image_size\n",
    "\n",
    "#     config['max_epochs'] = max_epochs\n",
    "#     config['accumulate_grad_batches'] = accumulate_grad_batches\n",
    "#     config['precision'] = precision\n",
    "\n",
    "#     config['optimizer'] = optimizer_name\n",
    "#     config['learning_rate'] = learning_rate\n",
    "#     config['weight_decay'] = weight_decay\n",
    "\n",
    "#     config['scheduler'] = scheduler_name\n",
    "\n",
    "#     config['wandb_experiment_name'] = (\n",
    "#         f\"TL_bs{batch_size}\"\n",
    "#         f\"_img{image_size}\"\n",
    "#         f\"_opt{optimizer_name}\"\n",
    "#         f\"_lr{learning_rate:.0e}\"\n",
    "#         f\"_wd{weight_decay:.0e}\"\n",
    "#         f\"_sch_{scheduler_name if scheduler_name else 'None'}\"\n",
    "#         f\"_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "#     )\n",
    "\n",
    "#     # WandB logger\n",
    "#     wandb_logger = WandbLogger(\n",
    "#         project=config['wandb_project_name'],\n",
    "#         name=config['wandb_experiment_name'],\n",
    "#         config={\n",
    "#             'batch_size': config['batch_size'],\n",
    "#             'image_size': config['image_size'],\n",
    "#             'max_epochs': config['max_epochs'],\n",
    "#             'accumulate_grad_batches': config['accumulate_grad_batches'],\n",
    "#             'precision': config['precision'],\n",
    "#             'optimizer': config['optimizer'],\n",
    "#             'learning_rate': config['learning_rate'],\n",
    "#             'weight_decay': config['weight_decay'],\n",
    "#             'scheduler': config['scheduler'],\n",
    "#             'dataset': 'DwarfRabbits-binary'\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # Create model and datamodule\n",
    "#     lightning_model = TransferLearningModule(\n",
    "#         model, \n",
    "#         learning_rate=config['learning_rate'],\n",
    "#         optimizer_name=config['optimizer'],\n",
    "#         weight_decay=config['weight_decay'],\n",
    "#         scheduler_name=config['scheduler'],\n",
    "#         )\n",
    "    \n",
    "#     # Define transformations                                       \n",
    "#     transform = v2.Compose([\n",
    "#         custom_transforms.CenterCropSquare(),\n",
    "#         v2.Resize((image_size, image_size)),\n",
    "#         v2.ToTensor(),\n",
    "#         v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     dm = dm = BinaryImageDataModule(\n",
    "#         data_dir=config['path_to_split_aug_pics'], \n",
    "#         transform=transform, \n",
    "#         batch_size=config['batch_size'], \n",
    "#         num_workers=2, \n",
    "#         persistent_workers=True\n",
    "#     )\n",
    "\n",
    "#     # Trainer\n",
    "#     trainer = Trainer(\n",
    "#         max_epochs=config['max_epochs'],\n",
    "#         precision=config['precision'],\n",
    "#         accumulate_grad_batches=config['accumulate_grad_batches'],\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=\"auto\",\n",
    "#         strategy=\"auto\",\n",
    "#         callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3, mode='min')],\n",
    "#         logger=wandb_logger,\n",
    "#         enable_progress_bar=False,  # Speeds up Optuna trials\n",
    "#         log_every_n_steps=10,  # Log every 10 steps\n",
    "#     )\n",
    "\n",
    "#     # Train\n",
    "#     try:\n",
    "#         trainer.fit(model=lightning_model, datamodule=dm)\n",
    "#         checkpoint_path = f\"checkpoints/{config['wandb_experiment_name']}.ckpt\"\n",
    "#         trainer.save_checkpoint(checkpoint_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during training: {e}\")\n",
    "#         wandb.finish()\n",
    "#         return float(\"inf\")\n",
    "\n",
    "#     # Get best validation score\n",
    "#     val_loss = trainer.callback_metrics.get(\"val_loss\")\n",
    "#     wandb.finish()\n",
    "\n",
    "#     # Return float for Optuna to minimize\n",
    "#     return val_loss.item() if val_loss else float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdea599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250527_105023-cxb8avx1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1' target=\"_blank\">binaryClassification_tl_2025-05-27_10-50-23</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | EfficientNet      | 10.7 M | train\n",
      "1 | criterion      | BCEWithLogitsLoss | 0      | train\n",
      "2 | sigmoid        | Sigmoid           | 0      | train\n",
      "3 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "4 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "5 | val_precision  | BinaryPrecision   | 0      | train\n",
      "6 | val_recall     | BinaryRecall      | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "10.7 M    Non-trainable params\n",
      "10.7 M    Total params\n",
      "42.791    Total estimated model params size (MB)\n",
      "539       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 12/12 [00:26<00:00,  0.45it/s, v_num=avx1, val_loss=0.360]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.90943</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>train_acc</td><td>0.87008</td></tr><tr><td>train_loss</td><td>0.33155</td></tr><tr><td>trainer/global_step</td><td>71</td></tr><tr><td>val_acc</td><td>0.846</td></tr><tr><td>val_loss</td><td>0.36006</td></tr><tr><td>val_precision</td><td>0.80137</td></tr><tr><td>val_recall</td><td>0.71779</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">binaryClassification_tl_2025-05-27_10-50-23</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen/runs/cxb8avx1</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen</a><br>Synced 5 W&B file(s), 21 media file(s), 28 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250527_105023-cxb8avx1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved as C:\\Users\\lukas\\SynologyDrive_IMS/SS25_MSYS_KAER-AI-PoseAct/21_Test_Data/Models/CNN\\binaryClassification_tl_2025-05-27_10-50-23.ckpt\n"
     ]
    }
   ],
   "source": [
    "# # Initialize the Wandb logger\n",
    "# # add time to the name of the experiment\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# # Initialize wandb logger\n",
    "# wandb_logger = WandbLogger(\n",
    "#     project=config['wandb_project_name'],\n",
    "#     name=config['wandb_experiment_name'] + '_' + current_time,\n",
    "#     config={\n",
    "#         'dataset': 'CIFAR-binary',\n",
    "#         'batch_size': config['batch_size'],\n",
    "#         'max_epochs': config['max_epochs'],\n",
    "#         'learning_rate': config['learning_rate']\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Train the model and log relevant metrics using PyTorch Lightning Trainer and WandbLogger\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=config['max_epochs'],\n",
    "#     default_root_dir='model/checkpoint/',\n",
    "#     accelerator=\"auto\",\n",
    "#     devices=\"auto\",\n",
    "#     strategy=\"auto\",\n",
    "#     callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n",
    "#     logger=wandb_logger\n",
    "# )\n",
    "\n",
    "# # Train of the model\n",
    "# trainer.fit(model=lightning_model, datamodule=dm)\n",
    "\n",
    "# # Finish wandb logging\n",
    "# wandb.finish()\n",
    "\n",
    "# # Create a filename with date identifier\n",
    "# model_filename = f\"{config['wandb_experiment_name']}_{current_time}.ckpt\"\n",
    "\n",
    "# # Save the model's state_dict to the path specified in config\n",
    "# save_path = os.path.join(os.path.dirname(config['path_to_models']), model_filename)\n",
    "# trainer.save_checkpoint(save_path)\n",
    "# print(f\"Model checkpoint saved as {save_path}\")\n",
    "# config['path_to_model'] = save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97973a8",
   "metadata": {},
   "source": [
    "### Transfer Learning with ResNet50D\n",
    "\n",
    "In this step, we utilize the ResNet50D model for transfer learning. The model is pre-trained on ImageNet, and we adapt it to our specific task by modifying the fully connected (`fc`) layer to match the number of output classes (`num_classes`).\n",
    "\n",
    "We freeze all layers except the `fc` layer to retain the pre-trained features while allowing the classifier to learn task-specific features. The model summary provides an overview of the architecture and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a00428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 1]                    --\n",
       "Sequential: 1-1                        [1, 64, 112, 112]         --\n",
       "    Conv2d: 2-1                       [1, 32, 112, 112]         (864)\n",
       "    BatchNorm2d: 2-2                  [1, 32, 112, 112]         (64)\n",
       "    ReLU: 2-3                         [1, 32, 112, 112]         --\n",
       "    Conv2d: 2-4                       [1, 32, 112, 112]         (9,216)\n",
       "    BatchNorm2d: 2-5                  [1, 32, 112, 112]         (64)\n",
       "    ReLU: 2-6                         [1, 32, 112, 112]         --\n",
       "    Conv2d: 2-7                       [1, 64, 112, 112]         (18,432)\n",
       "BatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
       "ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "Sequential: 1-5                        [1, 256, 56, 56]          --\n",
       "    Bottleneck: 2-8                   [1, 256, 56, 56]          (75,008)\n",
       "    Bottleneck: 2-9                   [1, 256, 56, 56]          (70,400)\n",
       "    Bottleneck: 2-10                  [1, 256, 56, 56]          (70,400)\n",
       "Sequential: 1-6                        [1, 512, 28, 28]          --\n",
       "    Bottleneck: 2-11                  [1, 512, 28, 28]          (379,392)\n",
       "    Bottleneck: 2-12                  [1, 512, 28, 28]          (280,064)\n",
       "    Bottleneck: 2-13                  [1, 512, 28, 28]          (280,064)\n",
       "    Bottleneck: 2-14                  [1, 512, 28, 28]          (280,064)\n",
       "Sequential: 1-7                        [1, 1024, 14, 14]         --\n",
       "    Bottleneck: 2-15                  [1, 1024, 14, 14]         (1,512,448)\n",
       "    Bottleneck: 2-16                  [1, 1024, 14, 14]         (1,117,184)\n",
       "    Bottleneck: 2-17                  [1, 1024, 14, 14]         (1,117,184)\n",
       "    Bottleneck: 2-18                  [1, 1024, 14, 14]         (1,117,184)\n",
       "    Bottleneck: 2-19                  [1, 1024, 14, 14]         (1,117,184)\n",
       "    Bottleneck: 2-20                  [1, 1024, 14, 14]         (1,117,184)\n",
       "Sequential: 1-8                        [1, 2048, 7, 7]           --\n",
       "    Bottleneck: 2-21                  [1, 2048, 7, 7]           (6,039,552)\n",
       "    Bottleneck: 2-22                  [1, 2048, 7, 7]           (4,462,592)\n",
       "    Bottleneck: 2-23                  [1, 2048, 7, 7]           (4,462,592)\n",
       "SelectAdaptivePool2d: 1-9              [1, 2048]                 --\n",
       "    AdaptiveAvgPool2d: 2-24           [1, 2048, 1, 1]           --\n",
       "    Flatten: 2-25                     [1, 2048]                 --\n",
       "Linear: 1-10                           [1, 1]                    (2,049)\n",
       "==========================================================================================\n",
       "Total params: 23,529,313\n",
       "Trainable params: 0\n",
       "Non-trainable params: 23,529,313\n",
       "Total mult-adds (Units.GIGABYTES): 4.33\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 190.67\n",
       "Params size (MB): 94.12\n",
       "Estimated Total Size (MB): 285.39\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "model = timm.create_model(\n",
    "    'resnet50d',      # Hardcoded for now\n",
    "    pretrained=True,\n",
    ")\n",
    "# Define number of classes and classifier\n",
    "num_classes = 1             # Hardcoded for now, Dwarf Rabbit OK/NOK output    \n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Freeze all layers except the classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Print model summary\n",
    "summary(model, input_size=(1, 3, 224, 224), depth=2)\n",
    "\n",
    "# Wrap the model in the LightningModule\n",
    "from models.model_transferlearning import TransferLearningModule\n",
    "lightning_model = TransferLearningModule(model, config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460f881",
   "metadata": {},
   "source": [
    "### Data Preparation for ResNet50D\n",
    "\n",
    "To prepare the data for training with ResNet50D, we define a set of image transformations that resize all images to 224x224 pixels, convert them to tensors, and normalize them using the standard ImageNet mean and standard deviation. These transformations ensure compatibility with the input requirements of the ResNet architecture.\n",
    "\n",
    "We then instantiate the `BinaryCIFARDataModule` with the defined transformations, batch size, and number of workers from the configuration. After setup, we create the training, validation, and test data loaders. The sizes of each dataset split are printed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations required for the used model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match ResNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dm = BinaryCIFARDataModule(transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "print('Train dataset size:', len(dm.train_dataset))\n",
    "print('Validation dataset size:', len(dm.val_dataset))\n",
    "print('Test dataset size:', len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398ae57",
   "metadata": {},
   "source": [
    "### Training and Logging with Weights & Biases\n",
    "\n",
    "In this step, we initialize the Weights & Biases (wandb) logger to track experiment metrics, hyperparameters, and model checkpoints. The logger is configured with project and experiment names, as well as key training parameters such as dataset, batch size, maximum epochs, and learning rate.\n",
    "\n",
    "We then set up the PyTorch Lightning `Trainer` with the wandb logger and an early stopping callback to monitor validation loss. The model is trained using the specified datamodule, and all relevant metrics are automatically logged to wandb for further analysis and visualization. After training, wandb logging is finalized to ensure all data is properly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e566d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Wandb logger\n",
    "wandb_logger = WandbLogger(\n",
    "    project=config['wandb_project_name'],\n",
    "    name=config['wandb_experiment_name'],\n",
    "    config={\n",
    "        'dataset': 'CIFAR-binary',\n",
    "        'batch_size': config['batch_size'],\n",
    "        'max_epochs': config['max_epochs'],\n",
    "        'learning_rate': config['learning_rate']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the model and log relevant metrics using PyTorch Lightning Trainer and WandbLogger\n",
    "trainer = Trainer(\n",
    "    max_epochs=config['max_epochs'],\n",
    "    default_root_dir='model/checkpoint/',\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n",
    "    logger=wandb_logger\n",
    ")\n",
    "\n",
    "trainer.fit(model=lightning_model, datamodule=dm)\n",
    "\n",
    "# Finish wandb logging\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672aec7a",
   "metadata": {},
   "source": [
    "### Transfer Learning with InceptionV4\n",
    "\n",
    "In this step, we utilize the InceptionV4 model for transfer learning. The model is pre-trained on ImageNet, and we adapt it to our specific task by modifying the `last_linear` layer to match the number of output classes (`num_classes`).\n",
    "\n",
    "We freeze all layers except the `last_linear` layer to retain the pre-trained features while allowing the classifier to learn task-specific features. The model summary provides an overview of the architecture and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a6fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "InceptionV4                                        [1, 1]                    --\n",
       "Sequential: 1-1                                  [1, 1536, 8, 8]           --\n",
       "    ConvNormAct: 2-1                            [1, 32, 149, 149]         (928)\n",
       "    ConvNormAct: 2-2                            [1, 32, 147, 147]         (9,280)\n",
       "    ConvNormAct: 2-3                            [1, 64, 147, 147]         (18,560)\n",
       "    Mixed3a: 2-4                                [1, 160, 73, 73]          (55,488)\n",
       "    Mixed4a: 2-5                                [1, 192, 71, 71]          (189,312)\n",
       "    Mixed5a: 2-6                                [1, 384, 35, 35]          (332,160)\n",
       "    InceptionA: 2-7                             [1, 384, 35, 35]          (317,632)\n",
       "    InceptionA: 2-8                             [1, 384, 35, 35]          (317,632)\n",
       "    InceptionA: 2-9                             [1, 384, 35, 35]          (317,632)\n",
       "    InceptionA: 2-10                            [1, 384, 35, 35]          (317,632)\n",
       "    ReductionA: 2-11                            [1, 1024, 17, 17]         (2,306,112)\n",
       "    InceptionB: 2-12                            [1, 1024, 17, 17]         (2,936,256)\n",
       "    InceptionB: 2-13                            [1, 1024, 17, 17]         (2,936,256)\n",
       "    InceptionB: 2-14                            [1, 1024, 17, 17]         (2,936,256)\n",
       "    InceptionB: 2-15                            [1, 1024, 17, 17]         (2,936,256)\n",
       "    InceptionB: 2-16                            [1, 1024, 17, 17]         (2,936,256)\n",
       "    InceptionB: 2-17                            [1, 1024, 17, 17]         (2,936,256)\n",
       "    InceptionB: 2-18                            [1, 1024, 17, 17]         (2,936,256)\n",
       "    ReductionB: 2-19                            [1, 1536, 8, 8]           (2,747,392)\n",
       "    InceptionC: 2-20                            [1, 1536, 8, 8]           (4,553,088)\n",
       "    InceptionC: 2-21                            [1, 1536, 8, 8]           (4,553,088)\n",
       "    InceptionC: 2-22                            [1, 1536, 8, 8]           (4,553,088)\n",
       "SelectAdaptivePool2d: 1-2                        [1, 1536]                 --\n",
       "    AdaptiveAvgPool2d: 2-23                     [1, 1536, 1, 1]           --\n",
       "    Flatten: 2-24                               [1, 1536]                 --\n",
       "Dropout: 1-3                                     [1, 1536]                 --\n",
       "Linear: 1-4                                      [1, 1]                    (1,537)\n",
       "====================================================================================================\n",
       "Total params: 41,144,353\n",
       "Trainable params: 0\n",
       "Non-trainable params: 41,144,353\n",
       "Total mult-adds (Units.GIGABYTES): 12.25\n",
       "====================================================================================================\n",
       "Input size (MB): 1.07\n",
       "Forward/backward pass size (MB): 120.71\n",
       "Params size (MB): 164.32\n",
       "Estimated Total Size (MB): 286.11\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "model = timm.create_model(\n",
    "    'inception_v4',      # Hardcoded for now\n",
    "    pretrained=True,\n",
    ")\n",
    "# Define number of classes and classifier\n",
    "num_classes = 1             # Hardcoded for now, Dwarf Rabbit OK/NOK output    \n",
    "model.last_linear = torch.nn.Linear(model.last_linear.in_features, num_classes)\n",
    "\n",
    "# Freeze all layers except the classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.last_linear.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Print model summary\n",
    "summary(model, input_size=(1, 3, 299, 299), depth=2)\n",
    "\n",
    "# Wrap the model in the LightningModule\n",
    "from models.model_transferlearning import TransferLearningModule\n",
    "lightning_model = TransferLearningModule(model, config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626bf0f",
   "metadata": {},
   "source": [
    "### Data Preparation for InceptionV4\n",
    "\n",
    "To prepare the data for training with InceptionV4, we define a set of image transformations that resize all images to 299x299 pixels, convert them to tensors, and normalize them using a mean and standard deviation of 0.5 for each channel. These transformations ensure compatibility with the input requirements of the InceptionV4 architecture.\n",
    "\n",
    "We then instantiate the `BinaryCIFARDataModule` with the defined transformations, batch size, and number of workers from the configuration. After setup, we create the training, validation, and test data loaders. The sizes of each dataset split are printed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75954051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations required for the used model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize images to match EfficientNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Standard ImageNet normalization\n",
    "])\n",
    "\n",
    "dm = BinaryCIFARDataModule(transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "print('Train dataset size:', len(dm.train_dataset))\n",
    "print('Validation dataset size:', len(dm.val_dataset))\n",
    "print('Test dataset size:', len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e1ff0",
   "metadata": {},
   "source": [
    "### Training and Logging with Weights & Biases\n",
    "\n",
    "In this step, we initialize the Weights & Biases (wandb) logger to track experiment metrics, hyperparameters, and model checkpoints. The logger is configured with project and experiment names, as well as key training parameters such as dataset, batch size, maximum epochs, and learning rate.\n",
    "\n",
    "We then set up the PyTorch Lightning `Trainer` with the wandb logger and an early stopping callback to monitor validation loss. The model is trained using the specified datamodule, and all relevant metrics are automatically logged to wandb for further analysis and visualization. After training, wandb logging is finalized to ensure all data is properly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b83af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Wandb logger\n",
    "wandb_logger = WandbLogger(\n",
    "    project=config['wandb_project_name'],\n",
    "    name=config['wandb_experiment_name'],\n",
    "    config={\n",
    "        'dataset': 'CIFAR-binary',\n",
    "        'batch_size': config['batch_size'],\n",
    "        'max_epochs': config['max_epochs'],\n",
    "        'learning_rate': config['learning_rate']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the model and log relevant metrics using PyTorch Lightning Trainer and WandbLogger\n",
    "trainer = Trainer(\n",
    "    max_epochs=config['max_epochs'],\n",
    "    default_root_dir='model/checkpoint/',\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n",
    "    logger=wandb_logger\n",
    ")\n",
    "\n",
    "trainer.fit(model=lightning_model, datamodule=dm)\n",
    "\n",
    "# Finish wandb logging\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDKI-Projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
