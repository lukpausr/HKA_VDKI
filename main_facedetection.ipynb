{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4845e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from torchvision.transforms import v2\n",
    "import timm\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "\n",
    "from data.datamodule import MultiClassImageDataModule\n",
    "from models.model_facedetection import TransferLearningModuleMulticlass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117666f9",
   "metadata": {},
   "source": [
    "### Loading Configuration\n",
    "\n",
    "In the following steps, we will load the configuration settings using the `load_configuration` function. The configuration is stored in the `config` variable which will be used throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74321032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Name: DESKTOP-LUKAS\n",
      "Loaded configuration from config/config_lukas.yaml\n"
     ]
    }
   ],
   "source": [
    "from config.load_configuration import load_configuration\n",
    "config = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414378fc",
   "metadata": {},
   "source": [
    "### Setting Seeds for Reproducibility\n",
    "\n",
    "To ensure comparable and reproducible results, we set the random seed using the `seed_everything` function from PyTorch Lightning. This helps in achieving consistent behavior across multiple runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e10d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # disable oneDNN optimizations for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86df64",
   "metadata": {},
   "source": [
    "### Checking for GPU Devices\n",
    "\n",
    "In this step, we check for the availability of GPU devices and print the device currently being used by PyTorch. This ensures that the computations are performed on the most efficient hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b717a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.7.0+cu128\n",
      "Using device:  cuda\n",
      "Cuda Version:  12.8\n",
      "NVIDIA GeForce RTX 5060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Torch Version: ', torch.__version__)\n",
    "print('Using device: ', device)\n",
    "if device.type == 'cuda':\n",
    "    print('Cuda Version: ', torch.version.cuda)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177b86e",
   "metadata": {},
   "source": [
    "### Defining Transformations and Instantiating DataModule\n",
    "\n",
    "In this step, we will define the necessary data transformations and initialize the `MulticlassImageDataModule` with the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df954014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1628\n",
      "Validation dataset size: 355\n",
      "Test dataset size: 355\n",
      "Example train data shape: torch.Size([3, 300, 300])\n",
      "Example train label: tensor([0., 1., 0., 0., 0., 0.])\n",
      "Example val data shape: torch.Size([3, 300, 300])\n",
      "Example val label: tensor([0., 1., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.Resize((300, 300)),  # Resize images to match EfficientNet input size\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
    "])\n",
    "\n",
    "dm = MultiClassImageDataModule(data_dir=config['path_to_bunnie_data_aug'], name_list=config['name_list'], transform=transform, batch_size=config['batch_size'], num_workers=2, persistent_workers=True)\n",
    "dm.setup()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "# test_loader = dm.test_dataloader()\n",
    "test_loader = val_loader  # Use validation data (from Janik) for testing in this example\n",
    "\n",
    "# Show a few images from the training set\n",
    "# from torchvision.utils import make_grid\n",
    "# def show_images(loader):\n",
    "#     images, labels = next(iter(loader))\n",
    "#     images = images[:16]  # Show only the first 16 images\n",
    "#     labels = labels[:16]\n",
    "#     grid = make_grid(images, nrow=4, padding=2)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "#     plt.title('Sample Images')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "#     print(labels[:16])  # Print corresponding labels\n",
    "# # show_images(train_loader)\n",
    "\n",
    "print('Train dataset size:', len(dm.train_dataset))\n",
    "print('Validation dataset size:', len(dm.val_dataset))\n",
    "print('Test dataset size:', len(dm.test_dataset))\n",
    "print('Example train data shape:', dm.train_dataset[0][0].shape)\n",
    "print('Example train label:', dm.train_dataset[0][1])\n",
    "print('Example val data shape:', dm.val_dataset[0][0].shape)\n",
    "print('Example val label:', dm.val_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c560cb9",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "In this step, we will define the model architecture and print its summary using the `ModelSummary` utility from PyTorch Lightning. This provides an overview of the model's layers, parameters, and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3442708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mikag\\.cache\\huggingface\\hub\\models--timm--convnextv2_base.fcmae_ft_in22k_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | Name                               | Type                  | Params | Mode \n",
      "---------------------------------------------------------------------------------------\n",
      "0   | model                              | ConvNeXt              | 87.7 M | train\n",
      "1   | model.stem                         | Sequential            | 6.5 K  | train\n",
      "2   | model.stem.0                       | Conv2d                | 6.3 K  | train\n",
      "3   | model.stem.1                       | LayerNorm2d           | 256    | train\n",
      "4   | model.stages                       | Sequential            | 87.7 M | train\n",
      "5   | model.stages.0                     | ConvNeXtStage         | 418 K  | train\n",
      "6   | model.stages.0.downsample          | Identity              | 0      | train\n",
      "7   | model.stages.0.blocks              | Sequential            | 418 K  | train\n",
      "8   | model.stages.0.blocks.0            | ConvNeXtBlock         | 139 K  | train\n",
      "9   | model.stages.0.blocks.0.conv_dw    | Conv2d                | 6.4 K  | train\n",
      "10  | model.stages.0.blocks.0.norm       | LayerNorm             | 256    | train\n",
      "11  | model.stages.0.blocks.0.mlp        | GlobalResponseNormMlp | 132 K  | train\n",
      "12  | model.stages.0.blocks.0.mlp.fc1    | Linear                | 66.0 K | train\n",
      "13  | model.stages.0.blocks.0.mlp.act    | GELU                  | 0      | train\n",
      "14  | model.stages.0.blocks.0.mlp.drop1  | Dropout               | 0      | train\n",
      "15  | model.stages.0.blocks.0.mlp.grn    | GlobalResponseNorm    | 1.0 K  | train\n",
      "16  | model.stages.0.blocks.0.mlp.fc2    | Linear                | 65.7 K | train\n",
      "17  | model.stages.0.blocks.0.mlp.drop2  | Dropout               | 0      | train\n",
      "18  | model.stages.0.blocks.0.shortcut   | Identity              | 0      | train\n",
      "19  | model.stages.0.blocks.0.drop_path  | Identity              | 0      | train\n",
      "20  | model.stages.0.blocks.1            | ConvNeXtBlock         | 139 K  | train\n",
      "21  | model.stages.0.blocks.1.conv_dw    | Conv2d                | 6.4 K  | train\n",
      "22  | model.stages.0.blocks.1.norm       | LayerNorm             | 256    | train\n",
      "23  | model.stages.0.blocks.1.mlp        | GlobalResponseNormMlp | 132 K  | train\n",
      "24  | model.stages.0.blocks.1.mlp.fc1    | Linear                | 66.0 K | train\n",
      "25  | model.stages.0.blocks.1.mlp.act    | GELU                  | 0      | train\n",
      "26  | model.stages.0.blocks.1.mlp.drop1  | Dropout               | 0      | train\n",
      "27  | model.stages.0.blocks.1.mlp.grn    | GlobalResponseNorm    | 1.0 K  | train\n",
      "28  | model.stages.0.blocks.1.mlp.fc2    | Linear                | 65.7 K | train\n",
      "29  | model.stages.0.blocks.1.mlp.drop2  | Dropout               | 0      | train\n",
      "30  | model.stages.0.blocks.1.shortcut   | Identity              | 0      | train\n",
      "31  | model.stages.0.blocks.1.drop_path  | Identity              | 0      | train\n",
      "32  | model.stages.0.blocks.2            | ConvNeXtBlock         | 139 K  | train\n",
      "33  | model.stages.0.blocks.2.conv_dw    | Conv2d                | 6.4 K  | train\n",
      "34  | model.stages.0.blocks.2.norm       | LayerNorm             | 256    | train\n",
      "35  | model.stages.0.blocks.2.mlp        | GlobalResponseNormMlp | 132 K  | train\n",
      "36  | model.stages.0.blocks.2.mlp.fc1    | Linear                | 66.0 K | train\n",
      "37  | model.stages.0.blocks.2.mlp.act    | GELU                  | 0      | train\n",
      "38  | model.stages.0.blocks.2.mlp.drop1  | Dropout               | 0      | train\n",
      "39  | model.stages.0.blocks.2.mlp.grn    | GlobalResponseNorm    | 1.0 K  | train\n",
      "40  | model.stages.0.blocks.2.mlp.fc2    | Linear                | 65.7 K | train\n",
      "41  | model.stages.0.blocks.2.mlp.drop2  | Dropout               | 0      | train\n",
      "42  | model.stages.0.blocks.2.shortcut   | Identity              | 0      | train\n",
      "43  | model.stages.0.blocks.2.drop_path  | Identity              | 0      | train\n",
      "44  | model.stages.1                     | ConvNeXtStage         | 1.8 M  | train\n",
      "45  | model.stages.1.downsample          | Sequential            | 131 K  | train\n",
      "46  | model.stages.1.downsample.0        | LayerNorm2d           | 256    | train\n",
      "47  | model.stages.1.downsample.1        | Conv2d                | 131 K  | train\n",
      "48  | model.stages.1.blocks              | Sequential            | 1.6 M  | train\n",
      "49  | model.stages.1.blocks.0            | ConvNeXtBlock         | 540 K  | train\n",
      "50  | model.stages.1.blocks.0.conv_dw    | Conv2d                | 12.8 K | train\n",
      "51  | model.stages.1.blocks.0.norm       | LayerNorm             | 512    | train\n",
      "52  | model.stages.1.blocks.0.mlp        | GlobalResponseNormMlp | 527 K  | train\n",
      "53  | model.stages.1.blocks.0.mlp.fc1    | Linear                | 263 K  | train\n",
      "54  | model.stages.1.blocks.0.mlp.act    | GELU                  | 0      | train\n",
      "55  | model.stages.1.blocks.0.mlp.drop1  | Dropout               | 0      | train\n",
      "56  | model.stages.1.blocks.0.mlp.grn    | GlobalResponseNorm    | 2.0 K  | train\n",
      "57  | model.stages.1.blocks.0.mlp.fc2    | Linear                | 262 K  | train\n",
      "58  | model.stages.1.blocks.0.mlp.drop2  | Dropout               | 0      | train\n",
      "59  | model.stages.1.blocks.0.shortcut   | Identity              | 0      | train\n",
      "60  | model.stages.1.blocks.0.drop_path  | Identity              | 0      | train\n",
      "61  | model.stages.1.blocks.1            | ConvNeXtBlock         | 540 K  | train\n",
      "62  | model.stages.1.blocks.1.conv_dw    | Conv2d                | 12.8 K | train\n",
      "63  | model.stages.1.blocks.1.norm       | LayerNorm             | 512    | train\n",
      "64  | model.stages.1.blocks.1.mlp        | GlobalResponseNormMlp | 527 K  | train\n",
      "65  | model.stages.1.blocks.1.mlp.fc1    | Linear                | 263 K  | train\n",
      "66  | model.stages.1.blocks.1.mlp.act    | GELU                  | 0      | train\n",
      "67  | model.stages.1.blocks.1.mlp.drop1  | Dropout               | 0      | train\n",
      "68  | model.stages.1.blocks.1.mlp.grn    | GlobalResponseNorm    | 2.0 K  | train\n",
      "69  | model.stages.1.blocks.1.mlp.fc2    | Linear                | 262 K  | train\n",
      "70  | model.stages.1.blocks.1.mlp.drop2  | Dropout               | 0      | train\n",
      "71  | model.stages.1.blocks.1.shortcut   | Identity              | 0      | train\n",
      "72  | model.stages.1.blocks.1.drop_path  | Identity              | 0      | train\n",
      "73  | model.stages.1.blocks.2            | ConvNeXtBlock         | 540 K  | train\n",
      "74  | model.stages.1.blocks.2.conv_dw    | Conv2d                | 12.8 K | train\n",
      "75  | model.stages.1.blocks.2.norm       | LayerNorm             | 512    | train\n",
      "76  | model.stages.1.blocks.2.mlp        | GlobalResponseNormMlp | 527 K  | train\n",
      "77  | model.stages.1.blocks.2.mlp.fc1    | Linear                | 263 K  | train\n",
      "78  | model.stages.1.blocks.2.mlp.act    | GELU                  | 0      | train\n",
      "79  | model.stages.1.blocks.2.mlp.drop1  | Dropout               | 0      | train\n",
      "80  | model.stages.1.blocks.2.mlp.grn    | GlobalResponseNorm    | 2.0 K  | train\n",
      "81  | model.stages.1.blocks.2.mlp.fc2    | Linear                | 262 K  | train\n",
      "82  | model.stages.1.blocks.2.mlp.drop2  | Dropout               | 0      | train\n",
      "83  | model.stages.1.blocks.2.shortcut   | Identity              | 0      | train\n",
      "84  | model.stages.1.blocks.2.drop_path  | Identity              | 0      | train\n",
      "85  | model.stages.2                     | ConvNeXtStage         | 58.0 M | train\n",
      "86  | model.stages.2.downsample          | Sequential            | 525 K  | train\n",
      "87  | model.stages.2.downsample.0        | LayerNorm2d           | 512    | train\n",
      "88  | model.stages.2.downsample.1        | Conv2d                | 524 K  | train\n",
      "89  | model.stages.2.blocks              | Sequential            | 57.5 M | train\n",
      "90  | model.stages.2.blocks.0            | ConvNeXtBlock         | 2.1 M  | train\n",
      "91  | model.stages.2.blocks.0.conv_dw    | Conv2d                | 25.6 K | train\n",
      "92  | model.stages.2.blocks.0.norm       | LayerNorm             | 1.0 K  | train\n",
      "93  | model.stages.2.blocks.0.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "94  | model.stages.2.blocks.0.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "95  | model.stages.2.blocks.0.mlp.act    | GELU                  | 0      | train\n",
      "96  | model.stages.2.blocks.0.mlp.drop1  | Dropout               | 0      | train\n",
      "97  | model.stages.2.blocks.0.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "98  | model.stages.2.blocks.0.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "99  | model.stages.2.blocks.0.mlp.drop2  | Dropout               | 0      | train\n",
      "100 | model.stages.2.blocks.0.shortcut   | Identity              | 0      | train\n",
      "101 | model.stages.2.blocks.0.drop_path  | Identity              | 0      | train\n",
      "102 | model.stages.2.blocks.1            | ConvNeXtBlock         | 2.1 M  | train\n",
      "103 | model.stages.2.blocks.1.conv_dw    | Conv2d                | 25.6 K | train\n",
      "104 | model.stages.2.blocks.1.norm       | LayerNorm             | 1.0 K  | train\n",
      "105 | model.stages.2.blocks.1.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "106 | model.stages.2.blocks.1.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "107 | model.stages.2.blocks.1.mlp.act    | GELU                  | 0      | train\n",
      "108 | model.stages.2.blocks.1.mlp.drop1  | Dropout               | 0      | train\n",
      "109 | model.stages.2.blocks.1.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "110 | model.stages.2.blocks.1.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "111 | model.stages.2.blocks.1.mlp.drop2  | Dropout               | 0      | train\n",
      "112 | model.stages.2.blocks.1.shortcut   | Identity              | 0      | train\n",
      "113 | model.stages.2.blocks.1.drop_path  | Identity              | 0      | train\n",
      "114 | model.stages.2.blocks.2            | ConvNeXtBlock         | 2.1 M  | train\n",
      "115 | model.stages.2.blocks.2.conv_dw    | Conv2d                | 25.6 K | train\n",
      "116 | model.stages.2.blocks.2.norm       | LayerNorm             | 1.0 K  | train\n",
      "117 | model.stages.2.blocks.2.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "118 | model.stages.2.blocks.2.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "119 | model.stages.2.blocks.2.mlp.act    | GELU                  | 0      | train\n",
      "120 | model.stages.2.blocks.2.mlp.drop1  | Dropout               | 0      | train\n",
      "121 | model.stages.2.blocks.2.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "122 | model.stages.2.blocks.2.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "123 | model.stages.2.blocks.2.mlp.drop2  | Dropout               | 0      | train\n",
      "124 | model.stages.2.blocks.2.shortcut   | Identity              | 0      | train\n",
      "125 | model.stages.2.blocks.2.drop_path  | Identity              | 0      | train\n",
      "126 | model.stages.2.blocks.3            | ConvNeXtBlock         | 2.1 M  | train\n",
      "127 | model.stages.2.blocks.3.conv_dw    | Conv2d                | 25.6 K | train\n",
      "128 | model.stages.2.blocks.3.norm       | LayerNorm             | 1.0 K  | train\n",
      "129 | model.stages.2.blocks.3.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "130 | model.stages.2.blocks.3.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "131 | model.stages.2.blocks.3.mlp.act    | GELU                  | 0      | train\n",
      "132 | model.stages.2.blocks.3.mlp.drop1  | Dropout               | 0      | train\n",
      "133 | model.stages.2.blocks.3.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "134 | model.stages.2.blocks.3.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "135 | model.stages.2.blocks.3.mlp.drop2  | Dropout               | 0      | train\n",
      "136 | model.stages.2.blocks.3.shortcut   | Identity              | 0      | train\n",
      "137 | model.stages.2.blocks.3.drop_path  | Identity              | 0      | train\n",
      "138 | model.stages.2.blocks.4            | ConvNeXtBlock         | 2.1 M  | train\n",
      "139 | model.stages.2.blocks.4.conv_dw    | Conv2d                | 25.6 K | train\n",
      "140 | model.stages.2.blocks.4.norm       | LayerNorm             | 1.0 K  | train\n",
      "141 | model.stages.2.blocks.4.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "142 | model.stages.2.blocks.4.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "143 | model.stages.2.blocks.4.mlp.act    | GELU                  | 0      | train\n",
      "144 | model.stages.2.blocks.4.mlp.drop1  | Dropout               | 0      | train\n",
      "145 | model.stages.2.blocks.4.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "146 | model.stages.2.blocks.4.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "147 | model.stages.2.blocks.4.mlp.drop2  | Dropout               | 0      | train\n",
      "148 | model.stages.2.blocks.4.shortcut   | Identity              | 0      | train\n",
      "149 | model.stages.2.blocks.4.drop_path  | Identity              | 0      | train\n",
      "150 | model.stages.2.blocks.5            | ConvNeXtBlock         | 2.1 M  | train\n",
      "151 | model.stages.2.blocks.5.conv_dw    | Conv2d                | 25.6 K | train\n",
      "152 | model.stages.2.blocks.5.norm       | LayerNorm             | 1.0 K  | train\n",
      "153 | model.stages.2.blocks.5.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "154 | model.stages.2.blocks.5.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "155 | model.stages.2.blocks.5.mlp.act    | GELU                  | 0      | train\n",
      "156 | model.stages.2.blocks.5.mlp.drop1  | Dropout               | 0      | train\n",
      "157 | model.stages.2.blocks.5.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "158 | model.stages.2.blocks.5.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "159 | model.stages.2.blocks.5.mlp.drop2  | Dropout               | 0      | train\n",
      "160 | model.stages.2.blocks.5.shortcut   | Identity              | 0      | train\n",
      "161 | model.stages.2.blocks.5.drop_path  | Identity              | 0      | train\n",
      "162 | model.stages.2.blocks.6            | ConvNeXtBlock         | 2.1 M  | train\n",
      "163 | model.stages.2.blocks.6.conv_dw    | Conv2d                | 25.6 K | train\n",
      "164 | model.stages.2.blocks.6.norm       | LayerNorm             | 1.0 K  | train\n",
      "165 | model.stages.2.blocks.6.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "166 | model.stages.2.blocks.6.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "167 | model.stages.2.blocks.6.mlp.act    | GELU                  | 0      | train\n",
      "168 | model.stages.2.blocks.6.mlp.drop1  | Dropout               | 0      | train\n",
      "169 | model.stages.2.blocks.6.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "170 | model.stages.2.blocks.6.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "171 | model.stages.2.blocks.6.mlp.drop2  | Dropout               | 0      | train\n",
      "172 | model.stages.2.blocks.6.shortcut   | Identity              | 0      | train\n",
      "173 | model.stages.2.blocks.6.drop_path  | Identity              | 0      | train\n",
      "174 | model.stages.2.blocks.7            | ConvNeXtBlock         | 2.1 M  | train\n",
      "175 | model.stages.2.blocks.7.conv_dw    | Conv2d                | 25.6 K | train\n",
      "176 | model.stages.2.blocks.7.norm       | LayerNorm             | 1.0 K  | train\n",
      "177 | model.stages.2.blocks.7.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "178 | model.stages.2.blocks.7.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "179 | model.stages.2.blocks.7.mlp.act    | GELU                  | 0      | train\n",
      "180 | model.stages.2.blocks.7.mlp.drop1  | Dropout               | 0      | train\n",
      "181 | model.stages.2.blocks.7.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "182 | model.stages.2.blocks.7.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "183 | model.stages.2.blocks.7.mlp.drop2  | Dropout               | 0      | train\n",
      "184 | model.stages.2.blocks.7.shortcut   | Identity              | 0      | train\n",
      "185 | model.stages.2.blocks.7.drop_path  | Identity              | 0      | train\n",
      "186 | model.stages.2.blocks.8            | ConvNeXtBlock         | 2.1 M  | train\n",
      "187 | model.stages.2.blocks.8.conv_dw    | Conv2d                | 25.6 K | train\n",
      "188 | model.stages.2.blocks.8.norm       | LayerNorm             | 1.0 K  | train\n",
      "189 | model.stages.2.blocks.8.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "190 | model.stages.2.blocks.8.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "191 | model.stages.2.blocks.8.mlp.act    | GELU                  | 0      | train\n",
      "192 | model.stages.2.blocks.8.mlp.drop1  | Dropout               | 0      | train\n",
      "193 | model.stages.2.blocks.8.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "194 | model.stages.2.blocks.8.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "195 | model.stages.2.blocks.8.mlp.drop2  | Dropout               | 0      | train\n",
      "196 | model.stages.2.blocks.8.shortcut   | Identity              | 0      | train\n",
      "197 | model.stages.2.blocks.8.drop_path  | Identity              | 0      | train\n",
      "198 | model.stages.2.blocks.9            | ConvNeXtBlock         | 2.1 M  | train\n",
      "199 | model.stages.2.blocks.9.conv_dw    | Conv2d                | 25.6 K | train\n",
      "200 | model.stages.2.blocks.9.norm       | LayerNorm             | 1.0 K  | train\n",
      "201 | model.stages.2.blocks.9.mlp        | GlobalResponseNormMlp | 2.1 M  | train\n",
      "202 | model.stages.2.blocks.9.mlp.fc1    | Linear                | 1.1 M  | train\n",
      "203 | model.stages.2.blocks.9.mlp.act    | GELU                  | 0      | train\n",
      "204 | model.stages.2.blocks.9.mlp.drop1  | Dropout               | 0      | train\n",
      "205 | model.stages.2.blocks.9.mlp.grn    | GlobalResponseNorm    | 4.1 K  | train\n",
      "206 | model.stages.2.blocks.9.mlp.fc2    | Linear                | 1.0 M  | train\n",
      "207 | model.stages.2.blocks.9.mlp.drop2  | Dropout               | 0      | train\n",
      "208 | model.stages.2.blocks.9.shortcut   | Identity              | 0      | train\n",
      "209 | model.stages.2.blocks.9.drop_path  | Identity              | 0      | train\n",
      "210 | model.stages.2.blocks.10           | ConvNeXtBlock         | 2.1 M  | train\n",
      "211 | model.stages.2.blocks.10.conv_dw   | Conv2d                | 25.6 K | train\n",
      "212 | model.stages.2.blocks.10.norm      | LayerNorm             | 1.0 K  | train\n",
      "213 | model.stages.2.blocks.10.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "214 | model.stages.2.blocks.10.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "215 | model.stages.2.blocks.10.mlp.act   | GELU                  | 0      | train\n",
      "216 | model.stages.2.blocks.10.mlp.drop1 | Dropout               | 0      | train\n",
      "217 | model.stages.2.blocks.10.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "218 | model.stages.2.blocks.10.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "219 | model.stages.2.blocks.10.mlp.drop2 | Dropout               | 0      | train\n",
      "220 | model.stages.2.blocks.10.shortcut  | Identity              | 0      | train\n",
      "221 | model.stages.2.blocks.10.drop_path | Identity              | 0      | train\n",
      "222 | model.stages.2.blocks.11           | ConvNeXtBlock         | 2.1 M  | train\n",
      "223 | model.stages.2.blocks.11.conv_dw   | Conv2d                | 25.6 K | train\n",
      "224 | model.stages.2.blocks.11.norm      | LayerNorm             | 1.0 K  | train\n",
      "225 | model.stages.2.blocks.11.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "226 | model.stages.2.blocks.11.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "227 | model.stages.2.blocks.11.mlp.act   | GELU                  | 0      | train\n",
      "228 | model.stages.2.blocks.11.mlp.drop1 | Dropout               | 0      | train\n",
      "229 | model.stages.2.blocks.11.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "230 | model.stages.2.blocks.11.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "231 | model.stages.2.blocks.11.mlp.drop2 | Dropout               | 0      | train\n",
      "232 | model.stages.2.blocks.11.shortcut  | Identity              | 0      | train\n",
      "233 | model.stages.2.blocks.11.drop_path | Identity              | 0      | train\n",
      "234 | model.stages.2.blocks.12           | ConvNeXtBlock         | 2.1 M  | train\n",
      "235 | model.stages.2.blocks.12.conv_dw   | Conv2d                | 25.6 K | train\n",
      "236 | model.stages.2.blocks.12.norm      | LayerNorm             | 1.0 K  | train\n",
      "237 | model.stages.2.blocks.12.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "238 | model.stages.2.blocks.12.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "239 | model.stages.2.blocks.12.mlp.act   | GELU                  | 0      | train\n",
      "240 | model.stages.2.blocks.12.mlp.drop1 | Dropout               | 0      | train\n",
      "241 | model.stages.2.blocks.12.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "242 | model.stages.2.blocks.12.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "243 | model.stages.2.blocks.12.mlp.drop2 | Dropout               | 0      | train\n",
      "244 | model.stages.2.blocks.12.shortcut  | Identity              | 0      | train\n",
      "245 | model.stages.2.blocks.12.drop_path | Identity              | 0      | train\n",
      "246 | model.stages.2.blocks.13           | ConvNeXtBlock         | 2.1 M  | train\n",
      "247 | model.stages.2.blocks.13.conv_dw   | Conv2d                | 25.6 K | train\n",
      "248 | model.stages.2.blocks.13.norm      | LayerNorm             | 1.0 K  | train\n",
      "249 | model.stages.2.blocks.13.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "250 | model.stages.2.blocks.13.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "251 | model.stages.2.blocks.13.mlp.act   | GELU                  | 0      | train\n",
      "252 | model.stages.2.blocks.13.mlp.drop1 | Dropout               | 0      | train\n",
      "253 | model.stages.2.blocks.13.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "254 | model.stages.2.blocks.13.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "255 | model.stages.2.blocks.13.mlp.drop2 | Dropout               | 0      | train\n",
      "256 | model.stages.2.blocks.13.shortcut  | Identity              | 0      | train\n",
      "257 | model.stages.2.blocks.13.drop_path | Identity              | 0      | train\n",
      "258 | model.stages.2.blocks.14           | ConvNeXtBlock         | 2.1 M  | train\n",
      "259 | model.stages.2.blocks.14.conv_dw   | Conv2d                | 25.6 K | train\n",
      "260 | model.stages.2.blocks.14.norm      | LayerNorm             | 1.0 K  | train\n",
      "261 | model.stages.2.blocks.14.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "262 | model.stages.2.blocks.14.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "263 | model.stages.2.blocks.14.mlp.act   | GELU                  | 0      | train\n",
      "264 | model.stages.2.blocks.14.mlp.drop1 | Dropout               | 0      | train\n",
      "265 | model.stages.2.blocks.14.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "266 | model.stages.2.blocks.14.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "267 | model.stages.2.blocks.14.mlp.drop2 | Dropout               | 0      | train\n",
      "268 | model.stages.2.blocks.14.shortcut  | Identity              | 0      | train\n",
      "269 | model.stages.2.blocks.14.drop_path | Identity              | 0      | train\n",
      "270 | model.stages.2.blocks.15           | ConvNeXtBlock         | 2.1 M  | train\n",
      "271 | model.stages.2.blocks.15.conv_dw   | Conv2d                | 25.6 K | train\n",
      "272 | model.stages.2.blocks.15.norm      | LayerNorm             | 1.0 K  | train\n",
      "273 | model.stages.2.blocks.15.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "274 | model.stages.2.blocks.15.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "275 | model.stages.2.blocks.15.mlp.act   | GELU                  | 0      | train\n",
      "276 | model.stages.2.blocks.15.mlp.drop1 | Dropout               | 0      | train\n",
      "277 | model.stages.2.blocks.15.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "278 | model.stages.2.blocks.15.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "279 | model.stages.2.blocks.15.mlp.drop2 | Dropout               | 0      | train\n",
      "280 | model.stages.2.blocks.15.shortcut  | Identity              | 0      | train\n",
      "281 | model.stages.2.blocks.15.drop_path | Identity              | 0      | train\n",
      "282 | model.stages.2.blocks.16           | ConvNeXtBlock         | 2.1 M  | train\n",
      "283 | model.stages.2.blocks.16.conv_dw   | Conv2d                | 25.6 K | train\n",
      "284 | model.stages.2.blocks.16.norm      | LayerNorm             | 1.0 K  | train\n",
      "285 | model.stages.2.blocks.16.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "286 | model.stages.2.blocks.16.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "287 | model.stages.2.blocks.16.mlp.act   | GELU                  | 0      | train\n",
      "288 | model.stages.2.blocks.16.mlp.drop1 | Dropout               | 0      | train\n",
      "289 | model.stages.2.blocks.16.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "290 | model.stages.2.blocks.16.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "291 | model.stages.2.blocks.16.mlp.drop2 | Dropout               | 0      | train\n",
      "292 | model.stages.2.blocks.16.shortcut  | Identity              | 0      | train\n",
      "293 | model.stages.2.blocks.16.drop_path | Identity              | 0      | train\n",
      "294 | model.stages.2.blocks.17           | ConvNeXtBlock         | 2.1 M  | train\n",
      "295 | model.stages.2.blocks.17.conv_dw   | Conv2d                | 25.6 K | train\n",
      "296 | model.stages.2.blocks.17.norm      | LayerNorm             | 1.0 K  | train\n",
      "297 | model.stages.2.blocks.17.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "298 | model.stages.2.blocks.17.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "299 | model.stages.2.blocks.17.mlp.act   | GELU                  | 0      | train\n",
      "300 | model.stages.2.blocks.17.mlp.drop1 | Dropout               | 0      | train\n",
      "301 | model.stages.2.blocks.17.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "302 | model.stages.2.blocks.17.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "303 | model.stages.2.blocks.17.mlp.drop2 | Dropout               | 0      | train\n",
      "304 | model.stages.2.blocks.17.shortcut  | Identity              | 0      | train\n",
      "305 | model.stages.2.blocks.17.drop_path | Identity              | 0      | train\n",
      "306 | model.stages.2.blocks.18           | ConvNeXtBlock         | 2.1 M  | train\n",
      "307 | model.stages.2.blocks.18.conv_dw   | Conv2d                | 25.6 K | train\n",
      "308 | model.stages.2.blocks.18.norm      | LayerNorm             | 1.0 K  | train\n",
      "309 | model.stages.2.blocks.18.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "310 | model.stages.2.blocks.18.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "311 | model.stages.2.blocks.18.mlp.act   | GELU                  | 0      | train\n",
      "312 | model.stages.2.blocks.18.mlp.drop1 | Dropout               | 0      | train\n",
      "313 | model.stages.2.blocks.18.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "314 | model.stages.2.blocks.18.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "315 | model.stages.2.blocks.18.mlp.drop2 | Dropout               | 0      | train\n",
      "316 | model.stages.2.blocks.18.shortcut  | Identity              | 0      | train\n",
      "317 | model.stages.2.blocks.18.drop_path | Identity              | 0      | train\n",
      "318 | model.stages.2.blocks.19           | ConvNeXtBlock         | 2.1 M  | train\n",
      "319 | model.stages.2.blocks.19.conv_dw   | Conv2d                | 25.6 K | train\n",
      "320 | model.stages.2.blocks.19.norm      | LayerNorm             | 1.0 K  | train\n",
      "321 | model.stages.2.blocks.19.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "322 | model.stages.2.blocks.19.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "323 | model.stages.2.blocks.19.mlp.act   | GELU                  | 0      | train\n",
      "324 | model.stages.2.blocks.19.mlp.drop1 | Dropout               | 0      | train\n",
      "325 | model.stages.2.blocks.19.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "326 | model.stages.2.blocks.19.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "327 | model.stages.2.blocks.19.mlp.drop2 | Dropout               | 0      | train\n",
      "328 | model.stages.2.blocks.19.shortcut  | Identity              | 0      | train\n",
      "329 | model.stages.2.blocks.19.drop_path | Identity              | 0      | train\n",
      "330 | model.stages.2.blocks.20           | ConvNeXtBlock         | 2.1 M  | train\n",
      "331 | model.stages.2.blocks.20.conv_dw   | Conv2d                | 25.6 K | train\n",
      "332 | model.stages.2.blocks.20.norm      | LayerNorm             | 1.0 K  | train\n",
      "333 | model.stages.2.blocks.20.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "334 | model.stages.2.blocks.20.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "335 | model.stages.2.blocks.20.mlp.act   | GELU                  | 0      | train\n",
      "336 | model.stages.2.blocks.20.mlp.drop1 | Dropout               | 0      | train\n",
      "337 | model.stages.2.blocks.20.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "338 | model.stages.2.blocks.20.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "339 | model.stages.2.blocks.20.mlp.drop2 | Dropout               | 0      | train\n",
      "340 | model.stages.2.blocks.20.shortcut  | Identity              | 0      | train\n",
      "341 | model.stages.2.blocks.20.drop_path | Identity              | 0      | train\n",
      "342 | model.stages.2.blocks.21           | ConvNeXtBlock         | 2.1 M  | train\n",
      "343 | model.stages.2.blocks.21.conv_dw   | Conv2d                | 25.6 K | train\n",
      "344 | model.stages.2.blocks.21.norm      | LayerNorm             | 1.0 K  | train\n",
      "345 | model.stages.2.blocks.21.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "346 | model.stages.2.blocks.21.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "347 | model.stages.2.blocks.21.mlp.act   | GELU                  | 0      | train\n",
      "348 | model.stages.2.blocks.21.mlp.drop1 | Dropout               | 0      | train\n",
      "349 | model.stages.2.blocks.21.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "350 | model.stages.2.blocks.21.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "351 | model.stages.2.blocks.21.mlp.drop2 | Dropout               | 0      | train\n",
      "352 | model.stages.2.blocks.21.shortcut  | Identity              | 0      | train\n",
      "353 | model.stages.2.blocks.21.drop_path | Identity              | 0      | train\n",
      "354 | model.stages.2.blocks.22           | ConvNeXtBlock         | 2.1 M  | train\n",
      "355 | model.stages.2.blocks.22.conv_dw   | Conv2d                | 25.6 K | train\n",
      "356 | model.stages.2.blocks.22.norm      | LayerNorm             | 1.0 K  | train\n",
      "357 | model.stages.2.blocks.22.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "358 | model.stages.2.blocks.22.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "359 | model.stages.2.blocks.22.mlp.act   | GELU                  | 0      | train\n",
      "360 | model.stages.2.blocks.22.mlp.drop1 | Dropout               | 0      | train\n",
      "361 | model.stages.2.blocks.22.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "362 | model.stages.2.blocks.22.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "363 | model.stages.2.blocks.22.mlp.drop2 | Dropout               | 0      | train\n",
      "364 | model.stages.2.blocks.22.shortcut  | Identity              | 0      | train\n",
      "365 | model.stages.2.blocks.22.drop_path | Identity              | 0      | train\n",
      "366 | model.stages.2.blocks.23           | ConvNeXtBlock         | 2.1 M  | train\n",
      "367 | model.stages.2.blocks.23.conv_dw   | Conv2d                | 25.6 K | train\n",
      "368 | model.stages.2.blocks.23.norm      | LayerNorm             | 1.0 K  | train\n",
      "369 | model.stages.2.blocks.23.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "370 | model.stages.2.blocks.23.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "371 | model.stages.2.blocks.23.mlp.act   | GELU                  | 0      | train\n",
      "372 | model.stages.2.blocks.23.mlp.drop1 | Dropout               | 0      | train\n",
      "373 | model.stages.2.blocks.23.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "374 | model.stages.2.blocks.23.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "375 | model.stages.2.blocks.23.mlp.drop2 | Dropout               | 0      | train\n",
      "376 | model.stages.2.blocks.23.shortcut  | Identity              | 0      | train\n",
      "377 | model.stages.2.blocks.23.drop_path | Identity              | 0      | train\n",
      "378 | model.stages.2.blocks.24           | ConvNeXtBlock         | 2.1 M  | train\n",
      "379 | model.stages.2.blocks.24.conv_dw   | Conv2d                | 25.6 K | train\n",
      "380 | model.stages.2.blocks.24.norm      | LayerNorm             | 1.0 K  | train\n",
      "381 | model.stages.2.blocks.24.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "382 | model.stages.2.blocks.24.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "383 | model.stages.2.blocks.24.mlp.act   | GELU                  | 0      | train\n",
      "384 | model.stages.2.blocks.24.mlp.drop1 | Dropout               | 0      | train\n",
      "385 | model.stages.2.blocks.24.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "386 | model.stages.2.blocks.24.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "387 | model.stages.2.blocks.24.mlp.drop2 | Dropout               | 0      | train\n",
      "388 | model.stages.2.blocks.24.shortcut  | Identity              | 0      | train\n",
      "389 | model.stages.2.blocks.24.drop_path | Identity              | 0      | train\n",
      "390 | model.stages.2.blocks.25           | ConvNeXtBlock         | 2.1 M  | train\n",
      "391 | model.stages.2.blocks.25.conv_dw   | Conv2d                | 25.6 K | train\n",
      "392 | model.stages.2.blocks.25.norm      | LayerNorm             | 1.0 K  | train\n",
      "393 | model.stages.2.blocks.25.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "394 | model.stages.2.blocks.25.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "395 | model.stages.2.blocks.25.mlp.act   | GELU                  | 0      | train\n",
      "396 | model.stages.2.blocks.25.mlp.drop1 | Dropout               | 0      | train\n",
      "397 | model.stages.2.blocks.25.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "398 | model.stages.2.blocks.25.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "399 | model.stages.2.blocks.25.mlp.drop2 | Dropout               | 0      | train\n",
      "400 | model.stages.2.blocks.25.shortcut  | Identity              | 0      | train\n",
      "401 | model.stages.2.blocks.25.drop_path | Identity              | 0      | train\n",
      "402 | model.stages.2.blocks.26           | ConvNeXtBlock         | 2.1 M  | train\n",
      "403 | model.stages.2.blocks.26.conv_dw   | Conv2d                | 25.6 K | train\n",
      "404 | model.stages.2.blocks.26.norm      | LayerNorm             | 1.0 K  | train\n",
      "405 | model.stages.2.blocks.26.mlp       | GlobalResponseNormMlp | 2.1 M  | train\n",
      "406 | model.stages.2.blocks.26.mlp.fc1   | Linear                | 1.1 M  | train\n",
      "407 | model.stages.2.blocks.26.mlp.act   | GELU                  | 0      | train\n",
      "408 | model.stages.2.blocks.26.mlp.drop1 | Dropout               | 0      | train\n",
      "409 | model.stages.2.blocks.26.mlp.grn   | GlobalResponseNorm    | 4.1 K  | train\n",
      "410 | model.stages.2.blocks.26.mlp.fc2   | Linear                | 1.0 M  | train\n",
      "411 | model.stages.2.blocks.26.mlp.drop2 | Dropout               | 0      | train\n",
      "412 | model.stages.2.blocks.26.shortcut  | Identity              | 0      | train\n",
      "413 | model.stages.2.blocks.26.drop_path | Identity              | 0      | train\n",
      "414 | model.stages.3                     | ConvNeXtStage         | 27.5 M | train\n",
      "415 | model.stages.3.downsample          | Sequential            | 2.1 M  | train\n",
      "416 | model.stages.3.downsample.0        | LayerNorm2d           | 1.0 K  | train\n",
      "417 | model.stages.3.downsample.1        | Conv2d                | 2.1 M  | train\n",
      "418 | model.stages.3.blocks              | Sequential            | 25.4 M | train\n",
      "419 | model.stages.3.blocks.0            | ConvNeXtBlock         | 8.5 M  | train\n",
      "420 | model.stages.3.blocks.0.conv_dw    | Conv2d                | 51.2 K | train\n",
      "421 | model.stages.3.blocks.0.norm       | LayerNorm             | 2.0 K  | train\n",
      "422 | model.stages.3.blocks.0.mlp        | GlobalResponseNormMlp | 8.4 M  | train\n",
      "423 | model.stages.3.blocks.0.mlp.fc1    | Linear                | 4.2 M  | train\n",
      "424 | model.stages.3.blocks.0.mlp.act    | GELU                  | 0      | train\n",
      "425 | model.stages.3.blocks.0.mlp.drop1  | Dropout               | 0      | train\n",
      "426 | model.stages.3.blocks.0.mlp.grn    | GlobalResponseNorm    | 8.2 K  | train\n",
      "427 | model.stages.3.blocks.0.mlp.fc2    | Linear                | 4.2 M  | train\n",
      "428 | model.stages.3.blocks.0.mlp.drop2  | Dropout               | 0      | train\n",
      "429 | model.stages.3.blocks.0.shortcut   | Identity              | 0      | train\n",
      "430 | model.stages.3.blocks.0.drop_path  | Identity              | 0      | train\n",
      "431 | model.stages.3.blocks.1            | ConvNeXtBlock         | 8.5 M  | train\n",
      "432 | model.stages.3.blocks.1.conv_dw    | Conv2d                | 51.2 K | train\n",
      "433 | model.stages.3.blocks.1.norm       | LayerNorm             | 2.0 K  | train\n",
      "434 | model.stages.3.blocks.1.mlp        | GlobalResponseNormMlp | 8.4 M  | train\n",
      "435 | model.stages.3.blocks.1.mlp.fc1    | Linear                | 4.2 M  | train\n",
      "436 | model.stages.3.blocks.1.mlp.act    | GELU                  | 0      | train\n",
      "437 | model.stages.3.blocks.1.mlp.drop1  | Dropout               | 0      | train\n",
      "438 | model.stages.3.blocks.1.mlp.grn    | GlobalResponseNorm    | 8.2 K  | train\n",
      "439 | model.stages.3.blocks.1.mlp.fc2    | Linear                | 4.2 M  | train\n",
      "440 | model.stages.3.blocks.1.mlp.drop2  | Dropout               | 0      | train\n",
      "441 | model.stages.3.blocks.1.shortcut   | Identity              | 0      | train\n",
      "442 | model.stages.3.blocks.1.drop_path  | Identity              | 0      | train\n",
      "443 | model.stages.3.blocks.2            | ConvNeXtBlock         | 8.5 M  | train\n",
      "444 | model.stages.3.blocks.2.conv_dw    | Conv2d                | 51.2 K | train\n",
      "445 | model.stages.3.blocks.2.norm       | LayerNorm             | 2.0 K  | train\n",
      "446 | model.stages.3.blocks.2.mlp        | GlobalResponseNormMlp | 8.4 M  | train\n",
      "447 | model.stages.3.blocks.2.mlp.fc1    | Linear                | 4.2 M  | train\n",
      "448 | model.stages.3.blocks.2.mlp.act    | GELU                  | 0      | train\n",
      "449 | model.stages.3.blocks.2.mlp.drop1  | Dropout               | 0      | train\n",
      "450 | model.stages.3.blocks.2.mlp.grn    | GlobalResponseNorm    | 8.2 K  | train\n",
      "451 | model.stages.3.blocks.2.mlp.fc2    | Linear                | 4.2 M  | train\n",
      "452 | model.stages.3.blocks.2.mlp.drop2  | Dropout               | 0      | train\n",
      "453 | model.stages.3.blocks.2.shortcut   | Identity              | 0      | train\n",
      "454 | model.stages.3.blocks.2.drop_path  | Identity              | 0      | train\n",
      "455 | model.norm_pre                     | Identity              | 0      | train\n",
      "456 | model.head                         | NormMlpClassifierHead | 8.2 K  | train\n",
      "457 | model.head.global_pool             | SelectAdaptivePool2d  | 0      | train\n",
      "458 | model.head.global_pool.pool        | AdaptiveAvgPool2d     | 0      | train\n",
      "459 | model.head.global_pool.flatten     | Identity              | 0      | train\n",
      "460 | model.head.norm                    | LayerNorm2d           | 2.0 K  | train\n",
      "461 | model.head.flatten                 | Flatten               | 0      | train\n",
      "462 | model.head.pre_logits              | Identity              | 0      | train\n",
      "463 | model.head.drop                    | Dropout               | 0      | train\n",
      "464 | model.head.fc                      | Linear                | 6.2 K  | train\n",
      "465 | criterion                          | CrossEntropyLoss      | 0      | train\n",
      "466 | softmax                            | Softmax               | 0      | train\n",
      "467 | train_accuracy                     | MulticlassAccuracy    | 0      | train\n",
      "468 | val_accuracy                       | MulticlassAccuracy    | 0      | train\n",
      "469 | val_precision                      | MulticlassPrecision   | 0      | train\n",
      "470 | val_recall                         | MulticlassRecall      | 0      | train\n",
      "---------------------------------------------------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "87.7 M    Total params\n",
      "350.796   Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    }
   ],
   "source": [
    "from models.model_facedetection import TL_EfficientNetB4, TL_ConvNextV2\n",
    "# model = TL_EfficientNetB4(num_classes=len(config['name_list']))\n",
    "model = TL_ConvNextV2(num_classes=len(config['name_list']))\n",
    "print(ModelSummary(model, max_depth=-1))\n",
    "\n",
    "# Initialize the Wandb logger\n",
    "# add time to the name of the experiment\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Initialize wandb logger\n",
    "wandb_logger = WandbLogger(\n",
    "    project=config['wandb_project_name'],\n",
    "    name=f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}\",\n",
    "    config={\n",
    "        'model': type(model).__name__,\n",
    "        'dataset': 'DwarfRabbits-binary',\n",
    "        'batch_size': config['batch_size'],\n",
    "        'max_epochs': config['max_epochs'],\n",
    "        'learning_rate': config['learning_rate']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize Trainer with wandb logger, using early stopping callback (https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)\n",
    "trainer = Trainer(\n",
    "    max_epochs=config['max_epochs'], \n",
    "    default_root_dir='model/checkpoint/', #data_directory, \n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\", \n",
    "    strategy=\"auto\",\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')], \n",
    "    logger=wandb_logger\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b53050",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04204521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merzlektor\u001b[0m (\u001b[33mVDKI-Hasen\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250613_115127-51dod7sa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/VDKI-Hasen/VDKI-Hasen/runs/51dod7sa' target=\"_blank\">Face_Recognition_EfficientNetB4_TL_ConvNextV2_2025-06-13_11-51-26</a></strong> to <a href='https://wandb.ai/VDKI-Hasen/VDKI-Hasen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/VDKI-Hasen/VDKI-Hasen' target=\"_blank\">https://wandb.ai/VDKI-Hasen/VDKI-Hasen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/VDKI-Hasen/VDKI-Hasen/runs/51dod7sa' target=\"_blank\">https://wandb.ai/VDKI-Hasen/VDKI-Hasen/runs/51dod7sa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 87.7 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "87.7 M    Total params\n",
      "350.796   Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:03<00:00,  0.61it/s]Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "f:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "f:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 65/65 [01:20<00:00,  0.80it/s, v_num=d7sa]Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "Epoch 1: 100%|| 65/65 [01:21<00:00,  0.80it/s, v_num=d7sa, val_loss=0.699]Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "Epoch 2: 100%|| 65/65 [01:21<00:00,  0.80it/s, v_num=d7sa, val_loss=0.805]Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "Epoch 3: 100%|| 65/65 [01:21<00:00,  0.80it/s, v_num=d7sa, val_loss=0.800]Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "Epoch 4: 100%|| 65/65 [01:21<00:00,  0.80it/s, v_num=d7sa, val_loss=0.875]Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "Epoch 5: 100%|| 65/65 [01:21<00:00,  0.80it/s, v_num=d7sa, val_loss=0.959]Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "Epoch 5: 100%|| 65/65 [01:35<00:00,  0.68it/s, v_num=d7sa, val_loss=0.920]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_acc</td><td>0.9687</td></tr><tr><td>train_loss</td><td>0.04961</td></tr><tr><td>trainer/global_step</td><td>389</td></tr><tr><td>val_acc</td><td>0.4493</td></tr><tr><td>val_loss</td><td>0.91978</td></tr><tr><td>val_precision</td><td>0.64815</td></tr><tr><td>val_recall</td><td>0.4493</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Face_Recognition_EfficientNetB4_TL_ConvNextV2_2025-06-13_11-51-26</strong> at: <a href='https://wandb.ai/VDKI-Hasen/VDKI-Hasen/runs/51dod7sa' target=\"_blank\">https://wandb.ai/VDKI-Hasen/VDKI-Hasen/runs/51dod7sa</a><br> View project at: <a href='https://wandb.ai/VDKI-Hasen/VDKI-Hasen' target=\"_blank\">https://wandb.ai/VDKI-Hasen/VDKI-Hasen</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250613_115127-51dod7sa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m wandb.finish()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Save the trained model checkpoint\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m timestamp = \u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnow\u001b[49m().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m save_path = os.path.join(config[\u001b[33m'\u001b[39m\u001b[33mpath_to_models\u001b[39m\u001b[33m'\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.ckpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m trainer.save_checkpoint(save_path)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'datetime' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "# Training of the model\n",
    "trainer.fit(model=model, datamodule=dm)\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()\n",
    "\n",
    "# Save the trained model checkpoint\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = os.path.join(config['path_to_models'], f\"{model.model_name}_model_{timestamp}.ckpt\")\n",
    "trainer.save_checkpoint(save_path)\n",
    "print(f\"Model saved to: {save_path}\")\n",
    "config['path_to_model'] = save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43266ec",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfebbd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-29 18:38:47,903] A new study created in memory with name: no-name-b254f67a-c015-40be-9181-75c57b24b4b4\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukas-pelz\u001b[0m (\u001b[33mHKA-EKG-Signalverarbeitung\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_183849-cntnxa38</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/cntnxa38' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr2e-04_wd9e-04_sch_None_2025-06-29_18-38</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/cntnxa38' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/cntnxa38</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.99015</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>train_acc</td><td>0.99002</td></tr><tr><td>train_loss</td><td>0.08454</td></tr><tr><td>trainer/global_step</td><td>519</td></tr><tr><td>val_acc</td><td>0.91111</td></tr><tr><td>val_loss</td><td>0.27259</td></tr><tr><td>val_precision</td><td>0.9109</td></tr><tr><td>val_recall</td><td>0.91111</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr2e-04_wd9e-04_sch_None_2025-06-29_18-38</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/cntnxa38' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/cntnxa38</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 60 media file(s), 80 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_183849-cntnxa38\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.2725939154624939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_184420-pxfb2snt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/pxfb2snt' target=\"_blank\">TL_ConvNextV2_base_cls2_bs128_img128_optSGD_lr4e-03_wd2e-05_sch_None_2025-06-29_18-44</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/pxfb2snt' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/pxfb2snt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98397</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>0.95358</td></tr><tr><td>train_loss</td><td>0.1205</td></tr><tr><td>trainer/global_step</td><td>272</td></tr><tr><td>val_acc</td><td>0.88353</td></tr><tr><td>val_loss</td><td>0.36478</td></tr><tr><td>val_precision</td><td>0.888</td></tr><tr><td>val_recall</td><td>0.88353</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs128_img128_optSGD_lr4e-03_wd2e-05_sch_None_2025-06-29_18-44</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/pxfb2snt' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/pxfb2snt</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 63 media file(s), 84 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_184420-pxfb2snt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.36477741599082947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_184740-o6vzvzky</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/o6vzvzky' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img300_optSGD_lr2e-03_wd9e-06_sch_StepLR_2025-06-29_18-47</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/o6vzvzky' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/o6vzvzky</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=37` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98141</td></tr><tr><td>epoch</td><td>36</td></tr><tr><td>train_acc</td><td>0.89545</td></tr><tr><td>train_loss</td><td>0.25472</td></tr><tr><td>trainer/global_step</td><td>258</td></tr><tr><td>val_acc</td><td>0.82487</td></tr><tr><td>val_loss</td><td>0.4345</td></tr><tr><td>val_precision</td><td>0.83177</td></tr><tr><td>val_recall</td><td>0.82487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img300_optSGD_lr2e-03_wd9e-06_sch_StepLR_2025-06-29_18-47</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/o6vzvzky' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/o6vzvzky</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 111 media file(s), 146 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_184740-o6vzvzky\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.4344993531703949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_185939-z8ph1l3b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/z8ph1l3b' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img224_optAdam_lr8e-04_wd2e-04_sch_None_2025-06-29_18-59</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/z8ph1l3b' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/z8ph1l3b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98655</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>train_acc</td><td>0.96911</td></tr><tr><td>train_loss</td><td>0.14079</td></tr><tr><td>trainer/global_step</td><td>90</td></tr><tr><td>val_acc</td><td>0.89636</td></tr><tr><td>val_loss</td><td>0.3195</td></tr><tr><td>val_precision</td><td>0.89626</td></tr><tr><td>val_recall</td><td>0.89636</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img224_optAdam_lr8e-04_wd2e-04_sch_None_2025-06-29_18-59</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/z8ph1l3b' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/z8ph1l3b</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 21 media file(s), 28 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_185939-z8ph1l3b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.31950441002845764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_190120-ahc3z4rz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/ahc3z4rz' target=\"_blank\">TL_ConvNextV2_base_cls1_bs32_img300_optSGD_lr2e-03_wd5e-06_sch_StepLR_2025-06-29_19-01</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/ahc3z4rz' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/ahc3z4rz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 87.7 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "87.7 M    Total params\n",
      "350.796   Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=39` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98582</td></tr><tr><td>epoch</td><td>38</td></tr><tr><td>train_acc</td><td>0.95928</td></tr><tr><td>train_loss</td><td>0.13446</td></tr><tr><td>trainer/global_step</td><td>506</td></tr><tr><td>val_acc</td><td>0.85132</td></tr><tr><td>val_loss</td><td>0.33601</td></tr><tr><td>val_precision</td><td>0.85526</td></tr><tr><td>val_recall</td><td>0.85132</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls1_bs32_img300_optSGD_lr2e-03_wd5e-06_sch_StepLR_2025-06-29_19-01</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/ahc3z4rz' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/ahc3z4rz</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 117 media file(s), 154 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_190120-ahc3z4rz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.33601468801498413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_191248-wb3bx8l0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/wb3bx8l0' target=\"_blank\">TL_ConvNextV2_base_cls1_bs32_img224_optAdam_lr6e-04_wd3e-06_sch_CosineAnnealingLR_2025-06-29_19-12</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/wb3bx8l0' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/wb3bx8l0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 87.7 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "87.7 M    Total params\n",
      "350.796   Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=24` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98677</td></tr><tr><td>epoch</td><td>23</td></tr><tr><td>train_acc</td><td>0.97762</td></tr><tr><td>train_loss</td><td>0.06986</td></tr><tr><td>trainer/global_step</td><td>623</td></tr><tr><td>val_acc</td><td>0.89332</td></tr><tr><td>val_loss</td><td>0.31552</td></tr><tr><td>val_precision</td><td>0.89442</td></tr><tr><td>val_recall</td><td>0.89332</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls1_bs32_img224_optAdam_lr6e-04_wd3e-06_sch_CosineAnnealingLR_2025-06-29_19-12</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/wb3bx8l0' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/wb3bx8l0</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 72 media file(s), 90 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_191248-wb3bx8l0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.315517783164978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_191648-c8w1vbtl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/c8w1vbtl' target=\"_blank\">TL_ConvNextV2_base_cls1_bs128_img128_optAdamW_lr4e-04_wd4e-03_sch_None_2025-06-29_19-16</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/c8w1vbtl' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/c8w1vbtl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 87.7 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "87.7 M    Total params\n",
      "350.796   Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=38` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98169</td></tr><tr><td>epoch</td><td>37</td></tr><tr><td>train_acc</td><td>0.9729</td></tr><tr><td>train_loss</td><td>0.13321</td></tr><tr><td>trainer/global_step</td><td>265</td></tr><tr><td>val_acc</td><td>0.87216</td></tr><tr><td>val_loss</td><td>0.36784</td></tr><tr><td>val_precision</td><td>0.874</td></tr><tr><td>val_recall</td><td>0.87216</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls1_bs128_img128_optAdamW_lr4e-04_wd4e-03_sch_None_2025-06-29_19-16</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/c8w1vbtl' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/c8w1vbtl</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 114 media file(s), 152 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_191648-c8w1vbtl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.3678447902202606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_192237-neltw849</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/neltw849' target=\"_blank\">TL_ConvNextV2_base_cls1_bs128_img300_optAdam_lr7e-03_wd1e-04_sch_StepLR_2025-06-29_19-22</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/neltw849' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/neltw849</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 87.7 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "87.7 M    Total params\n",
      "350.796   Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98805</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>0.98062</td></tr><tr><td>train_loss</td><td>0.03991</td></tr><tr><td>trainer/global_step</td><td>63</td></tr><tr><td>val_acc</td><td>0.90126</td></tr><tr><td>val_loss</td><td>0.2978</td></tr><tr><td>val_precision</td><td>0.90278</td></tr><tr><td>val_recall</td><td>0.90126</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls1_bs128_img300_optAdam_lr7e-03_wd1e-04_sch_StepLR_2025-06-29_19-22</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/neltw849' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/neltw849</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 48 media file(s), 64 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_192237-neltw849\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.2977973520755768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_192826-9zxqzdf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/9zxqzdf4' target=\"_blank\">TL_ConvNextV2_base_cls1_bs64_img128_optAdamW_lr6e-04_wd2e-05_sch_None_2025-06-29_19-28</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/9zxqzdf4' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/9zxqzdf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 87.7 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "87.7 M    Total params\n",
      "350.796   Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=35` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98331</td></tr><tr><td>epoch</td><td>34</td></tr><tr><td>train_acc</td><td>0.98188</td></tr><tr><td>train_loss</td><td>0.07583</td></tr><tr><td>trainer/global_step</td><td>244</td></tr><tr><td>val_acc</td><td>0.88049</td></tr><tr><td>val_loss</td><td>0.35526</td></tr><tr><td>val_precision</td><td>0.88293</td></tr><tr><td>val_recall</td><td>0.88049</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls1_bs64_img128_optAdamW_lr6e-04_wd2e-05_sch_None_2025-06-29_19-28</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/9zxqzdf4' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/9zxqzdf4</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 105 media file(s), 140 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_192826-9zxqzdf4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.35525766015052795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_193312-fgt0ooum</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/fgt0ooum' target=\"_blank\">TL_ConvNextV2_base_cls1_bs64_img128_optSGD_lr4e-03_wd5e-03_sch_None_2025-06-29_19-33</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/fgt0ooum' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/fgt0ooum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 87.7 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "6.2 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "87.7 M    Total params\n",
      "350.796   Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=39` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98202</td></tr><tr><td>epoch</td><td>38</td></tr><tr><td>train_acc</td><td>0.98231</td></tr><tr><td>train_loss</td><td>0.09195</td></tr><tr><td>trainer/global_step</td><td>272</td></tr><tr><td>val_acc</td><td>0.87632</td></tr><tr><td>val_loss</td><td>0.36417</td></tr><tr><td>val_precision</td><td>0.87824</td></tr><tr><td>val_recall</td><td>0.87632</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls1_bs64_img128_optSGD_lr4e-03_wd5e-03_sch_None_2025-06-29_19-33</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/fgt0ooum' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/fgt0ooum</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 117 media file(s), 156 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_193312-fgt0ooum\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.3641675114631653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_193838-005l9xhj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/005l9xhj' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr2e-04_wd7e-04_sch_CosineAnnealingLR_2025-06-29_19-38</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/005l9xhj' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/005l9xhj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98832</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>train_acc</td><td>0.96325</td></tr><tr><td>train_loss</td><td>0.0869</td></tr><tr><td>trainer/global_step</td><td>519</td></tr><tr><td>val_acc</td><td>0.88915</td></tr><tr><td>val_loss</td><td>0.28526</td></tr><tr><td>val_precision</td><td>0.89238</td></tr><tr><td>val_recall</td><td>0.88915</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr2e-04_wd7e-04_sch_CosineAnnealingLR_2025-06-29_19-38</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/005l9xhj' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/005l9xhj</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 60 media file(s), 80 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_193838-005l9xhj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.28526145219802856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_194412-8lwsl458</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8lwsl458' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr1e-04_wd6e-04_sch_CosineAnnealingLR_2025-06-29_19-44</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8lwsl458' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8lwsl458</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98741</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>train_acc</td><td>0.95246</td></tr><tr><td>train_loss</td><td>0.21596</td></tr><tr><td>trainer/global_step</td><td>519</td></tr><tr><td>val_acc</td><td>0.87857</td></tr><tr><td>val_loss</td><td>0.31647</td></tr><tr><td>val_precision</td><td>0.88167</td></tr><tr><td>val_recall</td><td>0.87857</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr1e-04_wd6e-04_sch_CosineAnnealingLR_2025-06-29_19-44</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8lwsl458' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8lwsl458</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 60 media file(s), 80 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_194412-8lwsl458\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.3164723515510559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_194947-8hn6l5ez</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8hn6l5ez' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr1e-04_wd9e-04_sch_CosineAnnealingLR_2025-06-29_19-49</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8hn6l5ez' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8hn6l5ez</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98852</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>train_acc</td><td>0.96554</td></tr><tr><td>train_loss</td><td>0.28849</td></tr><tr><td>trainer/global_step</td><td>519</td></tr><tr><td>val_acc</td><td>0.88915</td></tr><tr><td>val_loss</td><td>0.28412</td></tr><tr><td>val_precision</td><td>0.89274</td></tr><tr><td>val_recall</td><td>0.88915</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr1e-04_wd9e-04_sch_CosineAnnealingLR_2025-06-29_19-49</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8hn6l5ez' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/8hn6l5ez</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 60 media file(s), 76 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_194947-8hn6l5ez\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.28412315249443054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_195521-u4lr8gdb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/u4lr8gdb' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr3e-04_wd9e-04_sch_CosineAnnealingLR_2025-06-29_19-55</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/u4lr8gdb' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/u4lr8gdb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.99028</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>0.97849</td></tr><tr><td>train_loss</td><td>0.07498</td></tr><tr><td>trainer/global_step</td><td>415</td></tr><tr><td>val_acc</td><td>0.90807</td></tr><tr><td>val_loss</td><td>0.2578</td></tr><tr><td>val_precision</td><td>0.91034</td></tr><tr><td>val_recall</td><td>0.90807</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr3e-04_wd9e-04_sch_CosineAnnealingLR_2025-06-29_19-55</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/u4lr8gdb' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/u4lr8gdb</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 48 media file(s), 64 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_195521-u4lr8gdb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.2578020989894867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_195944-q7lp1eo3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/q7lp1eo3' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr3e-04_wd2e-03_sch_CosineAnnealingLR_2025-06-29_19-59</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/q7lp1eo3' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/q7lp1eo3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98982</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>train_acc</td><td>0.98248</td></tr><tr><td>train_loss</td><td>0.07552</td></tr><tr><td>trainer/global_step</td><td>337</td></tr><tr><td>val_acc</td><td>0.90655</td></tr><tr><td>val_loss</td><td>0.26011</td></tr><tr><td>val_precision</td><td>0.91017</td></tr><tr><td>val_recall</td><td>0.90655</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr3e-04_wd2e-03_sch_CosineAnnealingLR_2025-06-29_19-59</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/q7lp1eo3' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/q7lp1eo3</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 39 media file(s), 52 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_195944-q7lp1eo3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.2601087987422943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_200320-in431dpr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/in431dpr' target=\"_blank\">TL_ConvNextV2_base_cls2_bs32_img300_optAdamW_lr3e-04_wd1e-02_sch_CosineAnnealingLR_2025-06-29_20-03</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/in431dpr' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/in431dpr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98964</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>0.98846</td></tr><tr><td>train_loss</td><td>0.14469</td></tr><tr><td>trainer/global_step</td><td>560</td></tr><tr><td>val_acc</td><td>0.9164</td></tr><tr><td>val_loss</td><td>0.27214</td></tr><tr><td>val_precision</td><td>0.9168</td></tr><tr><td>val_recall</td><td>0.9164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs32_img300_optAdamW_lr3e-04_wd1e-02_sch_CosineAnnealingLR_2025-06-29_20-03</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/in431dpr' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/in431dpr</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 33 media file(s), 44 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_200320-in431dpr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.2721419036388397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_200607-0r0zj2tb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/0r0zj2tb' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img224_optAdamW_lr1e-03_wd2e-03_sch_CosineAnnealingLR_2025-06-29_20-06</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/0r0zj2tb' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/0r0zj2tb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98756</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>train_acc</td><td>0.98561</td></tr><tr><td>train_loss</td><td>0.04071</td></tr><tr><td>trainer/global_step</td><td>181</td></tr><tr><td>val_acc</td><td>0.91376</td></tr><tr><td>val_loss</td><td>0.33101</td></tr><tr><td>val_precision</td><td>0.91415</td></tr><tr><td>val_recall</td><td>0.91376</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img224_optAdamW_lr1e-03_wd2e-03_sch_CosineAnnealingLR_2025-06-29_20-06</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/0r0zj2tb' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/0r0zj2tb</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 21 media file(s), 28 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_200607-0r0zj2tb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.3310141861438751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_200734-w9xxs0sd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/w9xxs0sd' target=\"_blank\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr2e-04_wd2e-04_sch_CosineAnnealingLR_2025-06-29_20-07</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/w9xxs0sd' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/w9xxs0sd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98978</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>train_acc</td><td>0.97936</td></tr><tr><td>train_loss</td><td>0.05746</td></tr><tr><td>trainer/global_step</td><td>493</td></tr><tr><td>val_acc</td><td>0.90582</td></tr><tr><td>val_loss</td><td>0.25941</td></tr><tr><td>val_precision</td><td>0.90619</td></tr><tr><td>val_recall</td><td>0.90582</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs64_img300_optAdamW_lr2e-04_wd2e-04_sch_CosineAnnealingLR_2025-06-29_20-07</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/w9xxs0sd' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/w9xxs0sd</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 57 media file(s), 76 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_200734-w9xxs0sd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.2594107687473297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_201250-bapbmynj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/bapbmynj' target=\"_blank\">TL_ConvNextV2_base_cls2_bs128_img300_optAdamW_lr4e-04_wd2e-04_sch_CosineAnnealingLR_2025-06-29_20-12</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/bapbmynj' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/bapbmynj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.9903</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>train_acc</td><td>0.97663</td></tr><tr><td>train_loss</td><td>0.10172</td></tr><tr><td>trainer/global_step</td><td>194</td></tr><tr><td>val_acc</td><td>0.90503</td></tr><tr><td>val_loss</td><td>0.26334</td></tr><tr><td>val_precision</td><td>0.91003</td></tr><tr><td>val_recall</td><td>0.90503</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs128_img300_optAdamW_lr4e-04_wd2e-04_sch_CosineAnnealingLR_2025-06-29_20-12</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/bapbmynj' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/bapbmynj</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 45 media file(s), 60 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_201250-bapbmynj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.2633392810821533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250629_201731-7dax1zo0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/7dax1zo0' target=\"_blank\">TL_ConvNextV2_base_cls2_bs32_img224_optAdam_lr2e-04_wd5e-05_sch_CosineAnnealingLR_2025-06-29_20-17</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/7dax1zo0' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/7dax1zo0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model          | ConvNeXt            | 88.0 M | train\n",
      "1 | criterion      | CrossEntropyLoss    | 0      | train\n",
      "2 | softmax        | Softmax             | 0      | train\n",
      "3 | train_accuracy | MulticlassAccuracy  | 0      | train\n",
      "4 | val_accuracy   | MulticlassAccuracy  | 0      | train\n",
      "5 | val_precision  | MulticlassPrecision | 0      | train\n",
      "6 | val_recall     | MulticlassRecall    | 0      | train\n",
      "---------------------------------------------------------------\n",
      "263 K     Trainable params\n",
      "87.7 M    Non-trainable params\n",
      "88.0 M    Total params\n",
      "351.827   Total estimated model params size (MB)\n",
      "476       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation metrics could not be calculated/logged to wandb: Number of classes in y_true not equal to the number of columns in 'y_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\lukas\\anaconda3\\envs\\VDKI-Projekt\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Data ROC AUC</td><td>0.98751</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>train_acc</td><td>0.97663</td></tr><tr><td>train_loss</td><td>0.18376</td></tr><tr><td>trainer/global_step</td><td>662</td></tr><tr><td>val_acc</td><td>0.89974</td></tr><tr><td>val_loss</td><td>0.29534</td></tr><tr><td>val_precision</td><td>0.90247</td></tr><tr><td>val_recall</td><td>0.89974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TL_ConvNextV2_base_cls2_bs32_img224_optAdam_lr2e-04_wd5e-05_sch_CosineAnnealingLR_2025-06-29_20-17</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/7dax1zo0' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext/runs/7dax1zo0</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/VDKI-Hasen_Hyperparameter_Tuning_FD_ConvNext</a><br>Synced 5 W&B file(s), 39 media file(s), 52 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250629_201731-7dax1zo0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Validation loss: 0.29533612728118896\n",
      "Best trial:\n",
      "{'batch_size': 64, 'image_size': 300, 'max_epochs': 24, 'accumulate_grad_batches': 1, 'precision': '16-mixed', 'optimizer': 'AdamW', 'learning_rate': 0.00025362959450590003, 'weight_decay': 0.0009073540762400554, 'scheduler': 'CosineAnnealingLR', 'model_classifier_layers': 2}\n",
      "Best value (val_loss): 0.2578020989894867\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "config['sweep_id'] = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "from models.model_facedetection import getConvNextV2_model\n",
    "from training.hyperparameter_tuning import FDOptunaTrainer\n",
    "\n",
    "config['image_size'] = 224\n",
    "def objective(trial):\n",
    "    model_fct = getConvNextV2_model\n",
    "    trainer = FDOptunaTrainer(\n",
    "        model=model_fct,                        # Function to create the model\n",
    "        config=config,\n",
    "        normalize_mean=[0.485, 0.456, 0.406], \n",
    "        normalize_std=[0.229, 0.224, 0.225],\n",
    "        dataset_name=\"DwarfRabbits-multiclass\"\n",
    "    )\n",
    "    return trainer.run_training(trial)\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")  # because we minimize val_loss\n",
    "\n",
    "# Set verbosity to WARNING to reduce output clutter\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Start the hyperparameter optimization\n",
    "study.optimize(objective, n_trials=config['number_of_trials'])\n",
    "\n",
    "# Best result\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best value (val_loss):\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7bbdb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0.2725939154624939,
          0.36477741599082947,
          0.4344993531703949,
          0.31950441002845764,
          0.33601468801498413,
          0.315517783164978,
          0.3678447902202606,
          0.2977973520755768,
          0.35525766015052795,
          0.3641675114631653,
          0.28526145219802856,
          0.3164723515510559,
          0.28412315249443054,
          0.2578020989894867,
          0.2601087987422943,
          0.2721419036388397,
          0.3310141861438751,
          0.2594107687473297,
          0.2633392810821533,
          0.29533612728118896
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2725939154624939,
          0.2578020989894867,
          0.2578020989894867,
          0.2578020989894867,
          0.2578020989894867,
          0.2578020989894867,
          0.2578020989894867,
          0.2578020989894867
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1309a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cliponaxis": false,
         "hovertemplate": [
          "precision (CategoricalDistribution): 0.004858006953708311<extra></extra>",
          "batch_size (CategoricalDistribution): 0.01840836686634056<extra></extra>",
          "weight_decay (FloatDistribution): 0.018489657931502285<extra></extra>",
          "accumulate_grad_batches (CategoricalDistribution): 0.020144569578463997<extra></extra>",
          "model_classifier_layers (IntDistribution): 0.024130069837111242<extra></extra>",
          "scheduler (CategoricalDistribution): 0.05562719544981569<extra></extra>",
          "optimizer (CategoricalDistribution): 0.1171892491654373<extra></extra>",
          "learning_rate (FloatDistribution): 0.16698999832670416<extra></extra>",
          "image_size (CategoricalDistribution): 0.2171115625775792<extra></extra>",
          "max_epochs (IntDistribution): 0.3570513233133372<extra></extra>"
         ],
         "name": "Objective Value",
         "orientation": "h",
         "text": [
          "<0.01",
          "0.02",
          "0.02",
          "0.02",
          "0.02",
          "0.06",
          "0.12",
          "0.17",
          "0.22",
          "0.36"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.004858006953708311,
          0.01840836686634056,
          0.018489657931502285,
          0.020144569578463997,
          0.024130069837111242,
          0.05562719544981569,
          0.1171892491654373,
          0.16698999832670416,
          0.2171115625775792,
          0.3570513233133372
         ],
         "y": [
          "precision",
          "batch_size",
          "weight_decay",
          "accumulate_grad_batches",
          "model_classifier_layers",
          "scheduler",
          "optimizer",
          "learning_rate",
          "image_size",
          "max_epochs"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Hyperparameter Importances"
        },
        "xaxis": {
         "title": {
          "text": "Hyperparameter Importance"
         }
        },
        "yaxis": {
         "title": {
          "text": "Hyperparameter"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62725a",
   "metadata": {},
   "source": [
    "## Load pretrained model and perform evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937a7074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "UsageError",
     "evalue": "Run (iablt2e2) is finished. The call to `_config_callback` will be ignored. Please make sure that you are using an active run.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUsageError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Put model in evaluation mode and move to correct device\u001b[39;00m\n\u001b[32m      7\u001b[39m model.eval()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:775\u001b[39m, in \u001b[36mTrainer.test\u001b[39m\u001b[34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    774\u001b[39m \u001b[38;5;28mself\u001b[39m.testing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:817\u001b[39m, in \u001b[36mTrainer._test_impl\u001b[39m\u001b[34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[39m\n\u001b[32m    813\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    814\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    815\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    816\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[32m    819\u001b[39m results = convert_tensors_to_scalars(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:995\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    992\u001b[39m     call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_fit_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    993\u001b[39m     call._call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_fit_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m \u001b[43m_log_hyperparams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strategy.restore_checkpoint_after_setup:\n\u001b[32m    998\u001b[39m     log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: restoring module and callbacks from checkpoint path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\loggers\\utilities.py:100\u001b[39m, in \u001b[36m_log_hyperparams\u001b[39m\u001b[34m(trainer)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m logger \u001b[38;5;129;01min\u001b[39;00m trainer.loggers:\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hparams_initial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         \u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_hyperparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_initial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     logger.log_graph(pl_module)\n\u001b[32m    102\u001b[39m     logger.save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\lightning_utilities\\core\\rank_zero.py:41\u001b[39m, in \u001b[36mrank_zero_only.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rank == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:432\u001b[39m, in \u001b[36mWandbLogger.log_hyperparams\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    430\u001b[39m params = _sanitize_callable_params(params)\n\u001b[32m    431\u001b[39m params = _convert_json_serializable(params)\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_val_change\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\wandb\\sdk\\wandb_config.py:189\u001b[39m, in \u001b[36mConfig.update\u001b[39m\u001b[34m(self, d, allow_val_change)\u001b[39m\n\u001b[32m    187\u001b[39m sanitized = \u001b[38;5;28mself\u001b[39m._update(d, allow_val_change)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msanitized\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:406\u001b[39m, in \u001b[36m_log_to_run.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    403\u001b[39m     run_id = \u001b[38;5;28mself\u001b[39m._attach_id\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging.log_to_run(run_id):\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Users\\Mika\\Anaconda\\envs\\VDKI-Projekt\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:472\u001b[39m, in \u001b[36m_raise_if_finished.<locals>.wrapper_fn\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    466\u001b[39m message = (\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is finished. The call to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Please make sure that you are using an active run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m UsageError(message)\n",
      "\u001b[31mUsageError\u001b[39m: Run (iablt2e2) is finished. The call to `_config_callback` will be ignored. Please make sure that you are using an active run."
     ]
    }
   ],
   "source": [
    "model = TransferLearningModuleMulticlass.load_from_checkpoint(\n",
    "    config['path_to_model'],\n",
    "    model=timm.create_model('efficientnet_b4', pretrained=False, num_classes=len(config['name_list'])),\n",
    "    num_classes=len(config['name_list'])\n",
    ")\n",
    "# Put model in evaluation mode and move to correct device\n",
    "model.eval()\n",
    "trainer.test(model=model, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18bedf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataLoader Info:\n",
      "Batch size: 32\n",
      "Number of batches: 15\n",
      "Shuffle: N/A\n",
      "Number of workers: 2\n",
      "Dataset: <data.dataset.MultiClassImageDataset object at 0x0000025A2D1E2660>\n",
      "Sampler: <torch.utils.data.sampler.SequentialSampler object at 0x0000025A13630B90>\n",
      "Drop last: False\n",
      "Pin memory: False\n",
      "Persistent workers: True\n",
      "Prefetch factor: 2\n",
      "Timeout: 0\n",
      "Test batch images shape: torch.Size([32, 3, 300, 300])\n",
      "Test batch labels shape: torch.Size([32, 6])\n",
      "Predicted labels: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1]\n",
      "True labels:      [[0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Show information about the test_loader\n",
    "print(\"Test DataLoader Info:\")\n",
    "print(f\"Batch size: {test_loader.batch_size}\")\n",
    "print(f\"Number of batches: {len(test_loader)}\")\n",
    "print(f\"Shuffle: {test_loader.shuffle if hasattr(test_loader, 'shuffle') else 'N/A'}\")\n",
    "print(f\"Number of workers: {test_loader.num_workers}\")\n",
    "print(f\"Dataset: {test_loader.dataset}\")\n",
    "print(f\"Sampler: {test_loader.sampler}\")\n",
    "print(f\"Drop last: {test_loader.drop_last}\")\n",
    "print(f\"Pin memory: {test_loader.pin_memory}\")\n",
    "print(f\"Persistent workers: {test_loader.persistent_workers if hasattr(test_loader, 'persistent_workers') else 'N/A'}\")\n",
    "print(f\"Prefetch factor: {test_loader.prefetch_factor if hasattr(test_loader, 'prefetch_factor') else 'N/A'}\")\n",
    "print(f\"Timeout: {test_loader.timeout}\")\n",
    "\n",
    "# Show a batch of test images, their shapes, and model predictions\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "print(\"Test batch images shape:\", images.shape)\n",
    "print(\"Test batch labels shape:\", labels.shape)\n",
    "\n",
    "# Move images to the correct device\n",
    "images = images.to(device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "print(\"Predicted labels:\", preds.cpu().numpy())\n",
    "print(\"True labels:     \", labels.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDKI-Projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
